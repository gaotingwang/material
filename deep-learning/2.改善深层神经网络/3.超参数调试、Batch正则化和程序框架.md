[TOC]

## 超参数调试、Batch正则化和程序框架

### 1. 调试处理（Tuning process）

关于训练深度最难的事情之一是如何选择处理的参数的数量，结果证实一些超参数比其它的更为重要，最为广泛的学习应用是$\alpha$，学习速率是需要调试的最重要的超参数。

调参顺序：

1. 学习速率$\alpha$
2. mini-batch的大小、隐藏单元数量（hidden units）、Momentum参数$\beta$（0.9是个很好的默认值）
3. 影藏层数（hidden layers）、学习衰减率（learning rate decay）
4. 当应用Adam算法时，$\beta_{1}$(0.9)，${\beta}_{2}$(0.999)和$\varepsilon$ ($10^{-8}$)，一般是采用默认值，不太会去调节它们

在早一代的机器学习算法中，常见的做法是在网格中取样点，当参数的数量相对较少时，这个方法很实用。

![](http://www.ai-start.com/dl2017/images/75bfa084ea64d99b1d01a393a7c988a6.png)

在深度学习领域，常用的是随机选择点，接着用这些随机取的点试验超参数的效果。之所以这么做是因为，对于要解决的问题而言，很难提前知道哪个超参数最重要。

举个例子，假设超参数1是学习速率$\alpha$，取一个极端的例子，假设超参数2是Adam算法的分母中的$\varepsilon$。在这种情况下，$\alpha$的取值很重要，而$\varepsilon$取值则无关紧要。如果在网格中取点，接着试验了$\alpha$的5个取值。会发现，无论$\varepsilon$取何值，结果基本上都是一样的。所以，虽然共有25种模型，但进行试验的$\alpha$值只有5个。

对比而言，如果随机取值，会试验25个独立的$\alpha$，似乎更有可能发现效果做好的那个。

实践中，搜索的可能不止两个或三个超参数，有时很难预知哪个是最重要的超参数。对于具体应用而言，随机取值而不是网格取值表明，可以探究了更多重要超参数的潜在值，无论结果是什么。

在超参数取值时，另一个惯例是采用由粗糙到精细的策略：

![](http://www.ai-start.com/dl2017/images/c3b248ac8ca2cf646d5b705270e01e78.png)

比如在二维的那个例子中，进行了取值，也许会发现效果最好的某个点，也许这个点周围的其他一些点效果也很好，那在接下来要做的是放大这块小区域（小蓝色方框内），然后在其中更密集得取值或随机取值，聚集更多的资源，在这个蓝色的方格中搜索。

如果你怀疑这些超参数在这个区域的最优结果，那在整个的方格中进行粗略搜索后，可以知道接下来应该聚焦到更小的方格中。在更小的方格中，可以更密集得取点。所以这种从粗到细的搜索也经常使用。

### 2. 为超参数选择合适的范围（Using an appropriate scale to pick hyperparameters）

上一篇讲了随机取值可以提升搜索效率，但随机取值并不是在有效范围内的随机均匀取值，而是选择合适的标尺，用于探究这些超参数，这很重要。

![](https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/%E6%A0%87%E5%B0%BA.png)

假设搜索超参数$\alpha$，假设怀疑其值最小是0.0001或最大是1。如果画一条从0.0001到1的数轴，沿其随机均匀取值，那90%的数值将会落在0.1到1之间，结果就是，在0.1到1之间，应用了90%的资源，而在0.0001到0.1之间，只有10%的搜索资源，这看上去不太对。

反而用**对数标尺**搜索超参数的方式会更合理，因此这里不使用线性轴，分别依次取0.0001，0.001，0.01，0.1，1，**在对数轴上均匀随机取点**，这样，在0.0001到0.001之间，就会有更多的搜索资源可用，还有在0.001到0.01之间等等。
$$
\begin{split}
r &= -4 \times np.random.rand() \\ 
\alpha &= 10^r \\
\end
{split}
$$
所以在**Python**中，使`r=-4*np.random.rand()`，这样$r \in[-4, 0]$可以均匀取值，$\alpha$在0.0001，0.001，0.01，0.1，1，每个分段内均匀取值。

所以总结一下，在对数坐标下取值，取最小值的对数就得到$a$的值，取最大值的对数就得到$b$值，然后在对数轴上的$10^{a}$到$10^{b}$区间取值，在$a$，$b$间随意均匀的选取$r$值，将超参数设置为$10^{r}$，这就是在对数轴上取值的过程。

另一个棘手的例子是给$\beta$ 取值，用于计算指数的加权平均值。假设认为$\beta$是0.9到0.999之间的某个值。和上面的内容类似，如果想在0.9到0.999区间搜索，那就不能用线性轴取值。不要随机均匀在此区间取值，所以考虑这个问题最好的方法就是，要探究的是$1-\beta$，此值在0.1到0.001区间内，所以会给$1-\beta$取值，大概是从0.1到0.001，应用之前介绍的方法，这是$10^{-1}$，这是$10^{-3}$。值得注意的是，在之前是把最小值写在左边，最大值写在右边，但在这里颠倒了大小。这里，左边的是最大值，右边的是最小值。所以要做的就是在$[-3,-1]$里随机均匀的给$r$取值。设定了$1- \beta = 10^{r}$，所以$\beta = 1-10^{r}$，然后这就变成了在特定的选择范围内超参数随机取值。希望用这种方式得到想要的结果，在0.9到0.99区间探究的资源，和在0.99到0.999区间探究的一样多。

为什么要这样做，为什么用线性轴取值不是个好办法? 这是因为当$\beta$ 接近1时，所得结果的灵敏度会变化，即使$\beta$有微小的变化。所以$\beta$ 在0.9到0.9005之间取值，无关紧要，你的结果几乎不会变化。但$\beta$值如果在0.999到0.9995之间，这会对算法产生巨大影响。

因为这个公式$\frac{1}{1- \beta}$，当$\beta$接近1时，$\beta$就会对细微的变化变得很敏感。所以整个取值过程中，需要更加密集地取值，在$\beta$ 接近1的区间内，或者说，当$1-\beta$ 接近于0时，这样，就可以更加有效的分布取样点，更有效率的探究可能的结果。

如果没有在超参数选择中作出正确的标尺决定，别担心，即使在均匀的标尺上取值，如果数值总量较多的话，也会得到还不错的结果，尤其是应用从粗到细的搜索方法，在之后的迭代中，还是会聚焦到有用的超参数取值范围上。

### 3. 超参数训练的实践：Pandas VS Caviar（Hyperparameters tuning in practice: Pandas vs. Caviar）

在结束关于超参数搜索的讨论之前，最后分享关于如何组织超参数搜索过程。

如今的深度学习已经应用到许多不同的领域，某个应用领域的超参数设定，有可能通用于另一领域，不同的应用领域出现相互交融。深度学习领域中，发展很好的一点是，不同应用领域的人们会阅读越来越多其它研究领域的文章，跨领域去寻找灵感。

你也许已经找到一组很好的参数设置，并继续发展算法，或许在几个月的过程中，观察到你的数据会逐渐改变，正因为有了这些变化，原来的超参数的设定不再好用，所以建议，或许只是**重新测试或评估超参数，至少每隔几个月一次**，以确保你对数值依然很满意。

最后，关于如何搜索超参数的问题，两种重要的思想流派通常采用的两种重要但不同的方式：

![](http://www.ai-start.com/dl2017/images/a361c621a9a0a1a99b03eef8716c5799.png)

- 一种是只照看一个模型，通常是有庞大的数据组，但没有许多计算资源或足够的**CPU**和**GPU**的前提下。基本而言，只可以一次负担起试验一个模型或一小批模型。在这种情况下，即使当它在试验时，你也可以逐渐改良。比如，第0天，将随机参数初始化，然后开始试验，然后逐渐观察其学习曲线，在第1天内逐渐减少；之后试着增加一点学习速率，看看它会怎样，也许结果证明它做得更好，那是第二天的表现。两天后，它依旧做得不错，也许可以填充下**Momentum**或减少变量。每天都会观察它，不断调整参数。所以这是一个人照料一个模型的方法，观察它的表现，耐心地调试学习率，但那通常是因为没有足够的计算能力，不能在同一时间试验大量模型时才采取的办法。（熊猫方式，一次通常只花精力照顾一个）
- 另一种方法则是同时试验多种模型，设置了一些超参数，尽管让它自己运行，或者是一天甚至多天，然后获得像上图右边这样的学习曲线，用这种方式可以试验许多不同的参数设定，最后快速选择工作效果最好的那个。（鱼子酱方式，不对其中任何一个多加照料，只是希望其中一个或其中一群，能够表现出色）

这两种方式的选择，是由你拥有的计算资源决定的。如果拥有足够的计算机去平行试验许多模型，那绝对采用鱼子酱方式，尝试许多不同的超参数，看效果怎么样。

但在一些应用领域，比如在线广告设置和计算机视觉应用领域，那里的数据太多了，需要试验大量的模型，所以同时试验大量的模型是很困难的，它的确是依赖于应用的过程。那些应用熊猫方式多一些，像对婴儿一样照看一个模型，调试参数，试着让它工作运转。

### 4. 归一化网络的激活函数（Normalizing activations in a network）

当训练一个模型，比如**logistic**回归时，归一化输入特征可以加快学习过程。对于更深的模型，不仅输入了特征值$x$，而且有激活值$a^{[1]}$，$a^{[2]}$等等。对任何一个隐藏层而言，能否归一化$a$值？

比如说$a^{[2]}$的值，以更快的速度训练$w^{[3]}$，$b^{[3]}$，因为$a^{[2]}$是下一层的输入值，所以就会影响$w^{[3]}$，$b^{[3]}$的训练。简单来说，这就是**Batch**归一化的作用。

尽管严格来说，真正归一化的不是$a^{[2]}$，而是$z^{[2]}$，深度学习文献中有一些争论，关于在激活函数之前是否应该将值$z^{[2]}$归一化，或是否应该在应用激活函数$a^{[2]}$后再规范值。实践中，经常做的是归一化$z^{[2]}$，推荐其为默认选择。

**batch归一化：**

在神经网络中，对每个$z^{(i)}$值使其规范化，方法如下，减去均值再除以标准偏差，为了使数值稳定，通常将$\varepsilon$作为分母，以防$σ=0$的情况。
$$
\begin{eqnarray*}
\mu &= \frac{1}{m}\sum_{i=1}^mz^{(i)} \tag{1} \\ 
\sigma &= \frac{1}{m}\sum_{i=1}^m(z^{(i)} - \mu)^2  \tag{2}\\
z_{\text{norm}}^{(i)} &= \frac{z^{(i)} - \mu}{\sqrt{\sigma^{2} + \varepsilon}}  \tag{3}
\end
{eqnarray*}
$$
现在已把这些$z$值标准化，化为含平均值0和标准单位方差，所以$z$的每一个分量都含有平均值0和方差1。但我们不想让隐藏单元总是含有平均值0和方差1，也许隐藏单元有了不同的分布会有意义，所以要做的就是计算：
$$
{\tilde{z}}^{(i)}= \gamma z_{\text{norm}}^{(i)} +\beta \tag{4}
$$
这里$\gamma$和$\beta$是模型的学习参数，所以我们使用梯度下降或一些其它类似梯度下降的算法，比如Momentum，Nesterov和Adam，会不断更新$\gamma$和$\beta$，正如更新神经网络的权重一样。

请注意$\gamma$和$\beta$的作用是，我们可以随意设置${\tilde{z}}^{(i)}$的平均值，事实上，如果$\gamma= \sqrt{\sigma^{2} +\varepsilon}$，即分母项（$z_{\text{norm}}^{(i)} = \frac{z^{(i)} -\mu}{\sqrt{\sigma^{2} +\varepsilon}}$中的分母），$\beta$等于$\mu$，那么$\gamma z_{\text{norm}}^{(i)} +\beta$的作用在于，它会精确转化这个方程。如果这些成立（$\gamma =\sqrt{\sigma^{2} + \varepsilon},\beta =\mu$），那么${\tilde{z}}^{(i)} = z^{(i)}$。

通过对$\gamma$和$\beta$合理设定，规范化过程，即这四个等式，从根本来说，只是计算恒等函数，通过赋予$\gamma$和$\beta$其它值，可以构造含其它平均值和方差的隐藏单元值。

**补充说明：**

归一化输入特征$X$是怎样有助于神经网络中的学习，Batch归一化的作用是它适用的归一化过程，不只是输入层，甚至同样适用于神经网络中的深度隐藏层。用Batch归一化了一些隐藏单元值中的平均值和方差，训练输入和这些隐藏单元值的一个区别是，不想隐藏单元值必须是平均值0和方差1。

比如，如果**sigmoid**激活函数，不想让$z$值总是全部集中在0值附近，想使它们有更大的方差，或不是0的平均值，以便更好的利用非线性的**sigmoid**函数，而不是使所有的值都集中于0值附近的线性版本中。这就是为什么有了$\gamma$和$\beta$两个参数后，通过其可以确保所有的$z^{(i)}$值可以是我们想赋予的任意值，或者它的作用是保证隐藏的单元已使均值和方差标准化。所以它真正的作用是，使隐藏单元值的均值和方差标准化，即$z^{(i)}$有固定的均值和方差，均值和方差可以是0和1，也可以是其它值，它是由$\gamma$和$\beta$两参数控制的。

### 5. 将 Batch Norm 拟合进神经网络（Fitting Batch Norm into a neural network）

**Batch归一化在深度网络训练中的应用：**

![](http://www.ai-start.com/dl2017/images/55047d3da405778b6272e6722cd28ac6.png)

假设有一个这样的神经网络，每个神经单元负责计算两件事：第一，它先计算z；然后应用其到激活函数中再计算$a$，所以每个圆圈代表着两步的计算过程。

如果没有应用Batch归一化，会把输入$X$拟合到第一隐藏层，然后首先计算$z^{[1]}$，这是由$w^{[1]}$和$b^{[1]}$两个参数控制的。接着，会把$z^{[1]}$拟合到激活函数以计算$a^{[1]}$。

但Batch归一化的做法是将$z^{[1]}$值进行Batch归一化，简称**BN**，此过程将由${\beta}^{[1]}$和$\gamma^{[1]}$两参数控制，这一操作会给出一个新的规范化的$z^{[1]}$值（${\tilde{z}}^{[1]}$），然后将其输入激活函数中得到$a^{[1]}$，即$a^{[1]} = g^{[1]}({\tilde{z}}^{[ l]})$。

**==可以看出Batch归一化是发生在计算$z$和$a$之间的==**。直觉就是，与其应用没有归一化的$z$值，不如用归一化过的$\tilde{z}$。所以，网络的参数就会是$w^{[1]}$，$b^{[1]}$，$w^{[2]}$和$b^{[2]}$等等，我们将要去掉这些参数。但现在，我们又将另一些参数加入到此新网络中${\beta}^{[1]}$，${\beta}^{[2]}$，$\gamma^{[1]}$，$\gamma^{[2]}$等等。对于应用Batch归一化的每一层而言。需要注意是，这里的这些$\beta$（${\beta}^{[1]}$，${\beta}^{[2]}$等等）和Momentum中的超参数$\beta$没有任何关系。

所以现在，这些是算法的新参数，接下来可以使用想用的任何一种优化算法，比如使用梯度下降法来执行它。举个例子，对于给定层，会计算$d{\beta}^{[l]}$，接着更新参数$\beta$为${\beta}^{[l]} = {\beta}^{[l]} - \alpha d{\beta}^{[l]}$。也可以使用Adam或RMSprop或Momentum，来更新参数$\beta$和$\gamma$，并不是只应用梯度下降法。

之前已经解释过Batch归一化是怎么操作的，计算均值和方差，减去均值，再除以方差。如果使用的是深度学习编程框架，通常不必自己把Batch归一化步骤应用于Batch归一化层。比如说，在TensorFlow框架中，可以用这个函数（`tf.nn.batch_normalization`）来实现Batch归一化。实践中，不必自己操作所有这些具体的细节，但知道它是如何作用的，可以帮助自己更好的理解代码的作用。在深度学习框架中，Batch归一化的过程，经常是类似一行代码的东西。

**与mini-batch一起使用：**

实践中，Batch归一化通常和训练集的mini-batch一起使用。

![](http://www.ai-start.com/dl2017/images/27dc60faf8c8e4d8360b4c8091b85355.png)

应用Batch归一化的方式就是，第一个mini-batch($X^{\{1\}}$)，然后计算$z^{[1]}$，应用参数$w^{[1]}$和$b^{[1]}$。接着Batch归一化会减去均值，除以标准差，由${\beta}^{[1]}$和$\gamma^{[1]}$重新缩放，这样就得到了${\tilde{z}}^{[1]}$，而所有的这些都是在第一个mini-batch的基础上，再应用激活函数得到$a^{[1]}$。然后用$w^{[2]}$和$b^{[2]}$计算$z^{[2]}$……。所以这一切都是为了在第一个mini-batch($X^{\{1\}}$)上进行一步梯度下降法。

类似的工作，会在第二个mini-batch（$X^{\left\{2 \right\}}$）上计算$z^{[1]}$，然后用Batch归一化来计算${\tilde{z}}^{[1]}$等，这里的Batch归一化步骤也是如此。

在第二个mini-batch（$X^{\left\{2 \right\}}$）中的例子，**在mini-batch上也是重新计算$z^{[1]}$的均值和方差**，重新缩放的$\beta$和$\gamma$得到${\tilde{z}}^{[1]}$。

$z$的公式如下，$z^{[l]} =w^{[l]}a^{\left\lbrack l - 1 \right\rbrack} +b^{[l]}$，但Batch归一化做的是，它要看这个mini-batch，先将$z^{[l]}$归一化，结果为均值0和标准方差，再由$\beta$和$$\gamma$$重缩放，但这意味着，无论$b^{[l]}$的值是多少，都是要被减去的。因为在Batch归一化的过程中，要计算$z^{[l]}$的均值，再减去平均值，在此例中的mini-batch中增加任何常数，数值都不会改变，因为加上的任何常数都将会被均值减去所抵消（均值包含在$\mu$内，在计算$z_{\text{norm}}^{(i)}$时又会被减去）。

所以，如果在使用Batch归一化，可以消除这个参数（$b^{[l]}$），或者也可以暂时把它设置为0，那么参数变成$z^{[l]} = w^{[l]}a^{\left\lbrack l - 1 \right\rbrack}$，然后计算归一化的$z_{\text{norm}}^{[l]}$，${\tilde{z}}^{[l]} = \gamma^{[l]}z^{[l]} + {\beta}^{[l]}$，最后会用参数${\beta}^{[l]}$，以便决定${\tilde{z}}^{[l]}$的取值，这就是原因。

**总结：**

所以总结一下，因为Batch归一化超过了$z^{[l]}$的均值，$b^{[l]}$这个参数没有意义，所以，必须去掉它，由${\beta}^{[l]}$代替，${\beta}^{[l]}$是控制参数，会影响转移或偏置条件。

记住$z^{[l]}$的维数，因为在这个例子中，维数会是$(n^{[l]},1)$，$b^{[l]}$的尺寸为$(n^{[l]},1)$，那${\beta}^{[l]}$和$\gamma^{[l]}$的维度也是$(n^{[l]},1)$，因为有$n^{[l]}$隐藏单元，所以${\beta}^{[l]}$和$\gamma^{[l]}$用来将每个隐藏层的均值和方差缩放为网络想要的值。

假设使用mini-batch梯度下降法：

1. 运行$t=1$到batch数量的**for**循环

2. 在mini-batch $X^{\left\{ t\right\}}$上应用正向传播，用Batch归一化代替$z^{[l]}$为${\tilde{z}}^{[l]}$。

3. 然后，用反向传播计算$dw^{[l]}$和$db^{[l]}$，及所有$l$层所有的参数，$d{\beta}^{[l]}$和$d\gamma^{[l]}$（严格来说，要去掉$b$）。

4. 最后，更新这些参数：$w^{[l]} = w^{[l]} -\text{αd}w^{[l]}$，${\beta}^{[l]} = {\beta}^{[l]} - {αd}{\beta}^{[l]}$，对于$\gamma$也是如此$\gamma^{[l]} = \gamma^{[l]} -{αd}\gamma^{[l]}$。

   也可以用Momentum、RMSprop、Adam的梯度下降法。与其使用梯度下降法更新mini-batch，可以使用这些其它算法来更新。

如果使用深度学习编程框架之一，这会使Batch归一化的使用变得很容易。

### 6. Batch Norm 为什么奏效？（Why does Batch Norm work?）

一个原因是，归一化输入特征值$x$，使不同范围的特征值均值为0，方差1，可以加速学习。所以Batch归一化起的作用的原因，直观的一点就是，它在做类似的工作，但不仅仅对于这里的输入值，还有隐藏单元的值，这只是Batch归一化作用的冰山一角。

首先理解一个概念：**Covariate shift**，如果模型已经学习了$x$到$y$ 的映射，如果$x$ 的分布改变了，那么可能需要重新训练学习算法。

![](http://www.ai-start.com/dl2017/images/08f134faa74ebd283a5f5f19be43efab.png)

“Covariate shift”的问题怎么应用于神经网络呢？试想一个如上图的深度网络，从第三层来看看学习过程。从第三隐藏层的角度来看，它得到一些值，称为$a_{1}^{[2]}$，$a_{2}^{[2]}$，$a_{3}^{[2]}$，$a_{4}^{[2]}$，但这些值也可以看做是特征值$x_{1}$，$x_{2}$，$x_{3}$，$x_{4}$，第三层隐藏层的工作是找到一种方式，使这些值映射到$\hat y$。第三层向前还有参数$w^{[2]}$，$b^{[2]}$和$w^{[1]}$，$b^{[1]}$，如果这些参数改变，这些$a^{[2]}$的值也会改变。所以从第三层隐藏层的角度来看，这些隐藏单元的值在不断地改变，所以它就有了“Covariate shift”的问题。

==Batch归一化做的，是它减少了这些隐藏值**分布变化**的数量==。Batch归一化讲的是$z_{1}^{[2]}$，$z_{2}^{[2]}$的值可以改变，它们的确会改变，当神经网络在之前层中更新参数，Batch归一化可以确保无论其怎样变化，$z_{1}^{[2]}$，$z_{2}^{[2]}$的均值和方差保持不变，所以即使$z_{1}^{[2]}$，$z_{2}^{[2]}$的值改变，至少他们的均值和方差也会是均值0，方差1（或不一定必须是均值0，方差1，而是由${\beta}^{[2]}$和$\gamma^{[2]}$决定的值）。如果神经网络选择的话，可强制其为均值0，方差1，或其他任何均值和方差。但它做的是，它限制了在前层的参数更新，会影响数值分布的程度，第三层看到的这种情况，因此得到学习加速。

**Batch归一化减少了输入值改变的问题（被均值和方差限制着），它的确使这些值变得更稳定，神经网络的之后层就会有更坚实的基础。即使使输入分布改变了一些，它会改变得更少。它做的是当前层保持学习，当改变时，迫使后层适应的程度减小了，它减弱了前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习，稍稍独立于其它层，这有助于加速整个网络的学习。**

**Batch归一化还有一个作用，它有轻微的正则化效果**。Batch归一化中非直观的一件事是，因为在mini-batch上计算的均值和方差，而不是在整个数据集上，它只是由一小部分数据估计得出的，均值和方差有一些小的噪声。缩放从$z^{[l]}$到${\tilde{z}}^{[l]}$的过程也有一些噪音，因为它是用有些噪音的均值和方差计算得出的。

和dropout相似，它往每个隐藏层的激活值上增加了噪音，它使一个隐藏的单元，以一定的概率乘以0，以一定的概率乘以1，所以dropout含几重噪音。对比而言，Batch归一化含几重噪音，因为标准偏差的缩放和减去均值带来的额外噪音。同时这里的均值和标准差的估计值也是有噪音的，所以类似于dropout，Batch归一化有轻微的正则化效果。

因为给隐藏单元添加了噪音，这迫使后部单元不过分依赖任何一个隐藏单元，类似于dropout，它给隐藏层增加了噪音，因此有轻微的正则化效果。因为添加的噪音很微小，所以并不是巨大的正则化效果。如果想得到dropout更强大的正则化效果，可以将Batch归一化和dropout一起使用。

另一个轻微非直观的效果是，如果用了较大的mini-batch，比如说用了512，会减少噪音，因此减少了正则化效果。就是应用较大的mini-batch可以减少正则化效果。

Batch有轻微正则化效果，这确实不是其目的，只是它会对算法有额外的期望效应或非期望效应。不要把Batch归一化当作正则化，正则化几乎是一个意想不到的副作用。

### 7. 测试时的 Batch Norm（Batch Norm at test time）

![](http://www.ai-start.com/dl2017/images/fd1f67f2e97e814079607f190111f65f.png)

在训练时，这些就是用来执行Batch归一化的等式。在一个mini-batch中，将mini-batch的$z^{(i)}$值求和，计算均值，这里用m来表示这个mini-batch中的样本数量，而不是整个训练集。然后计算方差，再算$z_{\text{norm}}^{(i)}$，即用均值和标准差来调整，加上$\varepsilon$是为了数值稳定性。$\tilde{z}$是用$\gamma$和$\beta$再次调整$z_{\text{norm}}$得到的。

用于调节计算的$\mu$和$\sigma^{2}$是在整个mini-batch上进行计算，但是在测试时，不可能将测试集mini-batch中的6428或2056个样本同时处理，因此需要用其它方式来得到$\mu$和$\sigma^{2}$，而且如果只有一个样本，一个样本的均值和方差没有意义。

那么实际上，为了将神经网络运用于测试，就需要单独估算$\mu$和$\sigma^{2}$，在典型的Batch归一化运用中，需要用一个指数加权平均来估算，这个平均数涵盖了所有mini-batch。

假设我们有mini-batch，选择$l$层，那么在为$l$层训练$X^{\{ 1\}}$时，就得到了$ \mu^{\left\{1 \right\}[l]}$ ；当训练第二个mini-batch，在这一层和这个mini-batch中，就会得到$\mu^{\{2\}[l]}$值；然后在这一隐藏层的第三个mini-batch，你得到了第三个$\mu^{\left\{3 \right\}[l]}$值。正如之前用的指数加权平均来计算当前气温的指数加权平均，可以这样来追踪看到的这个均值向量的最新平均值，于是这个指数加权平均就成了这一隐藏层的均值的估值。同样的，可以用指数加权平均来追踪在这一层的$\sigma^{2}$的值等等。因此在用不同的mini-batch训练神经网络的同时，能够得到要查看的每一层的$\mu$和$\sigma^{2}$的平均数的实时数值。

最后在测试时，对应等式（$z_{\text{norm}}^{(i)} = \frac{z^{(i)} -\mu}{\sqrt{\sigma^{2} +\varepsilon}}$），只需要用$z$值来计算$z_{\text{norm}}^{(i)}$，用$\mu$和$\sigma^{2}$的指数加权平均，用手头的最新数值来做调整，最后可以用刚算出来的$z_{\text{norm}}$和神经网络训练过程中得到的$\beta$和$\gamma$参数来计算测试样本的$\tilde{z}$值。

总结一下就是，在训练时，$\mu$和$\sigma^{2}$是在整个mini-batch上计算出来的，但在测试时，需要逐一处理样本，方法是根据训练集估算$\mu$和$\sigma^{2}$，估算的方式有很多种，理论上是在最终的网络中运行整个训练集来得到$\mu$和$\sigma^{2}$，但在实际操作中，我们通常运用指数加权平均来追踪在训练过程中看到的$\mu$和$\sigma^{2}$的值。这套过程是比较稳健的，如果使用的是某种深度学习框架，通常会有默认的估算$\mu$和$\sigma^{2}$的方式，应该一样会起到比较好的效果。但在实践中，任何合理的估算隐藏单元$z$值的均值和方差的方式，在测试中应该都会有效。

### 8. Softmax 回归（Softmax regression）

到目前为止，讲到过的分类的例子都使用了二分分类，这种分类只有两种可能的标记0或1，如果有多种可能的类型的话呢？有一种logistic回归的一般形式，叫做Softmax回归，是多种分类中的一个，不只是识别两个分类。

![多分类](http://www.ai-start.com/dl2017/images/e65ba7b81d0b02d021c33bf0094f4059.png)

建立一个神经网络，其输出层有4个或者说$C$个输出单元，即输出层也就是$L$层的单元数量。我们想要输出层单元的数字告诉我们这4种类型中每个的概率有多大，所以这里的一个节点表示的是在输入$X$的情况下，会输出该类	的概率。因为这里的$\hat y$将是一个$4×1$维向量，因此它必须输出四个数字，这四种概率，加起来应该等于1。

在神经网络的最后一层，$z^{[L]}$是最后一层的$z$变量，计算方法是$z^{[L]} = W^{[L]}a^{[L-1]} + b^{[L]}$，算出了$z$之后，需要应用Softmax激活函数，这个激活函数对于Softmax层而言有些不同，它的作用是这样的:

1. 首先，要计算一个临时变量叫做t，$t = e^{z^{[L]}}$，$t$的维度和$z^{[L]}$是一样的。
2. 最后进行归一化，使和为1。因此$a^{[L]} = \frac{t}{\sum_{j =1}^{4}t_{i}}$，换句话说，$a^{[L]}$也是一个4×1维向量，而这个四维向量的第$i$个元素，$a_{i}^{[L]} = \frac{t_{i}}{\sum_{j =1}^{4}t_{i}}$。

来看一个例子，详细解释，假设算出了$z^{[L]} = \begin{bmatrix} 5 \\ 2 \\  - 1 \\ 3 \\ \end{bmatrix}$，接下来就是用这个元素取幂方法来计算$t$，所以$t =\begin{bmatrix} e^{5} \\ e^{2} \\ e^{- 1} \\ e^{3} \\ \end{bmatrix}$，求得值$t = \begin{bmatrix} 148.4 \\ 7.4 \\ 0.4 \\ 20.1 \\ \end{bmatrix}$，从向量$t$得到向量$a^{[L]}$就只需要将这些项目归一化，使总和为1。把$t$的元素都加起来，把这四个数字加起来，得到176.3，最终$a^{[l]} = \frac{t} {176.3}$。

之前激活函数都是接受单行数值输入，例如Sigmoid和ReLu激活函数，输入一个实数，输出一个实数。Softmax激活函数的特殊之处在于，因为需要将所有可能的输出归一化，就需要输入一个向量，最后输出一个向量。

### 9. 训练一个 Softmax 分类器（Training a Softmax classifier）

![Softmax分类器](http://www.ai-start.com/dl2017/images/b433ed42cdde6c732820c57eebfb85f7.png)

Softmax这个名称的来源是与所谓hardmax对比，hardmax会把向量$z$变成向量，hardmax函数会观察$z$的元素，然后在$z$中最大元素的位置放上1，其它位置放上0，所以$\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \\ \end{bmatrix}$是一个hardmax，也就是最大的元素的输出为1，其它的输出都为0。与之相反，Softmax所做的从$z$到这些概率的映射更为温和，这一名称背后所包含的想法，与hardmax正好相反。

举个例子，某个样本的目标输出，真实标签是$\begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\ \end{bmatrix}$，假设神经网络输出的是$\hat y$，$\hat y$是一个包括总和为1的概率的向量，$a^{[L]} = \hat y = \begin{bmatrix} 0.3 \\ 0.2 \\ 0.1 \\ 0.4 \\ \end{bmatrix}$。对于这个样本神经网络的表现不佳，实际就是属于第二个分类，但却只分配到20%的概率，所以在本例中表现不佳。

**损失函数：**

在Softmax分类中，一般用到的损失函数是:
$$
L(\hat y,y ) = - \sum_{j = 1}^{C}{y_{j}log\hat y_{j}}
$$
注意在这个样本中$y_{1} =y_{3} = y_{4} = 0$，因为这些都是0，只有$y_{2} =1$ 。如果看这个求和，所有含有值为0的$y_{j}$的项都等于0，最后$L\left( \hat y,y \right) = - \sum_{j = 1}^{4}{y_{j}\log \hat y_{j}} = - y_{2}{\ log} \hat y_{2} = - {\ log} \hat y_{2}$ 。

这就意味着，如果学习算法试图将它变小，因为梯度下降法是用来减少训练集的损失的，要使它变小的唯一方式就是使$-{\log}\hat y_{2}$变小，要想做到这一点，就需要使$\hat y_{2}$尽可能大，因为这些是概率，所以不可能比1大，就需要这项输出的概率尽可能地大（$y= \begin{bmatrix} 0.3 \\ 0.2 \\ 0.1 \\ 0.4 \\ \end{bmatrix}$中第二个元素）。

概括来讲，损失函数所做的就是它找到你的训练集中的真实类别，然后试图使该类别相应的概率尽可能地高，如果熟悉统计学中最大似然估计，这其实就是最大似然估计的一种形式。

这是单个训练样本的损失，整个训练集的损失$J$就是对所有训练样本的预测都加起来：
$$
J( w^{[1]},b^{[1]},\ldots\ldots) = \frac{1}{m}\sum_{i = 1}^{m}{L( \hat y^{(i)},y^{(i)})}
$$


最后还有一个实现细节，注意因为$C=4$，如果实现向量化，矩阵大写$Y$就是$\lbrack y^{(1)},y^{(2)}\ldots\ldots\ y^{\left( m \right)}\rbrack$，那么这个矩阵$Y$最终就是一个$C\times m$维矩阵。类似的$\hat{Y}$本身也是一个$C \times m$维矩阵。

**梯度下降：**

已经讲了如何实现神经网络前向传播的步骤，来得到这些输出。其实初始化反向传播所需要的关键步骤或者说关键方程是这个表达式：
$$
dz^{[L]} = \hat{y} -y
$$
有了这个可以计算$dz^{[L]}$，然后开始反向传播的过程，计算整个神经网络中所需要的所有导数。

对于这些编程框架，通常只需要专注于把前向传播做对，它自己会弄明白怎样反向传播，会帮你实现反向传播，所以这个表达式值得牢记（$dz^{[l]} = \hat{y} -y$）。

### 10. 深度学习框架（Deep Learning frameworks）

使用**Python**和**NumPy**实现深度学习算法，当开始应用很大的模型，否则它就越来越不实用了，至少对大多数人而言，从零开始全部靠自己实现并不现实，例如卷积神经网络，或者循环神经网络。

![深度学习框架](http://www.ai-start.com/dl2017/images/acb3843cd1085b0742f39677289890a0.png)

现在有许多深度学习框架，能让实现神经网络变得更简单。每个框架都针对某一用户或开发群体的，这里的每一个框架都是某类应用的可靠选择，有很多人写文章比较这些深度学习框架，以及这些深度学习框架发展得有多好，而且因为这些框架往往不断进化，每个月都在进步，这里分享推荐一下选择框架的标准：

- 一个重要的标准就是**便于编程**，这既包括神经网络的开发和迭代，还包括为产品进行配置，为了成千上百万，甚至上亿用户的实际使用，取决于你想要做什么。
- 第二个重要的标准是**运行速度**，特别是训练大数据集时，一些框架能让你更高效地运行和训练神经网络。
- 还有一个标准人们不常提到，但很重要，那就是这个框架**是否真的开放**，要是一个框架真的开放，它不仅需要开源，而且需要良好的管理。不幸的是，在软件行业中，一些公司有开源软件的历史，但是公司保持着对软件的全权控制，当几年时间过去，人们开始使用他们的软件时，一些公司开始逐渐关闭曾经开放的资源，或将功能转移到他们专营的云服务中。

这里的多个框架都是很好的选择，通过提供比数值线性代数库更高程度的抽象化，这里的每一个程序框架都能让你在开发深度机器学习应用时更加高效。

### 11. TensorFlow

有很多很棒的深度学习编程框架，其中一个是TensorFlow，这里展示TensorFlow程序的基本结构。

先提一个启发性的问题，假设有一个损失函数$J$需要最小化。在本例中使用高度简化的损失函数，$J(w)= w^{2}-10w+25$，也许已经注意到该函数其实就是${(w -5)}^{2}$，所以使它最小的$w$值是5，但假设我们不知道这点，只有这个函数，看一下怎样用**TensorFlow**将其最小化。

```python
import numpy as np
import tensorflow as tf  # 导入TensorFlow

#在TensorFlow中，用tf.Variable()来定义可变tensor，在后续运算中此值会发生变化
w = tf.Variable(0, dtype = tf.float32)

#定义x，x的值会是不同值，先用占位符占位，稍后会为x提供数值
x = tf.placeholder(tf.float32, [3,1]) # 3x1维

#然后定义损失函数：
#cost = tf.add(tf.add(w**2,tf.multiply(- 10.,w)),25)
#TensorFlow其实还重载了一般的加减运算等等，因此也可以把写成更好看的形式
#cost = w**2 - 10*w +25
cost = x[0][0]*w**2 +x[1][0]*w + x[2][0]

#就是要接入x的数据
coefficient = np.array([[1.],[-10.],[25.]])

#用0.01的学习率，目标是最小化损失。
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)

#最后下面的几行是惯用表达式:
init = tf.global_variables_initializer()

session = tf.Session()#这样就开启了一个TensorFlow session。
session.run(init)#来初始化全局变量。

#它所做的就是运行一步梯度下降法:
#session.run(train)
session.run(train, feed_dict = {x:coefficients})

#在一步梯度下降法之后，w的值
print(session.run(w))

#运行梯度下降1000次迭代：
for i in range(1000):
    #session.run(train)
    session.run(train, feed_dict = {x:coefficients})
print(session.run(w))


```

$w$是我们想要优化的参数，因此将它称为变量，需要做的就是定义一个损失函数，使用这些`add`和`multiply`之类的函数。TensorFlow知道如何对`add`和`mutiply`，还有其它函数求导，这就是为什么只需基本实现前向传播，它能弄明白如何做反向传播和梯度计算，因为它已经内置在`add`，`multiply`和平方函数中。

TensorFlow中的placeholder是一个之后会赋值的变量，这种方式便于把训练数据加入损失方程，把数据加入损失方程用的是这个句法，当运行训练迭代，用`feed_dict`来让`x=coefficients`。

如果在做mini-batch梯度下降，在每次迭代时，需要插入不同的mini-batch，那么每次迭代，就用`feed_dict`来喂入训练集的不同子集，把不同的mini-batch喂入损失函数需要数据的地方。

让TensorFlow如此强大的是，只需说明如何计算损失函数，它就能求导，而且用一两行代码就能运用梯度优化器，Adam优化器或者其他优化器。

在TensorFlow中也可以用with代码块的形式:

```python
with tf.Session() as sess:
    sess.run(init)
    print(sess.run(w))
```
TensorFlow程序的核心是计算损失函数，然后TensorFlow自动计算出导数，以及如何最小化损失，因此$cost = x[0][0]*w**2 + x[1][0]*w + x[2][0]$所做的就是让TensorFlow建立计算图，计算图所做的就是取$x[0][0]$，取$w$，然后将它平方，然后$x[0][0]$和$w^{2}$相乘，就得到了$x[0][0]*w^{2}$，以此类推，最终整个建立起来计算cost，最后你得到了损失函数。

![](http://www.ai-start.com/dl2017/images/421ad00776a97e6cd4fd926e35a5a419.png)

TensorFlow之类的编程框架已经内置了必要的反向函数，这也是为什么通过内置函数来计算前向函数，它也能自动用反向函数来实现反向传播，即便函数非常复杂，再帮你计算导数，这就是为什么不需要明确实现反向传播，这是编程框架能帮你变得高效的原因之一。

在编程框架中可以用一行代码做很多事情，例如，不想用梯度下降法，而是想用Adam优化器，只要改变这行代码，就能很快换掉它，换成更好的优化算法。所有现代深度学习编程框架都支持这样的功能，可以很容易就能编写复杂的神经网络。

