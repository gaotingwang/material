[TOC]

逻辑回归算法
---

**逻辑回归(Logistic Regression)**如下：
$$
h_\theta(x) = g(X\theta)
$$
其中$g$函数称为**S型函数(sigmoid function)**或逻辑函数(logistic function):
$$
g(z) = \frac{1}{1 + e^{-z}}
$$

```matlab
function g = sigmoid(z)

% g = SIGMOID(z) computes the sigmoid of z.
% Instructions: Compute the sigmoid of each value of z (z can be a matrix,vector or scalar).
g = 1.0 ./ ( 1.0 + exp(-z));

end
```

plotData函数对数据($x_1,x_2$)进行可视化，便于直观理解

```matlab
function plotData(X, y)
%   PLOTDATA(x,y) plots the data points with + for the positive examples
%   and o for the negative examples. X is assumed to be a Mx2 matrix.

% Create New Figure
figure; hold on;

% Instructions: Plot the positive and negative examples on a
%               2D plot, using the option 'k+' for the positive
%               examples and 'ko' for the negative examples.

% Find Indices of Positive and Negative Examples
pos = find(y==1); neg = find(y == 0);
% Plot Examples
plot(X(pos, 1), X(pos, 2), 'k+','LineWidth', 2, ...
     'MarkerSize', 7);
plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y', ...
     'MarkerSize', 7);

hold off;

end
```

### 决策边界(Decision Boundary)

对于满足$X\theta = 0$表达式的$X$这些点连成的线就叫决策边界。<mark>决策边界是假设函数$h_\theta(x)$的属性，取决于参数$\theta$</mark>，一旦能确定$\theta$的值，就能确定出决策边界。

```matlab
function plotDecisionBoundary(theta, X, y)
%PLOTDECISIONBOUNDARY Plots the data points X and y into a new figure with
%the decision boundary defined by theta
%   PLOTDECISIONBOUNDARY(theta, X,y) plots the data points with + for the 
%   positive examples and o for the negative examples. X is assumed to be 
%   a either 
%   1) Mx3 matrix, where the first column is an all-ones column for the 
%      intercept.
%   2) MxN, N>3 matrix, where the first column is all-ones

% Plot Data
plotData(X(:,2:3), y);
hold on

if size(X, 2) <= 3 % 二维特征
    % Only need 2 points to define a line, so choose two endpoints
    plot_x = [min(X(:,2))-2,  max(X(:,2))+2];

    % Calculate the decision boundary line
    plot_y = (-1./theta(3)).*(theta(2).*plot_x + theta(1));

    % Plot, and adjust axes for better viewing
    plot(plot_x, plot_y)
    
    % Legend, specific for the exercise
    legend('Admitted', 'Not admitted', 'Decision Boundary')
    axis([30, 100, 30, 100])
else
    % Here is the grid range
    u = linspace(-1, 1.5, 50); % 产生-1,1.5之间的50点行矢量
    v = linspace(-1, 1.5, 50);

    z = zeros(length(u), length(v));
    % Evaluate z = theta*x over the grid
    for i = 1:length(u)
        for j = 1:length(v)
            z(i,j) = mapFeature(u(i), v(j))*theta;
        end
    end
    z = z'; % important to transpose z before calling contour

    % Plot z = 0
    % Notice you need to specify the range [0, 0]
    contour(u, v, z, [0, 0], 'LineWidth', 2)
end
hold off

end
```

### 逻辑回归的代价函数

$$
\begin{split}
cost(h_\theta(x),y) &=& \{
\end{split}\

\begin{eqnarray*}
 -log(h_\theta(x))&,　&(y=1) \\
 -log(1-h_\theta(x))&,　&(y=0) 
\end{eqnarray*}
$$

$$
J(\theta) = \frac{1}{m}\sum_{i=1}^mcost(h_\theta(x),y)
$$

综上所得：
$$
J(\theta) = \frac{1}{m}\sum_{i=1}^m[-y^{(i)}\log(h_\theta(x^{(i)})) - (1-y^{(i)})\log(1-h_\theta(x^{(i)}))]
$$
<mark>矢量化实现后</mark>：
$$
J(\theta) = -\frac{1}{m}(y^Tlog(g(X\theta)) + (1-y)^Tlog(1- g(X\theta)))
$$

### 逻辑回归的梯度下降

$$
\begin{eqnarray*}
&& Repeat\{ && \\
&& && \theta_j := \theta_j - \alpha\frac{d}{d\theta_j}J(\theta) \\
&& \} &&
\end{eqnarray*}
$$

带入$J(\theta)$后:
$$
\begin{eqnarray*}
&& Repeat\{ && \\
&& && \theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\
&& \} &&
\end{eqnarray*}
$$
<p style="color:red;">此处的$h_\theta(x)$与线性回归中的$h_\theta(x)$是不一样的，线性回归的$h_\theta(x)$表示$X\theta$，而逻辑回归的$h_\theta(x)$表示$g(X\theta)$，所以它们的梯度下降公式实际也是不同的两个公式。</p>

矢量化实现后：
$$
\theta := \theta - \alpha\frac{1}{m} X^T(g(X\theta)-y)
$$

```matlab
function [J, grad] = costFunction(theta, X, y)
%COSTFUNCTION Compute cost and gradient for logistic regression
%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the
%   parameter for logistic regression and the gradient of the cost
%   w.r.t. to the parameters.

% Initialize some useful values
m = length(y); % number of training examples
g = sigmoid(X * theta); % g(z) the sigmoid of z

% Instructions: Compute the cost of a particular choice of theta.
%               You should set J to the cost.
%               Compute the partial derivatives and set grad to the partial
%               derivatives of the cost w.r.t. each parameter in theta

J = (-1 / m) * (y' * log(g) + (1 - y)' * log(1-g));
grad = (1 / m) * X' * (g - y); % J(Θ)对Θ每个维度的偏导

end
```



逻辑回归的多类别分类
---

依次拆分成每个类别对其他类别的二元分类，就得到了多个分类器，通过这些分类器来估算出给出$x$和参数$θ$时，$y=i$的概率：
$$
h_θ^{(i)}(x)=P(y=i|x;θ)　(i=1,2,3)
$$
训练样本$X \in R^{m×(N+1)}$，当训练类$k \in \{1,\ldots,K\}$的分类器时，需要一个m维向量的标签y，其中$y_i \in 0,1$表示第$i$个训练实例是否属于类$k$ ($y_i = 1$)，或者它属于其他不同的分类 ($y_i = 0$)。

参数<mark>$\Theta \in R^{K×(N+1)} $</mark>(此时$\theta$以行向量表示，不是之前的列向量)，其中**$\Theta$的每一行对应于一个分类**的学习逻辑回归参数。即$\Theta$矩阵中的行数等于下一层即输出层（不包括偏置单位）中的节点数，列数等于$X$即输入层的特征数。

假设预测手写体数字(0\~9)，则有10个类别（为了索引方便，索引是从1开始的，1\~9的索引对应的是1\~9，0的索引是10）。样本数m = 5000，特征数n = 400，$X \in R^{5000×401}$，$k \in \{1,\ldots,10\}$，$\Theta \in R^{10×401} $

```matlab
function [all_theta] = oneVsAll(X, y, num_labels, lambda)
% num_labels类别数量

% Some useful variables
m = size(X, 1); % 样本数量
n = size(X, 2); % 特征数量
all_theta = zeros(num_labels, n + 1); 

% Add ones to the X data matrix,记得给X加第一列1
X = [ones(m, 1) X];

% Instructions: You should complete the following code to train num_labels
%               logistic regression classifiers with regularization
%               parameter lambda. 
%
% Hint: theta(:) will return a column vector.
%
% Hint: You can use y == c to obtain a vector of 1's and 0's that tell you
%       whether the ground truth is true/false for this class.
%
% Note: For this assignment, we recommend using fmincg to optimize the cost
%       function. It is okay to use a for-loop (for c = 1:num_labels) to
%       loop over the different classes.
%
%       fmincg works similarly to fminunc, but is more efficient when we
%       are dealing with large number of parameters.
%
% Example Code for fmincg:
%
%     % Set Initial theta
%     initial_theta = zeros(n + 1, 1);
%     
%     % Set options for fminunc
%     options = optimset('GradObj', 'on', 'MaxIter', 50);
% 
%     % Run fmincg to obtain the optimal theta
%     % This function will return theta and the cost 
%     [theta] = fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), initial_theta, options);
%

options = optimset('GradObj', 'on', 'MaxIter', 50);

for i = 1:num_labels
	% 由于theta是以行向量表示的，取第i行的参数作为theta
	thetai = all_theta(i, :);
	% 这里特别处理是(y == i)表示第i个训练实例是否属于类k
	[costTheta] = fmincg (@(t)(lrCostFunction(t, X, (y == i), lambda)), thetai(:), options);
	% theta再从列向量转置变为行向量放回去
	all_theta(i, :) = costTheta';
end


end
```

总之，要做的就是训练这个逻辑回归分类器$h_θ^{(i)}(x)$，其中$i$对应每一个可能的$y=i$。

最后，为了做出预测，我们给定一个新的输入值$x$，用来做预测，我们要做的就是在我们的每个分类器里输入$x$，然后<mark>选择一个让$h_θ^{(i)}(x)$最大的$i$</mark>，这就是预测出的类别。

结果预测：

```matlab
% result是m x k维的矩阵
result = sigmoid(X * all_theta');
% 找出让h(i)最大的i，就是预测的类别
% 返回每行最大值，结果存在ans里，index里存的是每行最大值的列位置。
[ans, index] = max(result, [], 2);
% 预测值
p = index;
% 准确率
fprintf('\nTraining Set Accuracy: %f\n', mean(double(pred == y)) * 100);
```

