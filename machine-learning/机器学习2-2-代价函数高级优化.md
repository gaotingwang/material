[TOC]

代价函数高级优化
---

我们有了$J(\theta)$和$\frac{d}{d\theta_j}J(\theta) $，就可以用梯度下降算法来递减$\theta$，从而接近全局最小值。

事实上，还有其他更复杂的算法来求最小代价函数，eg:共轭梯度法(Conjugate Gradient)、变尺度法(BFGS)、限制变尺度法(L-BFGS)。

这三种算法缺点就是算法复杂度高，优点：

1. <mark>不需要手动选择学习速率$\alpha$</mark>(内部有一个线性搜索算法，可以自动尝试不同的学习速率$\alpha$)
2. 通常情况下收敛速度比梯度下降算法快

高级优化函数fminunc调用过程：

```matlab
%  1.首先需要定义一个costFunction,依次返回J(Θ)和J(Θ)对Θ每个维度的偏导
%  2.fminunc表示无约束最小化函数，需要传入一个存有配置信息的变量options
%  3.'GradObj', 'on'表示梯度目标参数为打开状态，需要给fminunc提供一个梯度
%  4.'MaxIter', 400表示迭代400次
options = optimset('GradObj', 'on', 'MaxIter', 400);

%  @(t)这将创建一个函数，使用参数t来调用costFunction
%  这个函数最终返回迭代完毕后的theta值，和J(Θ)的值
[theta, cost] = fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);
```



拟合问题
---

算法没有很好的拟合数据，称为欠拟合(under fitting)或高偏差(high bias)。

在变量过多时，训练出的模型总能很好拟合训练数据( $J(\theta)$趋近于0 )，但这种无法泛化到其他数据样本中，以至于无法预测新样本的值，称为过度拟合(over fitting)或高方差(high variance)。

解决过度拟合办法：

1. 减少选取特征的数量(可能会舍弃掉一些有用的特性)
   - 人工选择舍弃部分特征变量
   - 模型选择算法(自动选择采用哪些特征)
2. 正则化(可以理解为通过缩小$\theta$来变相的减小特征值的影响，假设$\theta_i$接近0，则$x_i$的特征相当于被舍弃)

### 正则化(Regularization)

正则化线性回归：
$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2 + \frac{\lambda}{2m}\sum_{j=1}^{n}{\theta_j}^2
$$
正则化逻辑回归：
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i)}))]+ \frac{\lambda}{2m}\sum_{j=1}^{n}{\theta_j}^2
$$
即在原来的$J(\theta)$中<mark>多加入了$\frac{\lambda}{2m}\sum_{j=1}^{n}{\theta_j^2}$</mark>，需要注意<mark>$\theta_j$是从1开始的，$\theta_0=\vec{1}$不参与正则化</mark>。

$\lambda$的作用就是控制两个不同目标的一个平衡关系：

1. 通过训练使假设函数更好的拟合训练数据，<mark>平衡欠拟合和过拟合</mark>($\lambda$过小，比如为0会出现过拟合，过大会出现欠拟合)
2. <mark>保持参数值$\theta$较小</mark>

#### 正则化后的梯度下降(同样需要注意<mark>$\theta_0$不参与正则化</mark>)：

$$
\begin{eqnarray*}
&& Repeat\{ && \\
&& && \theta_0 := \theta_0 - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}) \\
&& && \theta_j := \theta_j - \alpha[\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} + \frac{\lambda}{m}\theta_j] \\
&& \} &&
\end{eqnarray*}
$$

即：
$$
\theta_j := \theta_j (1-\alpha\frac{\lambda}{m})- \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$
比之前的梯度下降多出了$ (1-\alpha\frac{\lambda}{m})$，<mark>$\alpha\frac{\lambda}{m}$通常是小于1的正数，把$\theta_j$向0压缩了一点点</mark>。

逻辑回归的正则化后的`costFunction`代码实现：

```matlab
function [J, grad] = costFunctionReg(theta, X, y, lambda)
%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization
%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using
%   theta as the parameter for regularized logistic regression and the
%   gradient of the cost w.r.t. to the parameters. 

% Initialize some useful values
m = length(y); % number of training examples
g = sigmoid(X * theta); % g(z) the sigmoid of z

% theta0 不参与正则化,从theta的第二位开始截取形成新的theta
newTheta = theta(2:length(theta));
J = (-1 / m) * (y' * log(g) + (1 - y)' * log(1-g)) + (lambda / (2 * m)) * (newTheta' * newTheta);

grad = (1 / m) * X' * (g - y) + (lambda / m) * theta; 
% theta0 不参与正则化,所以需要重新计算grad(1)
grad(1) = (1 / m) * X(:,1)' * (g - y);

end

```

结果预测：

```matlab
result = sigmoid(X * theta);
% 预测值
p = (result >= 0.5); % 向量中>=0.5的为1，否则为0
% 准确率
fprintf('Train Accuracy: %f\n', mean(double(p == y)) * 100);
```

#### 基于正则化的正规方程

对于线性回归的正规方程$θ = (X^TX)^{−1}X^Ty$也可以进行正则化:
$$
θ = (X^TX + \lambda
\left[
\begin{matrix}
 0      & 0      & \cdots & 0      \\
 0      & 1      & \cdots & 0      \\
 \vdots & \vdots & \ddots & \vdots \\
 0      & 0      & \cdots & 1      \\
\end{matrix}
\right]
)^{−1}X^Ty
$$
正则化之前$ (X^TX)^{−1}$存在不可逆情况，<mark>正则化后不存在不可逆情况</mark>。



