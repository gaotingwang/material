[TOC]

处理大数据算法
---

梯度下降在大量数据的情况下，每一次的梯度下降的计算量就变得非常大，因为需要对所有的训练样本求和。因此，这种在每次迭代中对所有数据都进行计算的梯度下降算法也被称为**批量梯度下降(batch gradient descent)**。

| 对比         | 批量梯度下降                                   | 随机梯度下降                                   | 小批量梯度下降                                  |
| ---------- | ---------------------------------------- | ---------------------------------------- | ---------------------------------------- |
| 梯度下降Repeat | $\theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$ | $\theta_j := \theta_j - \alpha(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$ | $\theta_j := \theta_j - \alpha\frac{1}{10}\sum_{k=1}^{i+9}(h_\theta(x^{(k)}) - y^{(k)})x_j^{(k)}$ |
| 描述         | 1.每次遍历是针对所有样本（需要加合所有样本，对所有数据扫描）<br />2.指定遍历次数，通常很大 | 1.每次循环针对一个样本，遍历整个样本<br />2.由于m非常大，外层循环通常一次就好，最多10次，具体取决于样本大小 | 每次迭代使用b个样本，b的标准取值是2~100之间的一个数            |

### 随机梯度下降

重复执行梯度下降计算，注意，这里每一次计算$θ_j$不是遍历全部的训练集$m$，而是从$m$个训练集里取出1个样本来计算。所以每次梯度下降的计算只需要一个样本代入计算。
$$
cost(\theta,(x^{(i)},y^{(i)})) = \frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2 \\
J(\theta) = \frac{1}{m}\sum_{i=1}^{m}cost(\theta,(x^{(i)},y^{(i)}))
$$
随机梯度下降过程中，相比于批量梯度下降，会更曲折一些，但每一次的迭代都会更快，因为不需要对所有样本求和，每一次只需要保证拟合一个样本即可.

随机梯度下降和批量梯度下降这两种算法的收敛形式是不同的，可以发现<mark>随机梯度下降最终会在靠近全局最小值的区域内徘徊，而不是直接逼近全局最小值并停留在那里</mark>。但实际上这并没有太大问题，只要参数最终移动到某个非常靠近全局最小值的区域内，这也会得到一个较为不错的假设。

### 小批量梯度下降

**小批量梯度下降**的做法介于**批量梯度下降**和**随机梯度下降**之间。准确地说在这种方法中每次迭代使用b个样本，b是一个叫做**“小批量规模”**的参数。

**小批量梯度下降算法**和**随机梯度下降算法**相比优势在于**向量化**。批量处理的数据可以用一种更向量化的方法来实现，允许部分并行计算10个样本的和，而**随机梯度下降算法**每次只去计算一个样本，没有太多的并行计算。

缺点是需要额外的参数$b$。因此需要一些时间来调试$b$的大小。但是如果有一个好的向量化实现，这种方式会比随机梯度下降更快一些。

### 算法收敛检查

对于**随机梯度下降算法**，为了检查算法是否收敛，在每一次梯度下降的迭代执行前，都用当前的随机样本$(x^{(i)},y^{(i)})$来计算当前关于$\theta$的$cost$函数：
$$
cost(\theta,(x^{(i)},y^{(i)})) = \frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2
$$
为了检查随机梯度下降的收敛性，要做的是每1000次迭代，可以画出前一步中计算出的$cost$函数。

把这些$cost$函数画出来，并对算法处理的最后1000个样本的$cost$值求平均值（收敛的话，最后会在最小区域内徘徊，所以求平均值）。如果这样做的话它会很有效地帮你估计出算法在最后1000个样本上的表现。所以，不需要时不时地计算$J(\theta)$，那样的话需要所有的训练样本。

由于每次都是在一小部分的样本（1000个）上去求平均值，因此看起来是有噪声的。如果得到像这样的图，那么可以判断这个算法的cost值是在下降的，并且后来趋于平缓，这基本说明算法已经收敛了。

<img src="http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8DJ%E4%B8%8E%E8%BF%AD%E4%BB%A3%E6%AC%A1%E6%95%B0%E5%85%B3%E7%B3%BB.png" width="450px" alt="随机梯度下降J与迭代次数关系"/>

### 学习速率$\alpha$选则

- <mark>更小的学习速率，最终的振荡就会更小</mark>，有时候这一点小的区别可以忽略

- 把cost样本数从1000提高到5000组样本，<mark>样本数目提高可能会得到一条更平滑的曲线</mark>；当然增大它的缺点就是获取结果延迟。

- 如果出现下图蓝线这种情况，可能代价函数没有在减小。也有可能实际上代价函数是在下降的只不过蓝线用来平均的样本数量太小了，并且蓝线太嘈杂看不出来代价函数的趋势确实是下降的。所以可以用5000组样本来平均比用1000组样本来平均更能看出趋势。

  <img src="http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%AD%A6%E4%B9%A0%E9%80%9F%E7%8E%87.png" width="450px" alt="随机梯度下降学习速率"/>

### 使用可变学习速率

在大多数随机梯度下降法的典型应用中，学习速率$\alpha$一般是**保持不变**的。因此最终得到的结果一般来说是在最小值区域振荡。如果想让随机梯度下降确实收敛到全局最小值，可以随时间的变化减小学习速率$\alpha$的值。
$$
\alpha = \frac{C}{m + B}
$$
常数$C$和$B$是两个额外的参数，需要选择一下才能得到较好的表现。但很多人不愿意用这个办法的原因是最后会把时间花在确定常数上，这让算法显得更繁琐。

但如果能调整得到比较好的参数的话，得到的图形是随着迭代的增加，在最小值附近震荡的范围越来越小，最终几乎靠近全局最小的地方。

但由于确定这两个常数需要更多的工作量，并且我们通常也对能够很接近全局最小值的参数已经很满意了，因此在随机梯度下降中很少采用逐渐减小$\alpha$值的方法，更多的还是让$\alpha$的值为常数。

在线学习
---

在拥有连续的数据流涌进来，则需要一个算法来从中不断学习来模型化问题。

与以往的学习过程不同的是，在线学习中，<mark>每次梯度下降使用当前样本数据$(x,y)$来更新$\theta$，使用一次之后就丢弃了</mark>，之后永远都不会再次使用它。
$$
\theta_j := \theta_j - \alpha(h_\theta(x) - y)x_j \ \ \  (j = 0,\dots,n)
$$
因为数据本质上是自由的，而且数据本质上是无限的，那么或许就真的没必要重复处理一个样本。

当然，如果只有少量数据，那么就不选择像这样的在线学习算法，这种情况下最好是要保存好所有的数据，然后对这个数据集使用某种算法。

### 偏好调整

上述这种算法可以对正在变化的用户偏好进行调整。

如果随时间变化各种因素变得对用户的影响更大，用户行为发生变化，这种算法也可以根据用户偏好的变化进行调整。

用户群变了，对于参数$\theta$的变化与更新，该算法会逐渐调试到最新的用户群体所应该体现出来的参数。

### 预估点击率CTR

对于条件搜索类，假定每次用户进行一次搜索，我们回馈给用户十个结果。这样运行此类网站的一种方法就是连续给用户展示十个最佳猜测，这十个推荐是指用户可能想要的结果。

那么每次一个用户访问，根据用户是否点击，将会得到十个$(x,y)$样本数据对，然后利用一个在线学习算法来更新参数。更新过程中会对这十个样本利用10步梯度下降法，然后就可以丢弃这些数据了。

### 总结

**在线学习机制**，与**随机梯度下降算法**非常类似，唯一的区别的是不会使用一个固定的数据集，而是获取当前一个用户样本，从那个样本中学习，然后丢弃那个样本并继续下去。而且如果对某一种应用有一个连续的数据流，这样的算法可能会非常值得考虑。

当然在线学习的一个优点，就是如果有一个变化的用户群、又或者在尝试预测的事情在缓慢变化，这个<mark>在线学习算法可以慢慢地调试所学习到的假设，将其调节更新到最新的用户行为。</mark>

映射约减（Map Reduce）
---

梯度下降的算法都只能在一台计算机上运行，但有的时候，机器学习的运算量巨大，以至于单台机器处理起来特别耗时，所以人们希望能在多台机器上同时执行运算来提高效率。

<img src="http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/map-reduce.png" width="450px" alt="map-reduce"/>

对于梯度下降算法：
$$
\theta_j := \theta_j - \alpha\frac{1}{400}\sum_{i=1}^{400}(h_\theta(x^{(i)})-y^{(i)})x_j^{(j)}
$$
可以拆分为：
$$
temp_j^{(i)} := \sum_{(i-1) \times 100 + 1}^{i \times100}(h_\theta(x^{(i)})-y^{(i)})x_j^{(j)} \\
\theta_j := \theta_j - \alpha\frac{1}{400}\sum_{i=1}^{4}(temp_j^{(i)})
$$

- 将这4份训练样本的子集送给4台不同的计算机，每一台计算机对四分之一的训练数据进行求和运算
- 最后这4个求和结果 被送到一台中心计算服务器负责对结果进行汇总

### 使用条件

如果打算将Map Reduce技术用于加速某个机器学习算法，也就是说打算运用多台不同的计算机并行的进行计算，那么需要问自己一个很关键的问题，那就是**自己的机器学习算法是否可以表示为训练样本的某种求和**。

事实证明，很多机器学习算法的确可以表示为关于训练样本的函数求和。而在处理大数据时，这些算法的主要运算量在于对大量训练数据求和。



如果有一个很大的训练样本，也可以使用一台四核的计算机，然后将训练样本分成4份，让每一个核处理其中一份子样本。

如果算法实现使用的是某种自动利用多核的线性代数运算库，那么就没有必要去手动在单台机器上实现Map Reduce了。但并不是所有的库都会自动并行运算。

