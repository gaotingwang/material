[TOC]

线性回归算法
---

关于变量定义：
$$
\begin{eqnarray*}
n&:& 表示特这数量 \\
m&:& 表示训练样本数量 \\
X&:& 表示输入的特征值 \\
y&:&  表示输出变量值或目标标签 \\
(x^{(i)},y^{(i)})&:&  表示第i个训练样本 \\
x_j^{(i)}&:&  表示第i个输入特征的第j维值 \\
h&:&  表示一个从x到y的函数映射
\end{eqnarray*}
$$
关于$X$的预测函数：
$$
h_\theta(x^{(i)}) = \theta_0x_0^{(i)} + \theta_1x_1^{(i)} + \theta_2x_2^{(i)}\cdots+ \theta_nx_n^{(i)}　(x_0=1)
$$

### 代价函数：预测值与实际值的平方差和最小

**代价函数(Cost function)**如下：
$$
J(\theta_1,\theta_2,\ldots,\theta_n)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2
$$
矢量化实现后：
$$
J(\theta) = \frac{1}{2m}(X\theta - \vec{y})^{T}(X\theta - \vec{y})
$$

```matlab
function J = computeCost(X, y, theta)
%COMPUTECOST Compute cost for linear regression
%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the
%   parameter for linear regression to fit the data points in X and y

% Initialize some useful values
m = length(y); % number of training examples
predictions = X*theta; %  the function of h(x)
J = 1/(2*m) * sum((predictions-y).^2);

% yy = predictions-y;
% J = 1/(2*m)*(yy'*yy);

end
```

<mark>学习算法优化目标：找到合适的参数使得代价函数最小</mark>

### 梯度下降算法(Gradient Descent)

$$
\begin{eqnarray*}
&& Repeat\{ && \\
&& && \theta_j := \theta_j - \alpha\frac{d}{d\theta_j}J(\theta_1,\theta_2,\ldots,\theta_n) \\
&& \} &&
\end{eqnarray*}
$$

变形之后即：
$$
\begin{eqnarray*}
&& Repeat\{ && \\
&& && \theta_j := \theta_j - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(j)} \\
&& \} &&
\end{eqnarray*}
$$
矢量化实现后：
$$
\theta := \theta - \alpha\frac{1}{m} X^T(X\theta-y)
$$
实现梯度算法的重点在于要<mark>同时更新$ \theta_i$</mark>。

```matlab
function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
n = length(theta); % number of features
J_history = zeros(num_iters, 1);

for iter = 1:num_iters
    predictions = X * theta; % 同时更新θ，所以theta放在外层，这样内部循环引用的一直是旧的theta
    
    for j = 1:n
        theta(j) = theta(j) - alpha * (1 / m) * sum((predictions - y) .* X(:,j));
    end

    % Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);

end

% for iter = 1:num_iters
% 	theta = theta - (alpha / m) * X' * (X * theta - y);
% 	J_history(iter) = computeCost(X, y, theta);
% end;

end
```

#### 梯度算法最佳实践

- 缩放特征

  所有特征值都在一个相近范围，这样梯度下降就能更快收敛（<mark>对$x_0$不进行均值归一化操作</mark>）
  $$
  x_i := \frac{x_i (原值)- u_i(均值)}{s_i(标准偏差)}
  $$

  ```matlab
  function [X_norm, mu, sigma] = featureNormalize(X)

  X_norm = X;
  mu = zeros(1, size(X, 2));
  sigma = zeros(1, size(X, 2));   

  % mean(A) = mean(A,1) 返回每一列的平均值；mean(A,2) 返回每一行的平均值
  mu = mean(X) %均值

  % std(A，flag，dim) 对A求标准偏差，flag=0,1表示除以N-1还是N，dim=1,2表示沿列方向还是行方向
  sigma = std(X, 0, 1) %标准偏差

  mus = mu;
  for i=2:size(X, 1)
    mus = [mus;mu];
  end

  sigmas = sigma;
  for i=2:size(X, 1)
    sigmas = [sigmas;sigma];
  end

  X_norm = (X - mus) ./ sigmas

  % 上述代码可以简写为
  % mu = mean(X);
  % X_norm = bsxfun(@minus, X, mu);

  % sigma = std(X_norm);
  % X_norm = bsxfun(@rdivide, X_norm, sigma);

  end

  % 对预测值进行归一化处理
  % X_poly_test = bsxfun(@minus, X_poly_test, mu);
  % X_poly_test = bsxfun(@rdivide, X_poly_test, sigma);
  ```

- 学习率$\alpha$选择

  将$J(\theta)$作为迭代次数的函数，观察不同$\alpha$对该函数的影响，如果取值太小收敛慢，太大则无法收敛

  ```matlab
  % A 是 3x4的矩阵
  % a = numel(A);   返回数组A中元素个数，执行后a=12
  % b = numel(A, A > 3);  返回数组A中值大于3的元素个数，执行后b=6
  % c = numel(A, 1:2, 2:4);  相当于numel(A(1:2, 2:4)); 执行后c=6
  % Plot the convergence graph
  figure;
  plot(1:numel(J_history), J_history, '-b', 'LineWidth', 2);
  xlabel('Number of iterations');
  ylabel('Cost J');
  ```

结果预测：

```matlab
test = [1650 3];
% 对预测值也要做均值归一化处理
test = (test - mu) ./ sigma; 
% 加X0=1
test = [ones(1, 1) test];
price = test * theta; 
```

### 正规方程(Normal Equations)

如何最小化这个代价函数$J(\theta)$?实际上微积分告诉我们一种方法: **对每个参数$θ$求$J$的偏导数，然后把它们全部置零**。如果你这样做 并且求出$θ_0$，$θ_1$，一直到$θ_n$的值，这样就能得到能够最小化代价函数$J$的$θ$值。
$$
θ = (X^TX)^{−1}X^Ty
$$

```matlab
function [theta] = normalEqn(X, y)

theta = pinv((X' * X)) * X' * y

end
```

### 梯度下降法VS正规方程

何时应该使用梯度下降法，而何时应该使用正规方程法呢？这里列举了一些它们的优点和缺点：

假如有m个训练样本和n个特征变量 :

| 梯度下降算法      | 正规方程法                                    |
| ----------- | ---------------------------------------- |
| 需要选择一个学习速率α | 不需要选择一个学习速率α                             |
| 需要多次迭代      | 不需要多次迭代                                  |
| 当n很大时也能良好运行 | 需要计算$(X^TX)^{−1}$当n很大时，运行会很慢(因为复杂度为:$O(n^3)$) |
|             |                                          |

> 那么在什么情况下会考虑使用梯度下降算法呢？ 通常`n<10000`的情况下，建议使用正规方程法，当`n>10000`时，建议使用梯度下降法。

<mark>在m≤n的时候会出现$(X^TX)^{−1}$不可逆问题</mark>。一方面减少冗余特征，另一方面可以使用`pinv()`代替`inv()`，可以求解伪逆矩阵。

随着要学习算法越来越复杂，例如分类算法：像逻辑回归算法，会看到实际上对于那些问题，并不能使用正规方程法。对于那些更复杂的学习算法，不得不仍然使用梯度下降法。

因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题，因为标准方程法不适合用在它们上。但对于特征变量比较少的情况下，正规方程法是一个比梯度下降法更快的替代算法。所以这两算法都是值得学习的。

、、、