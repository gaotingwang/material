[TOC]

推荐系统(recommender systems)
---

![电影评分](http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.jpeg)

推荐系统问题就是：在给定上面的这些数据时，怎么去预测上面表格中那些”?”的值。这样就可以向用户推荐他们可能还没看过的新电影了。

申明定义：
$$
\begin{eqnarray*}
n_u&=&用户数量\\
n_m&=&电影数量\\
r(i,j)&=&如果用户j对电影i投过票记为1\\
y^{(i,j)}&=&用户j对电影i的评分 (只有在r(i,j)=1时，才有y^{(i,j)})
\end{eqnarray*}
$$
并且追加了两列特征$x_1$和$x_2$，分别表示当前电影属于**爱情片**和**动作片**的程度。用$n=2$来代表特征数（这里不包括$x_0$）。

### 预测用户对电影的评分

为了对用户对电影的评分进行预测，可以把每个观众打分的预测当成一个独立的线性回归问题。**对每个用户应用不同的线性回归模型**。具体来说，比如每一个用户$j$，都学习出一个参数$\theta^{(j)}$（一般情况下，<mark>$\theta^{(j)}$的维度都是$n+1$</mark>，$n$是特征数，不包括截距项$x_0$）。
$$
\theta^{(j)} = 用户j的参数向量 \\
x^{(i)} = 电影i的特征向量
$$
然后根据参数向量$\theta^{(j)}$与特征$x^{(i)}$的内积来预测用户对电影$i$的评分：
$$
(\theta^{(j)})^Tx^{(i)}
$$

#### 计算参数量$\theta$

为了学习第$j$个用户的参数$\theta^{(j)}$，执行以下运算：
$$
min_{\theta^{(j)}} \frac{1}{2}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{k=1}^{n}{(\theta_k^{(j)})^2}
$$

> 其中$\sum_{i:r(i,j)=1}$代表对所有满足$r(i,j)=1$的元素进行求和。
>
> $((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})$代表预测用户$j$对电影$i$的平分减去用户对这部电影的实际评分。
>
> $\frac{\lambda}{2}\sum_{k=1}^{n}{(\theta_k^{(j)})^2}$是正则化项。

为了学习全部用户的参数$\theta^{(1)},\dots,\theta^{(n_u)}$，执行以下运算：
$$
min_{\theta^{(1)},\dots,\theta^{(n_u)}}  \frac{1}{2}\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}{(\theta_k^{(j)})^2}
$$
通过这一运算，就能得出所有用户的参数向量$\theta^{(1)},\dots,\theta^{(n_u)}$，从而对所有用户作出预测了。

#### 优化目标

使用梯度下降算法来计算：

- $k = 0$时：
  $$
  \theta_k^{(j)} := \theta_k^{(j)} - \alpha\sum_{i:r(i,j)=1}((\theta^{(j)})^T-y^{(i,j)})^2x_k^{(i)}
  $$

- $k \neq 0$时：
  $$
  \theta_k^{(j)} := \theta_k^{(j)} - \alpha \left ( \sum_{i:r(i,j)=1}((\theta^{(j)})^T-y^{(i,j)})^2x_k^{(i)} + \lambda\theta_k^{(j)} \right )
  $$





### 协同过滤(collaborative filtering)

在实际情况中，我们并不知道每部电影具体的特征值是多少，我们可以着眼于用户，看看任意用户$j$对应的不同题材电影的喜欢程度$\theta^{(j)}$。有了这些参数值，理论上就能推测出每部电影的特征变量$x_1$和$x_2$的值。

在已知给定参数$\theta^{(1)},\dots,\theta^{(n_u)}$的情况下，对下面函数最小化，得到特征值$x^{(i)}$：
$$
min_{x^{(i)}} \frac{1}{2}\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{k=1}^{n}{(x_k^{(i)})^2}
$$
计算所有的特征，我们通过最小化下面的函数可以得到：
$$
min_{x^{(1)},\dots,x^{(n_m)}}  \frac{1}{2}\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}{(x_k^{(i)})^2}
$$
类似**先有鸡还是先有蛋**的问题。知道了$x$就能预测出$\theta$，反之，如果知道了$\theta$，就能预测出$x$。一直重复上述的计算过程，算法将会收敛到一组合理的特征值$x$，以及一组合理的对不同用户参数的估计值$\theta$。这就是基本的**协同过滤算法**，但这实际上不是我们最终使用的算法。

有一个更有效率的算法，不必再这样不停地来回计算，而是能同时把$x$和$\theta$同时计算出来。要做的就是把上面两个式子合而为一：同时最小化$x^{(1)},\dots,x^{(n_m)}$和$\theta^{(1)},\dots,\theta^{(n_u)}$：
$$
J(x^{(1)},\dots,x^{(n_m)},\theta^{(1)},\dots,\theta^{(n_u)})=\frac{1}{2}\sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2 +\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}{(x_k^{(i)})^2} + \frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^{n}{(\theta_k^{(j)})^2}
$$

```matlab
% J
predict = X * Theta'; % 维度num_movies x num_users
error2 = (predict - Y) .* (predict - Y);
J = sum(sum(R .* error2)) / 2; % R(i,j)不为0之和 

% 正则化J
J += (lambda / 2) * sum(sum(X .* X));
J += (lambda / 2) * sum(sum(Theta .* Theta));
```

#### 注意点

不需要不断地在$x$和$\theta$这两个参数之间不停折腾，所要做的是将这两组参数同时化简。

以这样的方法学习特征量时，<mark>必须要保证去掉$x_0$项，这样来保证学习到的特征量$x$是$n$维的</mark>，而不是$n+1$维的。

同样地，因为参数$\theta$与特征向量$x$是在同一个维度上，所以$\theta$也是$n$维的。因为如果没有$x_0$，那么$\theta_0$也不再需要。

#### 协同过滤算法的正式描述

1. 首先用较小的初始值来随机初始化参数$x^{(1)},\dots,x^{(n_m)}$和$\theta^{(1)},\dots,\theta^{(n_u)}$

2. 通过使用梯度下降算法（或者其他更高级的优化算法）来最小化代价函数$J(x^{(1)},\dots,x^{(n_m)},\theta^{(1)},\dots,\theta^{(n_u)})$。

   循环每一个$j=1,\dots,n_u;i=1,\dots,n_m$来执行对应的梯度下降：
   $$
   \begin{split}
   x_k^{(i)} &:=& x_k^{(i)} - \alpha \left ( \sum_{j:r(i,j)=1}((\theta^{(j)})^T-y^{(i,j)})^2\theta_k^{(j)} + \lambda x_k^{(i)} \right ) \\
   \theta_k^{(j)} &:=& \theta_k^{(j)} - \alpha \left ( \sum_{i:r(i,j)=1}((\theta^{(j)})^T-y^{(i,j)})^2x_k^{(i)} + \lambda\theta_k^{(j)} \right )
   \end{split}
   $$
   <mark>在这个公式中，不再用到$x_0=1$这一项，在做正则化的时候，也是对这$n$维的参数进行正则化，并不包括$x_0$和$\theta_0$</mark>。

   ```matlab
   % X_grad
   for i = 1 : size(X, 1) % 遍历每一个电影
   	idx = find(R(i, :) == 1); % 每部电影沿用户方向查找有用户评价的位置
   	Theta_temp = Theta(idx, :); % 取R(i, :)中为1的用户对应的theta
   	Y_temp =Y(i, idx) % 取R(i, :)中为1的Y值
   	X_grad(i, :) = (X(i, :) * Theta_temp' - Y_temp) * Theta_temp;
   	% 正则化
   	X_grad(i, :) += (lambda * X(i, :));
   end

   % Theta_grad
   for j = 1 : size(Theta, 1) % 遍历每一个用户
   	idx = find(R(:, j) == 1); % 每个用户沿电影方向查找有用户评价的位置
   	X_temp = X(idx, :);
   	Y_temp =Y(idx, j) % 取R(i, :)中为1的Y值
   	Theta_grad(j, :) = (X_temp * Theta(j, :)' - Y_temp)' * X_temp;
   	% 正则化
   	Theta_grad(j, :) += (lambda * Theta(j, :)); 
   end
   ```

3. 最终，通过用户对应的参数向量$\theta$和训练得出的特征向量$x$，来预测用户给出的评分$\theta^Tx$。

### 低秩矩阵分解

将用户评分数据存储到矩阵$Y$中，$Y^{(i,j)}$代表第$i$行第$j$列的数据，即第$i$部电影的第$j$个用户的评分：
$$
Y = 
\begin{bmatrix}
5  &  5 & 0 & 0   \\
5  &  ? & ? & 0   \\
?  &  4 & 0 & ?   \\
0  &  0 & 5 & 4   \\
0  &  0 & 5 & 0  
\end{bmatrix}
$$
同时，也可以得出下面预测评分矩阵，其中$(\theta^{(j)})^Tx^{(i)}$代表第$i$个电影中第$j$个用户，预计给出的评分：
$$
Y = 
\begin{bmatrix}
(\theta^{(1)})^Tx^{(1)}  &  (\theta^{(2)})^Tx^{(1)} & \dots & (\theta^{(n_u)})^Tx^{(1)}   \\
(\theta^{(1)})^Tx^{(2)}  &  (\theta^{(2)})^Tx^{(2)} & \dots & (\theta^{(n_u)})^Tx^{(2)}   \\
\vdots  &  \vdots & \vdots & \vdots   \\
(\theta^{(1)})^Tx^{(n_m)}  &  (\theta^{(2)})^Tx^{(n_m)} & \dots & (\theta^{(n_u)})^Tx^{(n_m)}   \\
\end{bmatrix}
$$
这个预测矩阵可以看做是**电影的特征矩阵**和**用户的参数矩阵的转置**的乘积:

电影矩阵：
$$
X =
\begin{bmatrix}
(x^{(1)})^T   \\
(x^{(2)})^T   \\
\vdots   \\
(x^{(n_m)})^T  
\end{bmatrix}
$$
用户参数矩阵：
$$
\Theta =
\begin{bmatrix}
(\theta^{(1)})^T   \\
(\theta^{(2)})^T   \\
\vdots   \\
(\theta^{(n_u)})^T  
\end{bmatrix}
$$

#### 其他功能

相关电影推荐：假设想找到和电影$i$最类似的一部电影，有一种方式就是求出所有电影的特征向量，然后和电影$i$的特征向量求欧氏距离，距离最小的那个就是最类似的：
$$
Small \lVert x^{(i)} - x^{(j)}\rVert \to 电影i和电影j最类似
$$

#### 均值归一化

要做的就是计算每个电影所得评分的均值，用向量$\mu$表示，然后将所有评分减去平均评分，得到新的$Y$，这样做的目的，**就是把每部电影的评分的平均数都调整为0**。

最终预测用户给出的分值是：
$$
(\theta^{(j)})^Tx^{(i)} + \mu_i
$$






