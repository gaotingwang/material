[TOC]

算法改进方向：

1. 通过使用更多的训练样本
2. 尝试选用更少的特征集，防止过拟合
3. 尝试选用更多的特征集
4. 尝试增加多项式特征的方法($x_1^2,x_2^2,x_1x_2,etc.$)
5. 增大正则化参数$\lambda$
6. 减小正则化参数$\lambda$

机器学习诊断法(Machine learning diagnostic)
---

诊断法是一种测试法，通过执行这种测试能够深入了解某种算法到底是否有用，并且也可以告诉你如何改进算法的效果。

### 评估假设函数

为了确保可以评估我们的假设函数，需要对测试数据进行拆分，第一部分（70%）将成为我们的**训练集(Training Set)**，第二部分（30%）将成为我们的**测试集(Test Set)**。

将所有数据按照7:3的比例划分，是一种常见的划分比例，$m$依然表示训练样本的总数，$m_{test}$表示测试样本的总数，$_{test}$表示这些样本是来自测试集。

#### 评估步骤

- 线性回归

  1. 需要对训练集进行学习，得到参数$\theta$

     <mark>就是最小化训练误差$J(\theta)$。这里的是使用那70%的数据训练得出的结果</mark>。

  2. 计算出测试误差
     $$
     J_{test}(\theta) = \frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)}) - y_{test}^{(i)})^2
     $$
     <mark>使用$J_{test}(\theta)$来表示测试误差</mark>，我们要做的是取出之前从训练集中学习得到的参数$\theta$带入到$J_{test}(\theta)$来计算测试误差。

- 分类问题

  对于分类问题，比如说逻辑回归，与上述非常类似

  1. 首先要从训练数据中（前70%）学习得到参数$\theta$

  2. 然后用下面的式子计算测试数据的误差值
     $$
     J_{test}(\theta) = -\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}[y_{test}^{(i)}log(h_\theta(x_{test}^{(i)})) + (1-y_{test}^{(i)})log(1- h_\theta(x_{test}^{(i)})]
     $$
     在分类问题中的测试误差$J_{test}(\theta)$其实也被称作**错误分类率**（也被称为**0/1错分率**）。表示预测到的正确或错误样本的情况。

     定义误差预测函数$err(h_\theta^{(x)},y)$:
     $$
     err(h_\theta^{(x)},y) =\{
     \begin{eqnarray*}
     &&1\ if\ h_\theta(x) \geq 0.5, y=0\ or\  h_\theta(x) < 0.5,y=1\\
     &&0 \ otherwise
     \end{eqnarray*}
     $$
     基于错误分类的二进制0或1错误结果，试集的平均测试误差是：
     $$
     Test \ error = \frac{1}{m_{test}}\sum_{i=1}^{m_{test}} err(h_\theta^{(x)},y)
     $$





#### 多项式模型的选择以及训练集/验证集/测试集的划分

如果想要确定对于某组数据**最合适的多项式次数**是几次，怎样选用正确的特征来构造学习算法，这些问题称之为**”模型选择问题“**。

假设$d$代表我们应该选择的多项式的次数，除了需要确定参数$\theta$之外，还要考虑如何确定这个多项式的次数$d$。因此，需要确定参数$d$最适当的取值。

由于我们使用的是**测试结果集**来衡量的代价函数，从而得到多项式次数$d$这个参数的值，但是这样仍然不能公平地说明这个假设可以推广到一般情况。（因为**现在找到的可能是一个最能拟合测试集的参数$d$**）。再用测试集来评价假设就显得不公平。

为了解决这一问题在模型选择中，通常采用以下的方法：给定某个数据集将其分为三段：**训练集(Training Set)**、**交叉验证集（cross validation set）**、**测试集(Test Set)**。

一种典型的分割比例是：训练集60%，交叉验证集20%，测试集20%（这个比例可稍作调整）。$m$表示训练样本的总数，$m_{cv}$表示交叉验证集个数，$m_{test}$表示测试集个数。
$$
J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2
$$

1. 即通过使用**训练集**对每一个假设函数依次去求最小化的代价函数$minJ(\theta)$，并求得对应的参数向量$\theta^{(d)}$。
2. 然后做的是在**交叉验证集**中测试这些假设的表现，测出$J_{cv}$来看看这些假设在交叉验证集中表现如何。
3. 这个过程中，我们没有使用**测试集**进行拟合，这样就回避了**测试集**的嫌疑。然后就可以光明正大的使用**测试集**来估计所选模型的泛化误差了。

<mark>最佳做法还是把数据分成 训练集、验证集、测试集。</mark>

### 偏差 VS 方差

当运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大、要么是方差比较大。即出现的情况要么是欠拟合、要么是过拟合问题。

<mark>高偏差(high bias)是欠拟合，高方差(high variance)是过拟合。理想的情况下，我们需要找到这两者之间的中庸之道。</mark>

哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关，搞清楚这一点非常重要。因为这两种情况其实是一个很有效的指示器，指示着改进算法最有效的方法和途径。

![多项式次数与偏差方差](http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%AC%A1%E6%95%B0%E4%B8%8E%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE.png)

从上图可以看出随着多项式次数的增多，训练集误差呈下降趋势，验证集误差呈先降后升的趋势。

#### 如何分辨算法处于偏差还是方差

上图中左边点表示偏差(bias)情况，右侧点表示方差(variance)情况。

- 偏差：<mark>训练集误差和验证集误差都很大</mark>，两者误差都很接近
- 方差：<mark>交叉验证集误差远远大于训练集误差</mark>，预示着算法正处于高方差和过拟合的情况

#### 正则化与偏差/方差

- **一种正则化参数$\lambda$取一个比较大的值（比如$\lambda$的值取为10000甚至更大）**

  在这种情况下，所有参数$\theta_i$将被大大惩罚，其结果是这些参数的值将近似于等于0，并且假设模型$h(x)$的值将等于或者近似等于$\theta_0$。

- **另一种情况是$\lambda$值很小（比如说$\lambda$的值等于0）**

  这种情况下，$\lambda$的值等于0，相当于没有正则化项，如果要拟合一个高阶多项式的话，通常会处于过拟合的情况。

- **只有取一个中间大小的，既不大也不小的$\lambda$值才会得到一组合理的对数据刚好拟合的$\theta_0$参数值。**

需要注意最初的代价函数$J(\theta)$是包含正则化项的，但在这里<mark>训练误差和交叉验证集误差定义为不包括正则化项</mark>。
$$
\begin{eqnarray*}
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m}\sum_{j=1}^m\theta^2 \\
J_{train}(\theta) = \frac{1}{2m_{train}}\sum_{i=1}^{m_{train}}(h_\theta(x_{train}^{(i)}) - y_{train}^{(i)})^2 \\
J_{cv}(\theta) = \frac{1}{2m_{cv}}\sum_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2
\end{eqnarray*}
$$
当$\lambda$的取值很小时，对训练集的拟合相对较好，因为没有使用正则化。而如果$\lambda$的值很大时，将处于高偏差问题(测试集误差很大)，不能对训练集很好地拟合，训练集误差$J_{train}(\theta)$的值会趋于上升。那么此时交叉验证集误差将也会很大。我们的假设不能在交叉验证集上表现地比较好。

![lambda与偏差方差](http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/lambda%E4%B8%8E%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE.jpeg)



```matlab
function [lambda_vec, error_train, error_val] = validationCurve(X, y, Xval, yval)

% Selected values of lambda (you should not change this)
lambda_vec = [0 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10]';

% You need to return these variables correctly.
error_train = zeros(length(lambda_vec), 1);
error_val = zeros(length(lambda_vec), 1);

for i = 1 : length(lambda_vec)
	lambda = lambda_vec(i);
	theta = trainLinearReg(X, y, lambda);
	error_train(i) = linearRegCostFunction(X, y, theta, 0);
	error_val(i) = linearRegCostFunction(Xval, yval, theta, 0);
end

end


% 开始绘图
[lambda_vec, error_train, error_val] = validationCurve(X_poly, y, X_poly_val, yval);

close all;
plot(lambda_vec, error_train, lambda_vec, error_val);
legend('Train', 'Cross Validation');
xlabel('lambda');
ylabel('Error');
```

当我们改变正则化参数$\lambda$的值时，交叉验证集误差和训练集误差随之发生的变化。当然，在中间取的某个$\lambda$的值，表现得刚好合适，这种情况下表现最好，交叉验证集误差或者测试集误差都很小。

### 学习曲线(Learning Curves)

很显然，如果训练集m很小，那么很容易就能把训练集拟合到很好，甚至完全拟合。事实上随着训练集容量的增大，平均训练误差是逐渐增大的，如果画出这条曲线就会发现**，训练集误差对假设进行预测的误差平均值随着m的增大而增大**。

交叉验证集误差是对完全陌生的交叉验证集数据进行预测得到的误差，当训练集很小的时候，泛化程度不会很好（意思是不能很好的适应新样本）。因此这个假设就不是一个理想的假设，只有当使用一个更大的训练集时，才有可能得到一个能够更好拟合数据的可能的假设。<mark>因此验证集误差误差都会随着训练集样本容量m的增加而减小</mark>，因为用的数据越多，获得的泛化能力就越强，或者说对新样本的适应能力就越强。因此数据越多，越能拟合出合适的假设。

<img src="http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.png" width="450px" alt="学习曲线"/>

```matlab
function [error_train, error_val] = learningCurve(X, y, Xval, yval, lambda)

% Number of training examples
m = size(X, 1);

% You need to return these values correctly
error_train = zeros(m, 1); % 训练集误差
error_val   = zeros(m, 1); % 交叉验证集误差


% 样本数从1到m分别计算对应的训练误差和交叉验证误差
for i = 1:m
	X_test = X(1:i, :);
	y_test = y(1:i);
	% 样本数为i时，通过高级梯度下降计算出theta
	[theta] = trainLinearReg(X_test, y_test, lambda); 
	% lambda传入0计算训练误差J_train
	error_train(i) = linearRegCostFunction(X_test, y_test, theta, 0); 
	% lambda传入0计算交叉验证误差J_cv
	error_val(i) = linearRegCostFunction(Xval, yval, theta, 0); 
end

end

lambda = 0;
[error_train, error_val] = learningCurve([ones(m, 1) X], y, ...
                  [ones(size(Xval, 1), 1) Xval], yval, ...
                  lambda);

plot(1:m, error_train, 1:m, error_val);
title('Learning curve for linear regression')
legend('Train', 'Cross Validation')
xlabel('Number of training examples')
ylabel('Error')
axis([0 13 0 150])
```

#### 高偏差下的学习曲线

当样本数量增多的时候，直线是对这组数据最接近的拟合，但一条直线再怎么接近，也不可能对这组数据进行很好的拟合。

训练误差一开始也是很小的，而在高偏差的情形中，你会发现训练集误差会逐渐增大，一直趋于接近交叉验证集误差，这是因为你的参数很少。但当m很大的时候，数据太多，此时训练集和交叉验证集的预测结果将会非常接近：

![高偏差学习曲线](http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E9%AB%98%E5%81%8F%E5%B7%AE%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.png)

<mark>高偏差的情形反映出的问题是交叉验证集和训练集误差都很大，也就是说，最终会得到一个值比较大的$J_{cv}(\theta)$和$J_{train}(\theta)$。</mark>

**所以如果学习算法正处于高偏差的情形，那么选用更多的训练集数据对于改善算法表现无益。**

#### 高方差下的学习曲线

假设使用一个很小的$\lambda$，很显然会对这组数据拟合的非常好，如果训练集样本容量很小时，训练集误差$J_{train}(\theta)$将会很小，随着训练集样本容量的增加，可能这个假设函数任然会对数据或多或少有一点过拟合，但很明显此时要对数据很好地拟合显得更加困难和吃力了，所以随着训练集样本容量的增大，我们会发现$J_{train}(\theta)$的值会随之增大，因为当训练样本越来越多的时候，我们就越难将训练集数据拟合得很好，但总体来说，训练集误差还是很小

在高方差的情形中，假设函数对数据过拟合，因此交叉验证集误差将会一直都很大，即便我们选择一个比较合适恰当的训练集样本数：

![高方差学习曲线](http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E9%AB%98%E6%96%B9%E5%B7%AE%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.png)

**算法处于高方差情形最明显的一个特点是在训练集误差和交叉验证集误差之间以一段很大的差距**，这个曲线也反映出如果考虑增大训练集的样本数，这两条学习曲线会逐渐靠近，**高方差情形下使用更多的样本数量对改进算法的表现事实上是有效果的。**

<mark>在改进一个学习算法的时候，通常要先画出这些学习曲线。这项工作会让你更轻松地看出偏差或方差的问题。</mark>

### 使用方案

使用正则化的线性回归拟合模型，却发现该算法没有达到预期效果。我们提到有如下选择方案：

1. 通过使用更多的训练样本

   <mark>对于**高方差问题**是有帮助的</mark>

2. 尝试选用更少的特征集，防止过拟合

   <mark>对于**高方差问题**是有帮助的</mark>

   模型处于高偏差问题，那么切记千万不要浪费时间视图从已有的特征中挑出一小部分来使用。

   如果你发现你的算法处于高方差的情形，那么你需要花一点时间来挑选出一小部分合适的特征，这是把时间用在了刀刃上的。

3. 尝试选用更多的特征集

   <mark>**一般可以帮助解决高偏差问题**</mark>

   一般是由于现有的假设函数太简单，因此我们才决定增加一些别的特征来让假设函数更好地拟合训练集。

4. 尝试增加多项式特征的方法($x_1^2,x_2^2,x_1x_2,etc.$)

   <mark>**修正高偏差问题**</mark>

5. 增大正则化参数$\lambda$，<mark>**修正高方差问题**</mark>

6. 减小正则化参数$\lambda$，<mark>**修正高偏差问题**</mark>

#### 方案选取总结

偏差/方差区分及对应方案：

- 偏差表现：1.多项式次数越小；2.$\lambda$越大；3.学习曲线中训练集误差和交叉验证集误差之间差距小

  应对方案：采用更多特征值(也可以增加多项式次幂)、减小正则化参数$\lambda$

- 方差表现：2.多项式次数越大；2.$\lambda$越小；3.学习曲线中训练集误差和交叉验证集误差之间差距大

  应对方案：使用更多样本、减少特征数、增大正则化参数$\lambda$

### 偏方差与神经网络关系

结构简单的神经网络参数就不会很多，很容易出现**欠拟合**。这种比较小型的神经网络其最大优势在于计算量较小。

**经常应用神经网络，特别是大型神经网络的话，你就会发现越大型的网络性能越好，但如果发生了过拟合，你可以使用正则化的方法来修正过拟合。**

**默认的情况是使用一个隐藏层。但是如果确实想要选择多个隐藏层，也可以试试把数据分割为训练集、验证集和测试集，然后使用交叉验证的方法比较一个隐藏层的神经网络然后试试两个三个隐藏层以此类推，然后看看哪个神经网络在交叉验证集上表现得最理想，然后对每一个模型都用交叉验证集数据进行测试，算出三种情况下（隐藏层分别为一层、两层、三层）的交叉验证集误差$J_{cv}(\theta)$，然后选出认为最好的神经网络结构。**



机器学习系统设计
---

构建一个学习算法的推荐方法为:

1. **构建一个简单能快速实现的算法**,并用交叉验证集数据测试这个算法
2. **绘制学习曲线**,决定是增加更多数据,或者添加更多特征,还是其他选择
3. **进行误差分析**:人工检查交叉验证集中我们算法中产生预测误差的实例(哪些数据被错误分类，分析是否缺少某些特征等)，看看这些实例是否有某种系统化的趋势

假设有一个快速而**不完美的算法实现**,又有一个**数值评估数据**(即交叉验证错误率),这会帮助你尝试新的想法,快速地发现这些想法是否能够提高算法的表现,从而会更快地做出决定。

### 偏斜类(skewed classes)问题

正样本的数量与负样本的数量相比，非常非常少。比如`y=1`的情况非常少，把这种情况叫做**偏斜类**。

说明：如果有一个偏斜类，用分类精确度并不能很好地衡量算法。虽然可能会获得一个很高的精确度，非常低的错误率。但是我们并不知道是否真的提升了分类模型的质量，因为模型都是在预测`y=0`，实际对`y=1`的情况并不能很好的预测。虽然仅预测`y=0`会将误差降低至`0.5%`。

#### 查准率（precision）和召回率（recall）

假设每个测试集中的样本都会等于0或者1，学习算法会为每一个测试集中的实例做出预测，预测值也是等于0或1。

<img src="http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E6%9F%A5%E5%87%86%E7%8E%87%E4%B8%8E%E5%8F%AC%E5%9B%9E%E7%8E%87.png" width="450px" alt="查准率与召回率"/>

<mark>在使用中将`y=1`作为某种我们希望检测的出现较少的类</mark>：

**查准率**：
$$
\frac{真阳性数量}{预测值为阳性数量} = \frac{真阳性数量}{真阳性数量 + 假阳性数量}
$$
高查准率说明对于为1的分类在预测情况下有很高的准确率。

**召回率**：
$$
\frac{真阳性数量}{实际阳性数量} = \frac{真阳性数量}{真阳性数量 + 假阴性数量}
$$

```matlab
% 真阳性
tp = sum((predictions == 1) & (yval == 1));
% 假阳性
fp = sum((predictions == 1) & (yval == 0));
% 假阴性
fn = sum((predictions == 0) & (yval == 1));
% 查准率
prec = tp / (tp + fp);
% 召回率
rec = tp / (tp + fn);
```

高召回率说明对于为1的分类在真实情况下有很高的准确率。

如果一个分类模型拥有高查准率和召回率，那么可以确信地说这个算法表现很好，即便它是一个很偏斜的类。

<mark>对于偏斜类的问题，查准率和召回率给予了更好的方法来检测学习算法表现如何，这是一种更好地评估学习算法的标准</mark>。当出现偏斜类时，比仅仅只用分类误差或者分类精度好。

如果$h_\theta(x)  \geq 0.5 $预测为1，$h_\theta(x)  < 0.5 $预测为0，称0.5为临界值(Threshold)，通过变动临界值可以控制权衡查准率和召回率，临界值的改变与查准率和召回率的关系：

<img src="http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/F1Score%E4%B8%B4%E7%95%8C%E5%80%BC.png" width="350px" alt="F1Score临界值"/>

如何权衡查准率和召回率，可以变动临界值，使得$F_1Score(F Score)$偏向于1：
$$
F_1Score:2\frac{PR}{P+R}
$$
**F1Score**给出了选取查准率和召回率的有效方法，因为无论是查准率等于0，还是召回率等于0，它都会得到一个很低的**F1Score**。因此，如果要得到一个很高的**F1Score**，算法的查准率和召回率都要接近于1。具体地说，如果P=0或者R=0，**F1Score**等于0。

在自动选择临界值来决定你希望预测$y=1$还是$y=0$，那么一个比较理想的办法是<mark>试一试不同的临界值，然后评估这些不同的临界值在**交叉检验集上**进行测试，然后选择哪一个临界值能够在交叉检验集上得到最高的F1Score</mark>，这是自动选择临界值的较好办法。

### 大数据集

<mark>**在特征值x包含足够多的用来准确预测y的信息时，获取大量的数据是提高算法性能的好方法。**</mark>(更通俗理解是在学习曲线为高方差的时候来获取更多数据)，通常情况下，先判断作为人类专家是否可以根据已知特征有信心预测出y值。



总结
---

<mark>学习机器学习算法是很重要，但更最重要的是：</mark>

1. **<mark>有多少数据</mark>**
2. **<mark>是否擅长做误差分析</mark>**（分析错误数据，为什么会产生错误，是缺少特征造成的，还是其他）
3. **<mark>是否熟练调试学习算法</mark>**
4. **<mark>想出如何设计新的特征变量(找出其他特征)</mark>**

