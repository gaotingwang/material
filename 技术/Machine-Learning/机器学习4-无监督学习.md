[TOC]

无监督学习
---

在无监督学习案例中，是对一组**无标记**的训练数据，数据之间不具任何关联的标记。将这种未标记的训练数据送入特定的算法，然后要求算法分析出数据的结构。这种划分的算法称为**聚类算法**。

### K-Means算法

K均值是一个迭代方法，它要做两件事情：

- 第一是**簇分配**(cluster assignment)
- 第二个是**移动聚类中心**(move cluster centroids)

K均值算法接受两个输入：

1. 参数$K$，表示想从数据中聚类出的簇的个数
2. 第二个输入参数是训练集$\{x^{(1)},x^{(2)},\ldots,x^{(m)}\}$

#### K-Means算法步骤

1. 随机初始化$K$个**聚类中心**，记作$\mu_1,\mu_2$一直到$\mu_K$。

2. 内部循环执行以下步骤：

   - 簇分配

     首先对于每个训练样本，用变量$c^{(i)}$表示$K$个聚类中心中最接近$x^{(i)}$的那个中心的下标，这就是簇分配。
     $$
     c^{(i)} = min_k = ||x^{(i)} - \mu_k||
     $$
     <mark>相当于给样本分别打上1~$k$的标记</mark>。

   - 移动聚类中心

     对于每个聚类中心：$k$从1循环到$K$，将$μ_k$赋值为这个簇的均值，之后重新开始簇分配。

如果存在一个没有点分配给它的聚类中心，那怎么办? 通常在这种情况下，就直接移除那个聚类中心，变为$K-1$个簇。如果仍需要$K$个簇，需要做的是重新随机找一个聚类中心。

#### 优化目标

$\mu_{c^{(i)}}$表示$x^{(i)}$所属的簇的聚类中心，关于K-Means的代价函数表示为：
$$
J(c^{(1)},\ldots,c^{(m)},\mu_1,\ldots,u_K) = \frac{1}{m}\sum_{i=1}^{m}||x^{(i)} - u_{c^{(i)}}||^2
$$
代价函数要做的就是找到使$J$最小的$c^{(i)}$和$\mu_i$，在K-Means算法中，也叫失真代价函数(distortion cost function)。

K-Means算法就是在做寻求$J$最小，拆开为两部分考虑：簇分配是将各点划分到离聚类中心最小的范围，聚类中心移动是寻找使各点到聚类中心最小的聚类中心。

#### 簇数量$K$选择

选择聚类的数目可能不总是那么容易，大部分情况下，对于数据集中有多少个聚类中心通常是模棱两可的。这就是无监督学习的一部分。没有给我们标签，所以不会总有一个清晰的答案。这就是为什么，做一个能够自动选择聚类数目的算法，是非常困难的原因之一。

- 肘部法则(Elbow Method)

  <img src="http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E8%82%98%E9%83%A8%E6%B3%95%E5%88%99.png" width="400px"/>

  在$K=3$的时候，到达一个肘点。此后畸变值就下降得非常慢。这样看起来，也许使用3个聚类数目是正确选择。

  **局限性**：也许得到的曲线没有一个清晰的肘点，而畸变值像是连续下降的，也许3是一个好选择，也许4是一个好选择，也许5也不差。如果实际情况中，遇到的肘点的位置并不明确，这使得用这个方法来选择聚类数目变得较为困难。

- 下游决定

  由需求决定多少个分类更有意义，能更好的满足需求。

#### 聚类中心选择

随机挑选$K$个训练样本作为聚类中心。

对于聚类中心的选择采用随机初始化的方式，首先要<mark>确保$K<m$</mark>。

为了避免局部最优的问题，需要**初始化K均值很多次，并运行K-Means方法很多次，通过多次尝试来<mark>选取$J$最小的情况</mark>，从而保证最终能得到一个足够好的结果。一个尽可能局部或全局最优的结果。**

事实证明，如果运行K均值方法时，所用的聚类数相当小。比如聚类数是从2到10之间的任何数的话，做多次的随机初始化，通常能够保证能有一个较好的局部最优解。但是如果K非常大的话，比如K比10大很多，就不太可能会有太大的影响。事实上，这种情况下有可能第一次随机初始化就会给出相当好的结果。

