[TOC]

神经网络(Neural Networks)
---

### 神经网络介绍

![神经网络架构图](http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%9B%BE.png)

逻辑回归不能形成更复杂的假设，因为Sigmoid函数$g(z)$中$z = \theta X$，所以它只是一个线性分类器。<mark>神经网络能够形成非线性假设的复杂模型</mark>。
$$
\begin{bmatrix}
   x_0  \\
   x_1  \\
   x_2  \\
   x_3
\end{bmatrix}
\rightarrow
\begin{bmatrix}
   a_1^{(2)}  \\
   a_2^{(2)}  \\
   a_3^{(2)}
\end{bmatrix}
\rightarrow h_\theta(x)
$$
在神经网络中$g(z)$叫**激励函数(activation funcation)**，参数$\Theta$叫权重。

输入节点第一层也被称为“**输入层 input layer**”，进入另一个节点，最终输出假设函数，称为“**输出层 output layer**”。在输入层和输出层之间有一个称为“**隐藏层 hidden layer**”的中间节点层，**在神经网络中，任何一个非输入层且非输出层，就被称为隐藏层**。这些中间层或“隐藏的”层节点$a_0^{(2)} \cdots a_n^{(2)}$并称为“**激活单位 activation units**”。

- $a_i^{(j)}$ 表示第$j$层的第$i$个**激励 activation**，所谓激励是指由一个具体神经元读入计算并输出的值
- <mark>每一层的$a_i$其实就是之前的特征变量</mark>
- $\Theta(j)$ 表示$j$层与下一层之间权重的**参数矩阵**(或者叫**权重矩阵**)
- 需要为每一层加上值永远为1的**偏置单元 bais unit** $a_0^{(j)}$

$$
a^{(2)}_1 = g(\Theta^{(1)}_{10}x_0 + \Theta^{(1)}_{11}x_1 + \Theta^{(1)}_{12}x_2 + \Theta^{(1)}_{13}x_3) \\
a^{(2)}_2 = g(\Theta^{(1)}_{20}x_0 + \Theta^{(1)}_{21}x_1 + \Theta^{(1)}_{22}x_2 + \Theta^{(1)}_{23}x_3) \\
a^{(2)}_3 = g(\Theta^{(1)}_{30}x_0 + \Theta^{(1)}_{31}x_1 + \Theta^{(1)}_{32}x_2 + \Theta^{(1)}_{33}x_3) \\
h_\Theta(x) = a^{(3)}_1 = g(\Theta^{(2)}_{10}a^{(2)}_0 + \Theta^{(2)}_{11}a^{(2)}_1 + \Theta^{(2)}_{12}a^{(2)}_2 + \Theta^{(2)}_{13}a^{(2)}_3)
$$

更一般的，如果一个神经网络在第$j$层有$S_j$个单元，在第$j+1$层有$S_{j+1}$个单元，那么<mark>第$j$层的权重$\Theta^{(j)}$的维度是：$S_{j+1} × (S_j+1)$</mark>。

### 向前传播(forwar propagation)

计算$h_\Theta(x) $的过程，称为**前向传播(forward propagation)**:

定义特征向量$x$，其中$x_0=1$：
$$
\begin{bmatrix}
   x_0  \\
   x_1  \\
   x_2  \\
   x_3
\end{bmatrix}
$$
定义$z^{(2)}$：
$$
z^{(2)} = 
\begin{bmatrix}
   z_1^{(2)}  \\
   z_2^{(2)}  \\
   z_3^{(2)}
\end{bmatrix}
$$
则$a^{(2)}$：
$$
z^{(2)} = \Theta^{(1)}a^{(1)} \\
a^{(2)} = g(z^{(2)})
$$
最后求$h_\Theta(x) $：
$$
z^{(3)} = \Theta^{(2)}a^{(2)} \\
h_\Theta(x)  = a^{(3)} = g(z^{(3)})
$$

### 神经网络的非线性假设的原理

神经网络的最后一层看起来很像逻辑回归，在逻辑回归中，我们就用这一个逻辑回归单元来预测$h(x)$的值。不同之处在于这里使用的是大写的$\Theta$而不是小写的$\theta$，神经网络的输入特征值是通过隐藏层计算的。即神经网络所做的工作看起来就像是逻辑回归，但是它不是使用$x_1$、$x_2$、$x_3$作为输入特征，而是使用$a^{(2)}_1$、$a^{(2)}_2$、$a^{(2)}_3$。

特征项$a^{(2)}_1$、$a^{(2)}_2$、$a^{(2)}_3$是通过输入的函数来学习的。具体来说，就是从第一层(Layer1)映射到第二层(Layer2)的函数。这个函数由第一组参数$\Theta^{(1)}$决定。所以，在神经网络中，它没有用输入特征$x_1$、$x_2$、$x_3$来训练逻辑回归，而是训练逻辑回归的输入: $a^{(2)}_1$、$a^{(2)}_2$、$a^{(2)}_3$。

<mark>总结来说$h(x)$是经由第一层的$x_1$、$x_2$、$x_3$做$\theta X$运算，再做$g(z)$运算，又经过$\theta X$运算和$g(z)$运算来得到的，所以$x$在经过多重的神经网络后也就变成了多项式，成为非线性的模型</mark>。

当层数很多的时候，有一个相对简单的输入量的函数作为第二层，而第三层可以建立在此基础上来计算更加复杂一些的函数，然后再下一层，又可以计算再复杂一些的函数。

#### 代价函数

大写字母$L$表示这个神经网络结构的总层数，用$S_l$表示第$l$层的神经元的数量，**这其中不包括$L$层的偏置单元**。

逻辑回归的代价函数：
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i)}))]+ \frac{\lambda}{2m}\sum_{j=1}^{n}{\theta_j}^2
$$
对于神经网络的代价函数是这个式子的一般化形式：
$$
J(\Theta)=-\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^K[y_k^{(i)}\log((h_\Theta(x^{(i)}))_k) + (1-y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k)]+ \frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{S_l}\sum_{j=1}^{S_{l+1}}{(\Theta_{j,i}^{(l)})^2}
$$
理解代价函数：非正则情形下，是对每种$K$值求其对应的逻辑回归的代价函数；正则化是( $\frac{\lambda}{2m} · 所有非偏置单元的平方$)

```matlab
% 1. 先根据前向传播算出h(x)的值
%    先给X添加偏置单元1
X = [ones(m,1) X];

%    算出隐藏层的计算结果
result1 = sigmoid(X * Theta1');
result1 = [ones(m, 1) result1]; % 给隐藏层加偏置单元1

%    计算输出层的结果
result2 = sigmoid(result1 * Theta2');
result = result2;

% 2. 根据预测出的结果h(x)来求代价函数，此处得出结果还不包括则正则化
for k = 1:num_labels
	% 循环体内是对每种K值计算对应的代价函数
	yk = (y == k); % y变成对应k值为1的形式 [0,0,1,0,0]
	resultk = result(:,k); % 标签k的预测结果h(x)
	Jk = (-1 / m) * (yk' * log(resultk) + (1 - yk)' * log(1-resultk));
	J = J + Jk;
end

% 3. 正则化代价函数
%    去除权重里的偏置单元对应的参数，它们不需要参加正则化
noBaisTheta1 = Theta1(: , 2:end);
noBaisTheta2 = Theta2(: , 2:end);

J = J + (lambda / (2 * m)) * (sum(sum(noBaisTheta1 .* noBaisTheta1)) + sum(sum(noBaisTheta2 .* noBaisTheta2)));
```

### <mark>反向传播(Backpropagation)</mark>

反向传播算法的作用是用来求代价函数最小化，最终的目标是计算$min_\Theta J(\Theta)$，要想最小化$J(\Theta)$需要使用$\frac{d}{d\Theta_{j,i}^{(l)}}J(\Theta) $。$\delta_j^{(l)}$记作第$l$层第$j$个节点的误差：

输出层每一项的误差(layer L = 4)：$\delta_j^{(4)} = h_\Theta(x)_j - y_j​$

隐藏层每一项的误差：$ \delta_j^{(3)} = (\Theta^{(3)})\delta_j^{(4)} \cdot \times g'(z^{(3)}) $，其中$ g'(z^{(3)}) = a^{(3)} \cdot  \times(1 - a^{(3)}) $，$ g'(z) $的算法实现：

```matlab
function g = sigmoidGradient(z)

g = sigmoid(z) .* (1 - sigmoid(z));

end
```

需要注意这里<mark>没有$δ^{(1)}$项</mark>，因为第一层是输入层，不存在误差。

如果忽略标准化所产生的项，可以证明想要的偏导项，恰好就是下面这个表达式：
$$
\frac{d}{d\Theta_{j,i}^{(l)}}J(\Theta)  = a_j^{(l)}\delta_i^{(l+1)}
$$
所以到现在，可以通过反向传播计算这些$\delta$项，可以非常快速的计算出所有参数的偏导数项。

#### 具体实现反向传播算法

首先设置$\Delta_{ij}^{(l)} = 0 (for　all　l,i,j)$，$\Delta_{ij}^{(l)}$最终被用来计算$\frac{d}{d\Theta_{j,i}^{(l)}}J(\Theta) $，$\Delta_{ij}^{(l)}$最终是m个样本累加出来的

For  i = 1 to m

1. 取每个样本$x^{(i)}$，令$a^{(1)} = x^{(i)}$
2. 依次计算出$l=2,3,\ldots,L$的$z^{(l)}$和$a^{(l)}$
3. 用$y^{(i)}$计算$\delta^{(L)} = h_\Theta(x) - y^{(i)}$，这里注意<mark>需要**分别**计算$y$变成对应$k$值为1的形式 [0,0,1,0,0]</mark>
4. 依次计算$\delta^{(L-1)},\delta^{(L-2)},\ldots,\delta^{(2)}$
5. $\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}\delta_i^{(l+1)}$，即$\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$

end

正则化之后$D_{ij}^{(l)}$的表示：
$$
\begin{eqnarray*}
D_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} + \frac{\lambda}{m}\Theta_{ij}^{(l)} &　&  (j \neq 0)  \\
D_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} &　&  (j = 0) 
\end{eqnarray*}
$$
具体代码实现：

```matlab
% 1. 正则化之前的渐变
for i = 1:m
	% 设定a(1)，也就是输入层的激励函数：a_1=x(i)
	a_1 = X(i,:); % 已经添加过偏置单元了，直接取
	% 执行向前传播，分别计算出第2层和第3层的激励值(z_2,a_2,z_3,a_3)
	z_2 = a_1 * Theta1';
	a_2 = [1 sigmoid(z_2)]; % 计算前添加偏置单元
	z_3 = a_2 * Theta2';
	a_3 = sigmoid(z_3);
	% 计算输出层的误差
    y_i = zeros(num_labels, 1);
    y_i(y(i)) = 1; % 输出值y变成对应k值为1的形式 [0,0,1,0,0]
    delta_3 = a_3' - y_i;
	% 计算隐藏层的delta不应该包括偏置单元
	delta_2 = (Theta2(: , 2:end))' * delta_3 .* sigmoidGradient(z_2');
	% 计算每一层的渐变
	Theta1_grad += (1 / m) * (delta_2 * a_1);
	Theta2_grad += (1 / m) * (delta_3 * a_2);
end

% 2. 正则化渐变
%    每一层的第一列不参与正则化
Theta1_nobais = [zeros(size(Theta1,1), 1) Theta1(:, 2:end)];
Theta2_nobais = [zeros(size(Theta2,1), 1) Theta2(:, 2:end)];
Theta1_grad = Theta1_grad + (lambda / m) * Theta1_nobais;
Theta2_grad = Theta2_grad + (lambda / m) * Theta2_nobais;
```

#### 反向传播的直观介绍

关于代价函数如果考虑非多类分类(k=1)并且忽略正则化：
$$
cost(t) = y^{(t)}\log(h_\Theta(x^{(t)})) + (1-y^{(t)})\log(1-h_\Theta(x^{(t)}))
$$
误差值实际上是成本函数的导数：
$$
\delta_j^{(l)} = \frac{d}{dz_j^{(l)}}cost(t)
$$
回想一下，我们的导数是与成本函数相切的线的斜率，所以斜率越陡，我们就越不正确。考虑下面的神经网络，看看如何计算一些$\delta_j^{(l)}$：

![反向传播介绍](http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BB%8B%E7%BB%8D.png)
$$
\delta_2^{(2)} = \Theta_{12}^{(2)}\delta_1^{(3)} + \Theta_{22}^{(2)}\delta_2^{(3)} \\
\delta_2^{(3)} = \Theta_{12}^{(3)}\delta_1^{(4)}
$$

### 将参数从矩阵展开成向量

对于神经网络，参数不再是向量而是矩阵，以便在高级最优化步骤中的使用，需要把参数从矩阵展开成向量：

```matlab
thetaVector = [Theta1(:); Theta2(:); Theta3(:);]
deltaVector = [D1(:); D2(:); D3(:)]
```

如果Theta1的尺寸是10x11，Theta2是10x11，Theta3是1x11，那么可以通过`reshape`取回我们的原始矩阵，如下所示：

```matlab
Theta1 = reshape(thetaVector(1:110),10,11)
Theta2 = reshape(thetaVector(111:220),10,11)
Theta3 = reshape(thetaVector(221:231),1,11)

% 通俗写法
Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
                 hidden_layer_size, (input_layer_size + 1));

Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
                 num_labels, (hidden_layer_size + 1));
```

### <mark>梯度检验(Gradient Checking)</mark>

渐变检查将确保我们的反向传播按预期工作。可以用下式近似我们的成本函数的导数：
$$
\frac{d}{d\Theta} \approx \frac{J(\Theta + \varepsilon) - J(\Theta - \varepsilon)}{2\varepsilon}
$$
对于多个矩阵theta，可以近似的求导数相对于$\Theta_j$如下
$$
\frac{d}{d\Theta_j} \approx \frac{J(\Theta_1,\ldots,\Theta_j + \varepsilon,\ldots,\Theta_n) - J(\Theta_1,\ldots,\Theta_j - \varepsilon,\ldots,\Theta_n)}{2\varepsilon}
$$
通常给$\varepsilon$取很小的值，比如可能取$\varepsilon=10^{-4}$，$\varepsilon$的取值在一个很大的范围内都是可行的，实际上，如果你让$\varepsilon$非常小，那么数学上上面的式子实际上就是导数。只是不用想非常非常小的$\varepsilon$，因为可能会产生数值问题，所以我通常让$\varepsilon=10^{-4}$就行。

```matlab
function numgrad = computeNumericalGradient(J, theta)              

numgrad = zeros(size(theta)); % 存放渐变检测后的值
perturb = zeros(size(theta)); % 用于对应位置放置epsilon
epsilon = 1e-4;
% numel求矩阵元素总数
for p = 1:numel(theta)
    % Set perturbation vector
    perturb(p) = epsilon;
    loss1 = J(theta - perturb);
    loss2 = J(theta + perturb);
    % Compute Numerical Gradient
    numgrad(p) = (loss2 - loss1) / (2*epsilon);
    perturb(p) = 0;
end

end

% 具体使用computeNumericalGradient
[cost, grad] = costFunc(nn_params);
numgrad = computeNumericalGradient(costFunc, nn_params);
% Visually examine the two gradient computations.  The two columns you get should be very similar. 
disp([numgrad grad]);
```

计算出numgrad之后，可以检查numgrad≈deltaVector。一旦验证BP算法是正确的，则不需要再次计算numgrad。

### <mark>随机初始化$\Theta$</mark>

在逻辑回归时，初始化所有变量为0是可行的，但在训练神经网络时，这样做是不可行的。

![theta初始化](http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ctheta%E5%88%9D%E5%A7%8B%E5%8C%96.png)

当初始化面这些颜色两两相同的权重时，这些权重都被赋予相同的初始值0，意味着经过计算后，这两个隐藏单元$a_1$，$a_2$的值是相同的；同样的原因，由于权重相同，也可以证明$\delta_1 = \delta_2$，这也就意味着，一旦更新梯度下降，相同颜色的权重的值，在最后将互为相等。因此，即使权重现在不都为0，但参数的值最后也互为相等。所以每次更新后，两个隐藏单元的输入的对应的参数将是相同的。这就意味着即使经过一次梯度下降的循环后，你会发现两个隐藏单元任然是两个完全相同的输入函数$a_1$，$a_2$，这个神经网络并不能计算出什么更有价值的东西。

为了解决这个神经网络变量初始化的问题，采用**随机初始化**的方法。需要做的是对$\Theta_{ij}^{(l)}$的每个值进行初始化，范围在$[−ε,ε]$之间（$−ε≤\Theta_{ij}^{(l)}≤ε$）。

```matlab
Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;

% 抽象方法
function W = randInitializeWeights(L_in, L_out)

epsilon_init = 0.12;
W = rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init;

end
```

### 神经网络算法合体

在多类别分类问题中，若$y$的取值范围是在$1,2,3,\ldots,10$之间，那么就有10个可能的分类，别忘了把$y$重新写成向量的形式。假设现在要表示第5个分类，也就是说$y=5$，那么在神经网络中，就不能直接使用数值5来表达，因为这种情况下，神经网络将有10个输出单元，应该用一个向量来表示:
$$
y = 
\begin{bmatrix}
   0  \\
   0  \\
   0  \\
   0  \\
   1  \\
   0  \\
   0  \\
   0  \\
   0  \\
   0  \\
\end{bmatrix}
$$

#### 训练神经网络步骤

主要分为6个步骤：

1. 构建一个神经网络并且随机初始化$\Theta$（**Randomly initialize Weight**）。

2. 执行向前传播算法，也就是对于神经网络的任意一个输入$x^{(i)}$计算出对应的$h_\Theta(x^{(i)})$。

3. 通过代码计算出代价函数$J(\Theta)$。

4. 执行反向传播算法(**Backprop**)来算出这些偏导数：$\frac{d}{d\Theta_{j,i}^{(l)}}J(\Theta) $。

5. 使用梯度检查来校验结果。用梯度检查来比较这些已经用反向传播算法得到的偏导数值$\frac{d}{d\Theta_{j,i}^{(l)}}J(\Theta) $与用数值方法得到的估计值进行比较，来检查确保这两种方法得到值是基本相近的。

   通过梯度检查，我们能确保我们的反向传播算法得到的结果是正确的，但必须要说明的一点是，<mark>检查结束后需要去掉梯度检查的代码，因为梯度检查计算非常慢</mark>。

6. 使用一个最优化算法（比如说梯度下降算法或者其他更加高级的优化方法，比如说BFGS算法，共轭梯度法，或者其他一些已经内置到`fminunc`函数中的方法），将所有这些优化方法和反向传播算法相结合，这样我们就能计算出这些偏导数项的值$\frac{d}{d\Theta_{j,i}^{(l)}}J(\Theta) $。

```matlab
options = optimset('MaxIter', 50);
lambda = 1;

% Create "short hand" for the cost function to be minimized
costFunction = @(p) nnCostFunction(p, ...
                                   input_layer_size, ...
                                   hidden_layer_size, ...
                                   num_labels, X, y, lambda);

% Now, costFunction is a function that takes in only one argument (the neural network parameters)
[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);
```

对于神经网络代价函数$J(\Theta)$是一个非凸函数，因此理论上是停留在局部最小值的位置。实际上，梯度下降算法和其他一些高级优化方法理论上都能收敛于局部最小值，但一般来讲这个问题其实并不是什么要紧的事，尽管我们不能保证这些优化算法一定会得到全局最优值，但通常来讲，像梯度下降这类的算法在最小化代价函数$J(\Theta)$的过程中，还是表现的很不错的，通常能够得到一个很小的局部最小值，尽管这可能不一定是全局最优值。