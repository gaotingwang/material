[TOC]

降维(dimensionality reduction)
---

用于数据压缩，加快学习算法，以及可视化的复杂数据集。

### 数据压缩

数据压缩不仅通过压缩数据使得数据占用更少的计算机内存和硬盘空间，它还能给算法提速。

当$k \leq n$时(此处的$k$不是聚类个数)，可以使用一个集合$z^{(i)} \in R^k$，即$\{z^{(1)},z^{(2)},\ldots,z^{(m)}\}$来进行降维。

- 2D to 1D

  ![降维-数据压缩1](http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E9%99%8D%E7%BB%B4-%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A91.png)

  在之前如果想要表示一个样本点，需要一个二维向量$(x_1,x_2)$，但是现在可以用一个一维向量$z_1$来表示这个样本点：

  ![降维-数据压缩2](http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E9%99%8D%E7%BB%B4-%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A92.png)

  现在只需要一个数字就可以表示了。这样就减少了一半的内存需求或者硬盘需求。更重要的是还会让我们的学习算法运行地更快。

- 3D to 2D

  降维在这里的作用，就是把所有的数据，都投影到一个二维的平面内。所以，要对所有的数据进行投影，使得它们落在这个平面上：

  ![降维-数据压缩3](http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/%E9%99%8D%E7%BB%B4-%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A93.png)

  把所有的点都投影到一个平面上了，所以现在只需要两个数:$z_1$和$z_2$来表示点在平面上的位置即可

### 可视化数据

对于大多数的机器学习应用，可视化数据真的可以帮助我们来开发高效的学习算法，但前提是我们能更好地理解数据。降维就是数据可视化的一种方法。

如果样本数据的维度很多，使用一个二维的向$z$来代替之前多维的$x$，这样就可以绘制2D的图像了。

当这么做的时候，对于降维算法输出的结果，通常不能赋予这些二维新特征一个物理含义。当绘制出来后，发现$z_1$和$z_2$分别对应很多之前的维度，通过观察$z_1$和$z_2$最为简洁地捕捉到两个维度变量的变化情况。

### 主成分分析PCA

PCA所做的就是**寻找一个低维的面使数据投射在上面，使得到达低维面的距离的平方和达到最小值**。数据到低维面的距离叫做**投影误差**。所以**PCA**所做的就是寻找一个投影平面，对数据进行投影，使得这个能够最小化。

<mark>另外在应用**PCA**之前，通常的做法是先进行**均值归一化**和**特征规范化**</mark>，使得特征均值为0，数值在可比较的范围之内。

假设有$n$维的数据想降到$k$维。在这种情况下我们不仅仅只寻找单个的向量($u^{(1)}$)来对数据进行投影，<mark>需要找到$k$个方向($u^{(k)}$)来对数据进行投影</mark>，从而最小化投影误差。

**PCA**做的就是：**寻找一组$k$维向量(一条直线、或者平面、或者诸如此类等等)对数据进行投影，来最小化正交投影误差。**

#### PCA与线性关系

![PCA与线性关系](http://gtw.oss-cn-shanghai.aliyuncs.com/machine-learning/Stanford/PCA%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%8C%BA%E5%88%AB.png)

在线性回归中，最小化的是竖直距离；而在PCA中最小化的是样本点与直线的垂直距离。

更一般的是在线性回归中，是有标签y的，而在PCA中没有标签y，拥有的只有特征向量。

#### PCA算法实现

- 数据预处理

  1. 均值归一化

     首先应该计算出每个特征的均值$\mu_j = \frac{1}{m}\sum_{i=1}^mx_j^{(i)}$，然后我们用$x_j−\mu_j$来替换掉$x_j$。这样就使得所有特征的均值为0。

  2. 特征缩放

     使用$x_j := \frac{x_j^{(i)}-\mu_j}{s_j}$，这里$s_j$表示特征$j$的度量范围，即该特征的最大值减去最小值。

- 算法实现

  1. 想要把数据从$n$维降低到$k$维，首先要做的是计算出下面这个协方差矩阵(通常用$\sum$来表示)：
     $$
     \sum = \frac{1}{m}\sum_{i=1}^{n}(x^{(i)})(x^{(i)})^T
     $$
     计算出这个协方差矩阵后，假如把它存为Octave中的一个名为`Sigma`的变量，需要做的是计算出`Sigma`矩阵的**特征向量(eigenvectors)**。

     在Octave中，可以使用如下命令来实现这一功能：

     ```
     [U,S,V] = svd(Sigma);
     ```

     svd将输出三个矩阵，真正需要的是$U$矩阵。$U$矩阵也是一个$n×n$矩阵：
     $$
     U = 
     \begin{bmatrix}
        | & | & | & \cdots & |  \\
        u^{(1)} & u^{(2)} & u^{(3)} & \cdots & u^{(n)}  \\
        | & | & | & \cdots & | 
     \end{bmatrix}
     \in R^{n×n}
     $$

  2. 如果想将数据的维度从$n$降低到$k$的话，只需要提取前$k$列向量。这样就得到了$u^{(1)}$到$u^{(k)}$，也就是用来投影数据的$k$个方向，组成的$n×k$的矩阵$U_{reduce}$：
     $$
     U_{reduce} = 
     \begin{bmatrix}
        | & | & | & \cdots & |  \\
        u^{(1)} & u^{(2)} & u^{(3)} & \cdots & u^{(k)}  \\
        | & | & | & \cdots & | 
     \end{bmatrix}
     \in R^{n×k}
     $$

     ```
     Ureduce = U(:,1:k);
     ```

  3. 然后使用$U_{reduce}$来对数据降维：
     $$
     z = 
     \begin{bmatrix}
        | & | & | & \cdots & |  \\
        u^{(1)} & u^{(2)} & u^{(3)} & \cdots & u^{(k)}  \\
        | & | & | & \cdots & | 
     \end{bmatrix}^Tx
     = 
     \begin{bmatrix}
        - & u^{(1)} & -  \\
        - & u^{(2)} & -  \\
        - & u^{(3)} & -  \\
       \vdots & \vdots & \vdots \\
        - & u^{(k)} & - 
     \end{bmatrix}x
     $$

     ```
     z = Ureduce'*x;
     ```

     $z$是$k×1$的矩阵，这里的$x$可以是训练集中的样本，也可以是交叉验证集中的样本，也可以是测试集样本。

#### 选则主成分的数量$k$

$k$是PCA算法的一个参数，称作主成分数量。

PCA所做的是尽量最小化**平均平方映射误差 (Average Squared Projection Error) **，因此PCA就是要将下面这个量最小化：
$$
\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2
$$
同时还要定义一下**数据的总变差(Total Variation)**，它的意思是 “平均来看，训练样本距离零向量（原点）多远”：
$$
\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2
$$
当我们去选择$k$值的时候，通过**平均平方映射误差**除以**数据的总变差**来表示数据的变化有多大。我们想要这个比值能够小于1%（数字0.01是人们经常用的一个值，另一个常用的值是0.05。如果选择了0.05，就意味着95%的差异性被保留了。从95到99是人们最为常用的取值范围。）：
$$
\frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2} \leq 0.01
$$

##### 具体实现：

当你调用`svd`来计算PCA时，会得到三个矩阵`[U,S,V]`，除了之前提到的`U`矩阵之外，`S`矩阵是一个$n×n$的对角矩阵，它只有在对角线上的元素不为0，其余的元素都是0:
$$
S = 
\begin{bmatrix}
 S_{11}   & 0      & \cdots & 0      \\
 0  & S_{22}      & \cdots & 0     \\
 \vdots & \vdots & \ddots & \vdots \\
 0  & 0      & \cdots & S_{nn}      \\
\end{bmatrix}
$$
可以通过这个$S$矩阵方便的计算出差异性那一项的值：
$$
\frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2} = 1 - \frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}}\leq 0.01
$$
即：
$$
\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}}\geq 0.99
$$
可以从1开始，慢慢增大$k$的值，来计算上面这个不等式，直到满足为止即可。通过这种方式，只需要调用一次svd函数，通过`svd`给出的`S`矩阵就可以通过依次增加$k$值的方式来求解了。

$\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}}$会告诉你百分之多少的差异性被保留了下来，这就是一个**平方投影误差的测量指标**。它可以带给你对于数据压缩后是否与原始数据相似带来一种很好的直观感受。

### 对压缩数据还原

根据$z=U_{reduce}^Tx$，如果想得到相反的情形，方程应这样变化:
$$
x_{approx}=U_{reduce}z
$$
为了检查维度，在这里$U_{reduce}$是一个$n×k$矩阵，$z$就是一个$k×1$维向量。将它们相乘得到的就是$n×1$维。所以$x_{approx}$是一个$n$维向量。

同时根据PCA的意图，投影的平方误差不能很大。也就是说$x_{approx}$将会与最开始用来导出$z$的原始$x$很接近。

这就是用低维度的特征数据$z$还原到未被压缩的特征数据的过程，找到一个与原始数据$x$近似的$x_{approx}$ ，称这一过程为**原始数据的重构(reconstruction)**。

### 总结

我们通过在训练集中找到了降维矩阵$U_{reduce}$，就可以<mark>将同样的对应关系应用到其他样本中</mark>了，比如交叉验证数集样本，或者用在测试数据集中。运行PCA的时候，只是在训练集那一部分来进行的，而不是在交叉验证的数据集或者测试集上运行。

在训练集上运行PCA后，得到了从$x$到$z$的映射，然后就可以将这个映射应用到交叉验证数据集，和测试数据集中。将数据降维到原有维度的五分之一或者十分之一，就分类的精确度而言，降维后的数据对学习算法几乎没有什么影响。

#### 错误使用场景

将高维度数据降维处理后，相较于原先的数据，会更不容易出现过拟合的现象。但在这里，<mark>为了解决过拟合问题而使用PCA是不适合的</mark>！如果担心过拟合问题，应该使用正则化方法，而不是使用PCA来对数据进行降维。

如果保留99%的方差，即保留绝大部分的方差，那也是舍弃了某些有用的信息。

对于过拟合问题，正则化效果也会比PCA更好，因为当你使用线性回归或者逻辑回归或其他的方法配合正则化时，这个最小化问题实际就变成了y值是什么，才不至于将有用的信息舍弃掉。然而PCA不需要使用到这些标签，它更容易将有价值信息舍弃。

总之，**使用PCA的目的是加速学习算法，但不应该用它来避免过拟合**。

#### 不要一开始带入PCA

首先需要在原始数据$x^{(i)}$上考虑的问题。并且根据具体情况来分析是否适合使用PCA，还是直接将原始数据带入到学习算法中。

<mark>**一开始不要将PCA方法就直接放到算法里**</mark>，先使用原始数据$x^{(i)}$看看效果。只有一个原因让我们相信算法出现了问题，<mark>那就是学习算法收敛地非常缓慢，占用内存或者硬盘空间非常大，所以想压缩数据。只有当$x^{(i)}$效果不好的时候，那么就考虑用PCA来进行压缩数据</mark>。

#### PCA使用

尽管有这些需要注意的地方，PCA仍旧是一种不可思议的有用的算法。PCA的使用频率也很高，大部分时候都用它来加快学习算法。但PCA通常都是被用来压缩数据以减少内存使用或硬盘空间占用的、或者用来可视化数据的。同时PCA也是一种强有力的无监督学习算法。

