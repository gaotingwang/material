[TOC]

ES的Analyzer（分析器）由Tokenizer（分词器）和Filter（过滤器）组成。

ES对要建索引的文档进行分析，文本被处理前先要做一些预处理， 比如去掉里面的HTML标记， 这些处理的算法被称为**Character Filter(字符过滤器)**。

之后从文档中提取出若干**Token(词元)**， 这些算法称为**Tokenizer(分词器)**。

再然后这些Token会被进一步处理， 比如转成小写，转时态等， 这些处理算法被称为**Token Filter(词元处理器)**。被处理后的结果被称为**Term(词)**， 文档中包含了几个这样的Term被称为**Frequency(词频)**。 

最后引擎会建立Term和原文档的Inverted Index(倒排索引)， 这样就能根据Term很快到找到源文档了。 这整个的分析算法被称为**Analyzer(分析器)**。

mapping映射管理
---

创建索引的时候，预先定义字段类型及属性（创建在_type上） 

### 内置类型

- String: `text`会进行分析，`keyword`不会建立倒排索引不会进行分析
- 数字类型：`long`,`integer`,`short`,`byte`,`double`,`float`
- 日期类型：`date`
- bool类型：`boolean`
- binary类型：`binary` 二进制数据不会被检索
- 复杂类型：`object`,`nested`(对象数组)
- geo类型：地理位置`geo-point`,` geo-shape`
- 专业类型：`ip`,` competion`(搜索建议)

![](https://gtw.oss-cn-shanghai.aliyuncs.com/es/es%E6%98%A0%E5%B0%84.png)

<font color="blue">index下type结构设计最佳实践:同一index下会对type结构做合并操作，所以应该将相似结构的type放在同一index下。如果两个type的field完全不相同，放在同一index下，那么在底层存储时每条记录至少一半的field在Lucene中是空值，会严重影响性能。</font>

### 创建映射

```json
PUT /lagou
{
  "mappings": {
    "job" : { # 对应指定的_type
      "properties" : {
        "title" : {
          "store": true,
          "type" : "text",
          "analyzer": "ik_max_word" # analyzer指定分析器
        },
        "company" : {
          "properties" : {
            "name" : {
              "store": true,
              "type" : "text",
              "index": "not_analyzed" # 不使用分词
           	},
            "addr" : {
              "store": true,
          	  "type" : "keyword" # 不会建立倒排索引不会进行分析
            }
          }
        },
        "desc" : {
          "type" : "text"
        },
        "comment": {
          "type": "integer"
        },
        "add_time" : {
          "type" : "date",
          "format": "yyyy-MM-dd"
        }
      }
    }
  }
}

# 通过/_mapping查看指定_type的mapping情况
GET /lagou/_mapping/job
```

### 定制动态映射模板

```json
PUT /my_index
{
    "mappings":{
        "my_type":{
            # 和properties同级
            "dynamic_templates":[
                {
                    "en":{
            			# 匹配到以en结尾的字段
            			# 假设创建了title_en字段，就会匹配到了dynamic模板，使用english分词器
            			# 会过滤停用词，is这种停用词就会被过滤掉，用is来搜索就搜索不到了
                        "match":"*_en",
                        "match_mapping_type":"string",
                        "mapping":{
                            "type":"string",
                            "analyzer":"english"
                        }
                    }
                }
            ]
        }
    }
}
```

### 更新映射

当创建一个索引的时候，可以指定类型的映射。也可以使用 `/_mapping` 为`_type`（或者为存在的类型更新映射）增加映射。

尽管**==可以增加一个存在的映射，但不能修改存在的域映射==**。如果一个域的映射已经存在，那么该域的数据可能已经被索引。如果意图修改这个域的映射，索引的数据可能会出错，不能被正常的搜索。

比如可以更新一个映射来添加一个新域，但不能将一个存在的域从 `analyzed` 改为 `not_analyzed` 。

```json
PUT /lagou/_mapping/job
{
  # true，陌生字段自动映射；false，陌生字段忽略；strict，陌生字段报错
  "dynamic": "strict",
  # 使用/_mapping 增加一个新的名为 tag 的 not_analyzed 的文本域，
  "properties" : {
    "tag" : {
      "type" :    "text",
      "index":    "not_analyzed",
      # 设置是否要将field的值包含在_all field中
      "include_in_all": false
    }
  }
  "_source": {"enabled": false},
  # 没指定任何field进行搜索时，就是使用_all field在搜索。
  # 将所有field打包在一起，作为一个_all field，建立索引。
  "_all": {"enabled": true}
}
```

上述方法无法用于将原有的如 title域 映射类型改变，如果想要修改只能通过`DELETE /lagou`删除原有映射，然后重建。

### 查看映射

```
GET /lagou/_mapping/job
```



## TF/IDF

- Term frequency：搜索文本中的各个词条在field文本中出现了多少次，出现次数越多，就越相关
- Inverse document frequency：搜索文本中的各个词条在整个索引的所有文档中出现了多少次，出现的次数越多，就越不相关
- Field-length norm：field长度，field越长，相关度越弱



查询理解
---

**==查看分析器解析的结果==**：

```json
# 可以查看分词器将指定文本拆分成什么样
GET /_analyze
{
   "analyzer": "ik_smart", # 指定分析器
   "text": "Python网络" # 要分析的文本内容
}
```

在指定index下查询的分析：

```json
GET lagou/_analyze
{
  "field": "title",
  "text": "Java工程师"
}
```

### 搜索不正确原因定位

对于合法查询，使用 `/_validate/query?explain` 将返回可读的描述，这对准确理解 Elasticsearch 是如何解析的非常有用的：

```json
GET /us,gb/_validate/query?explain
{
    "query": {
        "match": {
            "tweet": "really powerful"
        }
    }
}

{
    "valid": true, 
  	"_shards" : { ... },
    "explanations": [
      	# 每一个 index 都会返回对应的 explanation
        {
            "index": "us", 
            "valid": true, 
            "explanation": "tweet:really tweet:powerful"
        }, 
        {
            "index": "gb", 
            "valid": true, 
            "explanation": "tweet:realli tweet:power" # 字段的分析器为 english 分析器。
        }
    ]
}
```



## 定制分析器

```json
PUT /my_index
{
    "settings":{
        "analysis":{
        	# 设置字符过滤器
            "char_filter":{
                "&_to_and":{
                    "type":"mapping",
                    "mappings":[
                        "&=> and"
                    ]
                }
            },
            # 设置过滤器
            "filter":{
                "my_stopwords":{
                    "type":"stop",
                    "stopwords":[
                        "the",
                        "a"
                    ]
                }
            },
            # 最终指定分析器
            "analyzer":{
                "my_analyzer":{ # 自定义分析器的名称
                    "type":"custom",
                    "char_filter":[
                        "html_strip",
                        "&_to_and"
                    ],
                    "tokenizer":"standard", # 使用标准分词器
                    "filter":[
                        "lowercase",
                        "my_stopwords"
                    ]
                }
            }
        }
    }
}
```

使用自定义的分析器：

```json
PUT /my_index/_mapping/my_type
{
    "properties":{
        "content":{
            "type":"text",
            "analyzer":"my_analyzer"
        }
    }
}

GET /my_index/_analyze
{
    "text":"tom&jerry are a friend in the house, <a>, HAHA!!",
    "analyzer":"my_analyzer"
}
```

### 定制文件路径分析器

```json
PUT /fs
{
    "settings": {
        "analysis": {
            "analyzer": {
                "paths": { # 自定义分析器名称
                    "tokenizer": "path_hierarchy" #路径分析器
                }
            }
        }
    }
}

GET /fs/_analyze
{
    "text":"/a/b/c",
    "analyzer":"paths"
}
```

