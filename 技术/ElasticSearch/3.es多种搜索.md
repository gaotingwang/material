[TOC]

## Query String Search

```json
# 搜索全部内容
GET /ecommerce/product/_search


{
    # 耗费的毫秒数
    "took": 2, 
    # 是否超时
    "timed_out": false, 
    # 扫描分配
    "_shards": {
        "total": 5, 
        "successful": 5, 
        "failed": 0
    }, 
    "hits": {
        # 扫描总结果数
        "total": 3, 
        # 最高得分
        "max_score": 1, 
        "hits": [
            {
                "_index": "ecommerce", 
                "_type": "product", 
                "_id": "2", 
        		# 该条记录得分
                "_score": 1, 
        		# 记录内容
                "_source": {
                    "name": "jiajieshi yagao", 
                    "desc": "youxiao fangzhu", 
                    "price": 25, 
                    "producer": "jiajieshi producer", 
                    "tags": [
                        "fangzhu"
                    ]
                }
            }, 
            ....
        ]
    }
}
```

Query String Search还支持传递请求参数`?q=-field:content`，`-`表示查询field不包含指定内容，默认为`+`。`GET /ecommerce/product/_search?q=-name:yaogao`查询名称中不包含yaogao的商品。

也可以不加field直接进行搜索，`GET /ecommerce/product/_search?q=yaogao`，这样将document的内容整体变为一个content（Text类型），然后对这个content进行分词，做匹配搜索

`_search`查询，默认情况下没有timeout。`GET /ecommerce/product/_search?timeout=10ms`开启timeout机制，每个shard只能在指定的timeout时间内把查询到的结果返回（查到多少返回多少），这样可以为时间敏感的搜索提供良好的支持。



## 基本查询：使用es内置查询条件查询

一个查询语句的典型结构：

```json
GET /_search
{
    "query": {
        # 针对具体字段的查询结构
        QUERY_TYPE_NAME: {
            FIELD_NAME: {
                ARGUMENT: VALUE,
                ARGUMENT: VALUE,...
            }
        }
    } 
}
```

<font color="blue">在多个shard的场景下，可能会出现relevance score不准确问题，这是因为IDF分数根据当前shard上的document数量来的计算。如果生产环境，数据量很大的情况下，数据分布均匀的话，不会出现此情况。在测试环境可以通过设定shard=1，或者在查询时附带search_type=dfs_query_then_fetch参数，会将local IDF取出来计算global IDF，会将所有shard的doc作为上下文来进行计算。</font>

###查询所有文档

```json
# match_all相当于不带条件的全查询
GET /lagou/_search
{
    "query": {
        "match_all": {}
    }
}
```

### match查询

```json
GET /lagou/job/_search
{
    "query": {
      	# 根据分词器进行分词查询
        "match": {
            "title": "python学习"
        }
    },
	# 指定返回的字段
	"_source": ["title", "url"],
	# stored_fields也可以指定返回的字段，但只可以指定stored为true的字段，否则也不会显示
	"stored_fields": ["title","company"],
	# 高亮显示
	"highlight": {
        # 指定高亮需要添加的标签
        "pre_tags": "<span class='keyWord'>",
        "post_tags": "</span>",
        # 指定高亮的字段
        "fields": {
            "title": {},
            "content": {
                # 如果内容较长，不可能全部显示出来，以多个分段的形式展示出来
                # 指定每个分段的的大小，默认100
				"fragment_size" : 150, 
                # 最多显示多少个分段
                "number_of_fragments" : 3
            }
        }
  	},
	# 插叙结果排序
	"sort": [
        { "work_years_min": "desc" }
    ],
	# 分页查询
	"from": 0,
	"size": 10
}
```

如果想对搜索精准度进行控制（多关键字查询底层都会转换成bool查询）：

```json
# 对搜索关键字全部匹配
GET /lagou/job/_search
{
    "query": {
        "match": {
            "title": {
                "query": "java elasticsearch", 
              	# 使用 and 表示搜索出来的结果必须包含java和elasticsearch
                "operator": "and"
            }
        }
    }
}

# 满足75%关键字
GET /lagou/job/_search
{
    "query": {
        "match": {
            "title": {
                "query": "java elasticsearch spark hadoop", 
              	# 满足75%关键字
                "minimum_should_match": "75%"
            }
        }
    }
}
```

如果对一个string field进行排序，结果往往不准确，因为分词后是多个单词，再排序就不是我们想要的结果。通常解决方案是，将一个string field建立两次索引，一个分词，用来进行搜索；一个不分词，用来进行排序

```json
PUT /website 
{
    "mappings":{
        "article":{
            "properties":{
                "title":{
                    "type":"text",
                    "fields":{
                        # 再建一次索引，不使用分词，用于排序
                        "raw":{
                            "type":"string",
                            "index":"not_analyzed"
                        }
                    },
                	# 使用正排索引
                    "fielddata":true
                },
                ...
            }
        }
    }
}

GET /website/article/_search
{
    "query":{
        "match_all":{

        }
    },
    "sort":[
        {	
            # 对string使用新的排序列
            "title.raw":{
                "order":"desc"
            }
        }
    ]
}
```

### term精确查找

match查询时对query内容进行分词，然后对每个分词在倒排索引中查找结果。

term查询时不会对query内容进行分词，**直接使用query内容在倒排索引中查找结果**。

```json
GET /lagou/job/_search
{
    "query": {
        # 如果字段使用默认分词，直接使用Python在倒排索引中查找结果，但此时倒排索引中都是分词后的小写内容
        # 所以使用Python搜索无结果，但python可以
        "term": {
            "content": "Python"
        }
    }
}

GET /lagou/job/_search
{
    "query": {
        "terms": {
      		# 满足数组中的任意一个都会返回
            "title": ["工程师", "python", "java"]
        }
    }
}
```

### filter查询

查询同时，通过filter不影响打分的情况下筛选数据

```json
GET /test/test/_search
{
    "query": {
        "constant_score": {
            "filter": {
                # term 查询被用于精确值匹配
              	# 这些精确值可能是数字、时间、布尔或者那些 not_analyzed 的字符串
                "term": {
                    "tag": "full_text"
                }
            }
        }
    }
}

GET /forum/article/_search
{
    "query" : {
        "constant_score" : { 
            "filter" : {
                "term" : { 
                    # 如果type=text，直接用 "articleID" : "XHDK-A-1293-#fJ3"搜索不到结果
                    # 因为会进行分词，用"articleID":"xhdk"或"1293"可以搜索到结果,但"XHDK"不行；
                    
                    # 如果type=keyword,这时相当于执行sql的=，"articleID" : "xhdk"搜不到结果
                    # 必须为"articleID":"XHDK-A-1293-#fJ3"才能搜到结果；
                    
                    # type=text，新版es默认会设置两个field，一个是field本身，比如articleID
                    # 还有一个就是field.keyword，默认不分词，会最多保留256个字符  
                    
                    # 对于不分词的term搜索，相当于执行sql的=，内容必须全匹配才可以搜到结果，
                    # articleID.keyword不分词，此处若使用"XHDK-A-1293"搜不到结果
                    "articleID.keyword" : "XHDK-A-1293-#fJ3"
                }
            }
        }
    }
}
```

query查询：会计算doc对搜索条件的relevance score，还会根据这个score去排序
filter查询：只是简单过滤出想要的数据，不计算relevance score，也不排序

一个search请求中，可能会发出多个filter条件，会为每个filter条件在倒排索引中搜索到的结果，构建一个bitset，[0, 0, 0, 1, 0, 1]。**遍历每个filter条件对应的bitset，会先从最稀疏的开始遍历**，就可以先过滤掉尽可能多的数据。

filter比query的好处就在于会caching，但实际上缓存的并不是一个filter返回的完整的document list数据结果。而是将filter对应的bitset缓存起来，这样下次就不用扫描倒排索引了，可以大幅提升性能。

如果document有新增或修改，那么cached bitset会被自动更新。以后只要是有相同的filter条件的，会直接使用这个过滤条件对应的cached bitset。

### multi_match查询

```json
# 某一个field中匹配到了尽可能多的关键词，被排在前面；
# 与一般多字段匹配不同，多字段匹配得分是根据每个query得分*fetch query数量/总query数量，最终是让尽可能多的field参与到分数计算中，匹配到field最多的记录得分最高
# dis_max默认取得分最高的那条query,所以会查询出包含关键字最多的字段的doc得分最高
GET /forum/article/_search
{
    "query": {
        "dis_max": {
            "queries": [
                # 设置不同的query
                { "match": { "title": "java beginner" }}, 
                { "match": { "body":  "java beginner" }}
            ],
            # 将其他低分的query的分数，乘以tie_breaker，最后综合得分来查看
            "tie_breaker": 0.3
        }
    }
}

# 多字段匹配查询
GET /lagou/_search
{
   "query": {
       "multi_match": {
           "query": "python java js",
           # type=best_fields相当于上面的dis_max查询
           # type=most_fields就是上面说的一般多字段匹配查询
           "type":"best_fields",
           # 在most_fields情况下，无法使用minimum_should_match去掉长尾
           "minimum_should_match": "50%"
           # 在多个字段中查询关键字，title的权重为3
           "fields": ["title^3", "desc"]
   		}
   }
}
```

一个唯一标识，跨了多个field，比如姓名可以散落在多个field中，如first_name和last_name中。要想查姓名为"Peter Smith"的记录，使用most_fields方式查询时，对query查询内容分词后，分词在doc的不同field中出现词频不一样，得分高低不一样，导致查询结果不理想。比如"Smith"在author_first_name这个field中，在所有doc的这个Field中，出现的频率很低，导致IDF分数很高缘故，最后"Smith Williams"可能结果得分最高。

解决方式一：使用copy to，将多个field组合成一个field

```json
# 使用copy_to,将多个field组合成一个field
PUT /forum/_mapping/article
{
  "properties": {
      "nauthor_first_name": {
          "type":     "string",
          "copy_to":  "new_author_full_name" 
      },
      "author_last_name": {
          "type":     "string",
          "copy_to":  "new_author_full_name" 
      },
      # new_author_full_name为隐藏字段，source中不可见
      "new_author_full_name": {
          "type":     "string"
      }
  }
}

GET /forum/article/_search
{
    "query": {
        "match": {
            "new_author_full_name": "Peter Smith"
        }
    }
}
```

此种方式还是可能会因为分词词频关系导致查询不准确，比如"Smith"出现较少，区分度较高，得分高，"Peter"出现次数多，得分很低，最后还是会导致full_name为"Smith Williams"得分最高。

解决方式二：使用原生cross_fields方式

```json
GET /forum/article/_search
{
    "query": {
        "multi_match": {
            "query": "Peter Smith",
            # 要求每个分词都必须在任何一个field中出现
            # Peter必须在author_first_name或author_last_name中出现
			# Smith必须在author_first_name或author_last_name中出现
            "type": "cross_fields", 
            # 当type=cross_fields时，必须指定and operator
            "operator": "and",
            "fields": ["author_first_name", "author_last_name"]
        }
    }
}
```

使用cross_fields，在得分计算时，将每个query在每个field中的IDF都取出来，取最小值，避免分词出现过高得分情况。

"Smith"在author_first_name这个field中，在所有doc的这个Field中，出现的频率很低，导致IDF分数很高；但"Smith"在所有doc的author_last_name field中的频率较高，所以IDF分数是正常的，不会太高；然后对于"Smith"来说，会取两个IDF分数中，较小的那个分数。就不会出现IDF分过高的情况。

### match_phrase查询

单纯使用match查询"python系统"，会将只要含有"python"和"系统"的doc都查询到，若想查询同时包含"python"和"系统"且二者有距离限制的doc，需要使用match_phrase查询。

```json
# 短语查询
GET /lagou/_search
{
   "query": {
     # 查询首先解析查询字符串来产生一个词条列表。然后会搜索所有的词条，
	 # 但只保留含有了所有搜索词条的文档，并且词条的位置要邻接。
     "match_phrase": {
       "title": { // title字段
         # 先进行分词，拆分为[python, 系统]，要求必须满足所有分词
         "query": "python系统", 
       	 # slop指定了分词之间允许的最大距离，默认为0
         "slop": 6
       }
     }
   }
}
```

使用match_phrase会抛弃仅含有"python"和"系统"的doc，要想返回结果中也包含仅含有"python"和"系统"的doc，且得分不会很高。bool组合match和match_phrase一起，来实现精准度和召回率的平衡：

```json
GET /forum/article/_search
{
    "query":{
        "bool":{
            "must":{
                "match":{
                    "title":{
                        "query":"java spark"
                    }
                }
            },
            "should":{
                # 在slop以内，如果java spark能匹配上一个doc，
                # 那么就会对doc贡献自己的relevance score，如果java和spark靠的越近，那么就分数越高
                "match_phrase":{
                    "title":{
                        "query":"java spark",
                        "slop":50
                    }
                }
            }
        }
    }
}
```

match_phrase比match的性能差大概10~20倍，在毫秒级是可以接受的，如果要做性能优化，可以做rescore。

默认情况下，match也许匹配了1000个doc，proximity match全都需要对每个doc进行一遍运算，判断能否slop移动匹配上，然后去贡献自己的分数。

但是很多情况下，match出来也许1000个doc，其实用户大部分情况下是分页查询的，所以可能最多只会看前几页，比如一页是10条，最多也许就看5页，就是50条。

proximity match只要对前50个doc进行slop移动去匹配，去贡献自己的分数即可，不需要对全部1000个doc都去进行计算和贡献分数。

```json
GET /forum/article/_search 
{
    "query": {
        "match": {
            "content": "java spark",
            "minimum_should_match": "50%"
        }
    },
    # 重打分
    "rescore": {
        "window_size": 50, # 对match query结果的前50条记录
        "query": {
            "rescore_query": {
    			# 根据slop移动，贡献自己的分数
                "match_phrase": {
                    "content": {
                        "query": "java spark",
                        "slop": 50
                    }
                }
            }
        }
    }
}
```

### 范围查询

```json
GET /my_store/products/_search
{
    "query": {
        "range": {
            "price": {
                "gte": 20, 
                "lt": 40, 
                "boost": 2 # 权重
            }
        }
    }
}
```

### 表达式查询

```json
# 前缀搜索，不会计算relevance score
GET /my_index/address/_search
{
    "query": {
        # 对于type为keyword字段搜索，可以使用前缀搜索
        "perfix": {
            "code": "C3DK" # 以C3DK开头的doc
        }
    }
}

# 通配符查询
GET /my_index/address/_search
{
    "query": {
        "wildcard": {
          	# ? 匹配任意字符， * 匹配 0 或多个字符
            "postcode": "W?F*HW" 
        }
    }
}

# 正则查询
GET /my_index/address/_search
{
    "query": {
        "regexp": {
            "postcode": "W[0-9].+" 
        }
    }
}
```

###`null`值处理

```json
# SELECT * FROM posts WHERE tags IS NOT NULL
GET /my_index/posts/_search
{
    "query" : {
    	"bool": {
         	"filter" : {
                "exists" : { 
                    "field" : "tags" 
                }
        	}
        } 
    }
}

# SELECT * FROM  posts WHERE tags IS NULL
GET /my_index/posts/_search
{
    "query" : {
    	"bool": {
         	"filter" : {
                "missing" : { 
                    "field" : "tags" 
                }
        	}
        } 
    }
}
```



## 搜索推荐

关于[suggestor详解](https://elasticsearch.cn/article/142)，用户输入一部分字，搜索引擎会给出建议词，如：输入ela,搜索引擎可以给出以ela开头的一些词。优点：

1. 对于用户，可以减少输入
2. 对于搜索引擎，增加了用户输入的准确性

同时es的suggester有纠错的功能，如输入sabew,可以返回正确的saber。Suggester一共有四种：

- Term suggester

  把输入以一个个term的形式来分析，并返回建议：

  ```json
  POST /blogs/_search
  {
      "suggest":{
          "my-suggestion":{
              "text":"lucne rock",
              # 指定suggest类型为term
              "term":{
                  "suggest_mode":"missing",
              	# 要搜索的字段
                  "field":"body"
              }
          }
      }
  }
  
  ...
  "suggest": {
      # 对搜索内容"lucne rock"进行term分词为"lucne"和"rock"
      "my-suggestion": [
        {
          "text": "lucne",
          "offset": 0,
          "length": 5,
      	# 对lucne分词建议词选项
          "options": [
            {
              "text": "lucene",
              "score": 0.8,
              "freq": 2
            }
          ]
        },
        {
          "text": "rock",
          "offset": 6,
          "length": 4,
          "options":
        }
      ]
  }
  ```

  基于analyze过的单个term去提供建议，并不会考虑多个term之间的关系。API调用方只需为每个token挑选options里的词，组合在一起返回给用户前端即可。

- Prase Suggester

  以词组为单位返回建议

  ```json
  POST /blogs/_search
  {
      "suggest": {
          "my-suggestion": {
              "text": "lucne and elasticsear rock",
              # 指定suggest类型为phrase
              "phrase": {
                  "field": "body",
                  "highlight": {
                      "pre_tag": "<em>",
                      "post_tag": "</em>"
                  }
              }
          }
      }
  }
  
  "suggest": {
      "my-suggestion": [
        {
          "text": "lucne and elasticsear rock",
          "offset": 0,
          "length": 26,
          "options": [
            {
              "text": "lucene and elasticsearch rock",
              "highlighted": "<em>lucene</em> and <em>elasticsearch</em> rock",
              "score": 0.004993905
            },
            {
              "text": "lucne and elasticsearch rock",
              "highlighted": "lucne and <em>elasticsearch</em> rock",
              "score": 0.0033391973
            },
            {
              "text": "lucene and elasticsear rock",
              "highlighted": "<em>lucene</em> and elasticsear rock",
              "score": 0.0029183894
            }
          ]
        }
      ]
    }
  ```

  Phrase suggester在Term suggester的基础上，会考量多个term之间的关系，比如是否同时出现在索引的原文里，相邻程度，以及词频等等

- Completion Suggester**提供了自动完成搜索即用型的功能，不适用于拼写纠错**。但Completion Suggester还支持模糊查询，意味着可以在搜索中输入拼写错误并仍然可以获得结果。

- Context Suggester

`completion`提供自动完成/搜索功能，可以在用户输入时引导用户查看相关结果，从而提高搜索精度。理想情况下，自动完成功能应该与用户键入的速度一样快，以提供与用户输入内容相关的即时反馈。因此，`completion`建议器针对速度进行了优化。搜索建议能够快速查找的数据结构，但构建成本高并且存储在内存中。

### 设置`completion`类型的mapping

```json
PUT music
{
    "mappings": {
        "song" : {
            "properties" : {
                # 搜索建议字段的type为completion
                "suggest" : {
                    "type" : "completion", #自动补全建议
                	"analyzer": "ik_smart"
                },
                # 其他字段
                "title" : {
                    "type": "keyword"
                }
            }
        }
    }
}
```

### suggest字段中的值存储

```json
PUT music/song/1?refresh # refresh将创建完一个文档后立即刷新索引，为了保证搜索可见
{
    "suggest" : {
        # 在搜索时会对数组中的值进行补全
        "input": [ "Nevermind", "Nirvana" ],
        "weight" : 34
    }
}


PUT music/song/1?refresh
{
	# 多值并采用不同权重
    "suggest" : [
        {
            "input": [ "架构","架构师"],
            "weight": 10
        },
        {
            "input": [ "空间","团队","发展","气氛"],
            "weight": 6
        }
    ]
}
```

### suggest查询

suggest查询主要用于输入补全，可纠错。

```json
# 模糊匹配查询
GET /my_index/my_type/_search
{
   "query": {
      # fuzzy表示模糊搜索
      "fuzzy": {
         # 搜索字段
         "text": {
           "value": "surprize", # 模糊搜索的词
           "fuzziness": 1, # 编辑距离：从一个词边为另一个词需要增、删、改、交换的最少次数
           "prefix_length": 0 # 搜索value前多少位不参与编辑的长度
         }
      }
   }
}

# suggest查询
# completion suggest本身不支持纠错，但可以结合模糊匹配进行纠错功能实现
POST music/_search?pretty
{
    "suggest": {
         # suggest查询后返回的变量名称，可以任意指定
         "my-suggest" : {
            # "prefix" : "nor", # 要匹配的前面几个固定词
            "text": "value", # 要搜索的值
        	# 指定suggest类型为completion
            "completion" : {
                "field" : "title-suggest", # 要搜索的字段
                # 此处使用fuzzy模糊搜索
                "fuzzy" : {
                    "fuzziness" : 1
                }
            }
         }
    }
}
```



## 批量查询

批量查询可以减少网络开销，在进行查询时，**如果一次性要查询多条数据的话，一定要考虑批量操作，尽可能减少网络开销，提高性能**。es提供了`mget` API 可以检索很多文档，要求有一个 `docs` 数组作为参数：

```json
GET /_mget
{
   "docs" : [
      {
         "_index" : "website",
         "_type" :  "blog",
         "_id" :    2
      },
      {
         "_index" : "website",
         "_type" :  "pageviews",
         "_id" :    1,
         # 如果想检索一个或者多个特定的字段，那么可以通过 _source 参数来指定这些字段的名字
         "_source": "views"
      }
   ]
}

# 如果想检索的数据都在相同的 _index 中（甚至相同的 _type 中），则可以在 URL 中指定默认的 /_index/_type 
GET /website/blog/_mget
{
   "docs" : [
      { "_id" : 2 },
      { 
        "_type" : "pageviews", 
        "_id" :   1 
      }
   ]
}

# 如果所有文档的 _index 和 _type 都是相同的，可以只传一个 ids 数组，而不是整个 docs 数组：
GET /website/blog/_mget
{
   "ids" : [ "2", "1" ]
}

# 对于返回结果，如果第二个文档未能找到并不妨碍第一个文档被检索到，每个文档都是单独检索和报告的。
```



## 组合查询

==倒排索引源于应用中需要根据属性值来查找记录，索引表中每一项包括一个属性值和具有该属性值的各记录的地址。由于不是由记录确定属性值，而是由属性值确定记录的位置，因而称为倒排索引。==

Elasticsearch 使用的查询语言（DSL） 拥有一套查询组件，可以在以下两种情况下使用：过滤情况（filtering context）和查询情况（query context）。

- 当使用于 *过滤情况* 时，查询被设置成一个“不评分”查询。回答也是非常的简单，yes 或者 no ，二者必居其一。

  为了明确和简单，用 "filter" 这个词表示不评分

- 当使用于 *查询情况* 时，查询就变成了一个“评分”的查询。也要去判断这个文档是否匹配，==同时它还需要判断这个文档的匹配程度如何==。

  一个评分查询计算每一个文档与此查询的 _相关程度_，同时将这个相关程度分配给表示相关性的字段 `_score`，并且按照相关性对匹配到的文档进行排序。这种相关性的概念是非常适合全文搜索的情况，因为全文搜索几乎没有完全 “正确” 的答案。

```json
GET /test/test/_search
{
    "query": {
      	# 用 bool 查询来实现组合多查询
      	# 每一个子查询都独自地计算文档的相关性得分。
      	# 一旦他们的得分被计算出来， bool 查询就将这些得分进行合并并且返回一个代表整个布尔操作的得分。
        "bool": {
          	# 文档必须匹配这些条件才能被包含进来
      		# 如果没有 must 语句，那么至少需要能够匹配其中的一条 should 语句。
      		# 如果存在至少一条 must 语句，则对 should 语句的匹配没有要求。
            "must": {
      			# 无论在任何字段上进行的是全文搜索还是精确查询，match 查询是可用的标准查询。
      			# 在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串
      			# 如果在一个精确值的字段上使用它，如数字、日期、布尔或者一个not_analyzed 字符串字段，那么它将会精确匹配给定的值：
                "match": {
                    "title": "how to make millions"
                }
            }, 
		   # 必须不匹配
            "must_not": {
                "match": {
                    "tag": "spam"
                }
            }, 
		   # 如果满足这些语句中的任意语句，将增加 _score ，否则，无任何影响。
		   # 主要用于修正每个文档的相关性得分。
		   # 默认情况下，should可以不匹配任何一个
		   # 如果没有must，should必须匹配至少一个，这情况下可以通过mum_should_match指定匹配个数
            "should": [
                {
                    "match": {
                        "tag": "starred1"
                    }
                },
               {
                    "match": {
                        "tag": "starred2"
                    }
                },
               {
                    "match": {
                        "tag": "starred3"
                    }
                }
            ], 
		   # 必须匹配，但它不评分，以过滤模式来进行，是根据过滤标准来排除或包含文档。
            "filter": {
              	# 将 bool 查询包裹在 filter 语句中，我们可以在过滤标准中增加布尔逻辑
                "bool": {
                    "must": [
                        {
                            "range": {
                                "price": {
                                    "lte": 29.99
                                }
                            }
                        }
                    ], 
                    "must_not": [
                        {
                            "term": {
                                "category": "ebooks"
                            }
                        }
                    ]
                }
            }
        }
		# 尽管没有 bool 查询使用这么频繁，constant_score 查询也是查询工具。
		# 它将一个不变的常量评分应用于所有匹配的文档。它被经常用于只需要执行一个 filter 而没有其它查询
		"constant_score":   {
              "filter": {
                  "term": { "category": "ebooks" } 
              }
          }
    }
}

```



## Scroll查询

scroll查询主要用于==海量数据查询==，一批一批查，直到所有数据查询完成。scroll查询会在第一次搜索的时候，保存一个当时的视图快照，之后只会基于该旧的视图快照进行数据搜索。如果期间数据发生变化，用户是看不到数据变化的。

```json
# scroll参数指定搜索请求的时间窗口
GET /jobbole/article/_search?scroll=1m
{
    "query":{
        "match_all":{}
    },
    "sort":[
        "_doc"
    ],
    "size":5
}

# 查询返回结果中会有scroll_id,下次scroll查询必须将此id带上
GET /_search/scroll
{
    "scroll":"1m",
    # scroll_id 会保存该次查询时的上下文
    "scroll_id":"DnF1ZX..."
}
```



## 操控查询得分

可以在查询时设置boost来提高对指定内容查询的得分，也可以通过negative_boost来降低得分：

```json
GET /forum/article/_search 
{
    "query": {
        "boosting": {
            "positive": {
                "match": {
                    "content": "java"
                }
            },
            # negative的doc，会乘以negative_boost，降低分数
            "negative": {
                "match": {
                    "content": "spark"
                }
            },
            "negative_boost": 0.2
        }
    }
}
```

也可以通过自定义一个function_score函数指定field来增强得分，如阅读人数越多，得分越高：

```json
GET /forum/article/_search
{
    "query": {
        # function_score
        "function_score": {
        	# multi_match查询
            "query": {
                "multi_match": {
                    "query": "java spark",
                    "fields": ["tile", "content"]
                }
            },
			# 指定field来增强得分
            "field_value_factor": {
                # 指定field，new_score = number_of_votes = follower_num的值
                "field": "follower_num",
                # new_score = old_score * log(1 + number_of_votes)
                "modifier": "log1p",
                # new_score = old_score * log(1 + factor * number_of_votes)
                "factor": 0.5
            },
			# 可以决定分数与指定字段的值如何计算，multiply，sum，min，max，replace
            "boost_mode": "sum",
			# 限制计算出来的分数不要超过max_boost指定的值
            "max_boost": 2
        }
    }
}
```

