[TOC]

##深度网络优化

###参数初始化

为了防止梯度炸裂，训练出一个权重不会增长或消失过快的深度网络，初始化参数使用方法：
$$
w^{[l]} = np.random.randn( \text{shape})*\text{np.}\text{sqrt}(\frac{1}{n^{[l-1]}})
$$
如果用的是Relu激活函数，而不是$\frac{1}{n}$，方差设置为$\frac{2}{n}$，效果会更好。如果使用**tanh**函数，可以用公式$\sqrt{\frac{1}{n^{[l-1]}}}$或公式$\sqrt{\frac{2}{n^{[l-1]} + n^{\left[l\right]}}}$ 。

```python
def initialize_parameters_he(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    np.random.seed(3)
    parameters = {}
    L = len(layers_dims)    # integer representing the number of layers
     
    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * np.sqrt(2 / layers_dims[l - 1])
        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))
        
    return parameters
```

### 正则化

如果训练数据集不够大，因为深度学习模型的灵活性和容量，过度拟合可能会是一个严重问题。

#### L2 regularization

**代价函数:**
$$
J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{\lambda}{2m} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost}
$$
注：需要对每一层的$W^{[l]}$进行加合

```python
def compute_cost_with_regularization(A3, Y, parameters, lambd):
    """
    Implement the cost function with L2 regularization. See formula (2) above.
    
    Arguments:
    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)
    Y -- "true" labels vector, of shape (output size, number of examples)
    parameters -- python dictionary containing parameters of the model
    
    Returns:
    cost - value of the regularized loss function (formula (2))
    """
    m = Y.shape[1]
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    W3 = parameters["W3"]
    
    # 之前未正则化的代价函数
    cross_entropy_cost = compute_cost(A3, Y)  
    
    # 正则化项
    L2_regularization_cost = (lambd / (2 * m)) * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)))
    
    cost = cross_entropy_cost + L2_regularization_cost
    
    return cost
```

**反向传播：**

需要对梯度gradient也进行正则化：$\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W$，所以$dW_{new} = dW_{old} + \frac{\lambda}{m} W$。

```python
dW3 = 1./m * np.dot(dZ3, A2.T) + (lambd / m) * W3
```

#### Dropout

**正向传播：**

随着dropout，因为其他神经元可能在任何时候都被关闭，所以神经元对其他特定神经元的激活变得不那么敏感（一般不会将dropout应用到输入层和输出层）。进行dropout需要经过4个步骤：

1. 使用`np.random.rand(shape)`初始化一个和$A^{[l]}$同样大小的$D^{[l]}$
2. 根据`Dl = Dl < keep_prob`设定$D^{[l]}$，这样可以保留keep_prob%的元素
3. 通过`Al ∗ Dl`来关闭一些神经元
4. 为了==保证$Z^{[l+1]}$的期望值==，进行`al /= keep-prob`处理

```python
def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):
    """
    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.
    
    Arguments:
    X -- input dataset, of shape (2, number of examples)
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3"
    keep_prob - probability of keeping a neuron active during drop-out, scalar
    
    Returns:
    A3 -- last activation value, output of the forward propagation, of shape (1,1)
    cache -- tuple, information stored for computing the backward propagation
    """
    
    # retrieve parameters
    W1 = parameters["W1"],b1 = parameters["b1"],W2 = parameters["W2"],b2 = parameters["b2"],W3 = parameters["W3"],b3 = parameters["b3"]
    
    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID
    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)
    
    # 假设只对第二层进行dropout
    Z2 = np.dot(W2, A1) + b2
    A2 = relu(Z2)
    # Step 1: initialize matrix D2 = np.random.rand(..., ...)
    D2 = np.random.rand(A2.shape[0], A2.shape[1]) 
    # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)
    D2 = D2 < keep_prob   
    # Step 3: shut down some neurons of A2
    A2 = A2 * D2       
    # Step 4: scale the value of neurons that haven't been shut down
    A2 = A2 / keep_prob                                         

    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)
    
    # 需要对D也缓存下来,在反向传播时使用
    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)
    
    return A3, cache
```

**反向传播：**

实现dropout反向传播非常简单，主要分两步：

1. 在正向传播时使用了`Al ∗ Dl`，同理，在反向传播时，也需要关闭相同的神经元：`dAl ∗ Dl`
2. 在正向传播时`al /= keep-prob`保证了下一层$Z$的期望，在反向传播时也需要`dAl /=keep-prob `（因为$A^{[l]}$被缩放了，所以它的导数$dA^{[l]}$也需要进行缩放）

```python
...

# Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation
dA2 = dA2 * D2  
# Step 2: Scale the value of neurons that haven't been shut down
dA2 = dA2 / keep_prob   

...
```

### 梯度检验

为了确保反向传播的正确性，需要进行梯度检验，首先需要对每个参数进行双边误差计算：
$$
d\theta_{\text{approx}}\left[i \right] = \frac{J\left( \theta_{1},\theta_{2},\ldots\theta_{i} + \varepsilon,\ldots \right) - J\left( \theta_{1},\theta_{2},\ldots\theta_{i} - \varepsilon,\ldots \right)}{2\varepsilon}
$$

```python
# 把所有矩阵W和b转换成向量之后，做连接运算，得到一个巨型向量 
parameters_values = dictionary_to_vector(parameters) 
num_parameters = parameters_values.shape[0]

J_plus = np.zeros((num_parameters, 1))
J_minus = np.zeros((num_parameters, 1))
gradapprox = np.zeros((num_parameters, 1))

for i in range(num_parameters):
        
    thetaplus = np.copy(parameters_values)                                      
    thetaplus[i][0] = thetaplus[i][0] + epsilon                                
    J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus)) #需要再把向量转变为矩阵计算反向传播                
    
    thetaminus = np.copy(parameters_values)                                    
    thetaminus[i][0] = thetaminus[i][0] - epsilon                              
    J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))               

    # Compute gradapprox[i]
    gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)

```

最后判断真实导数向量与估算的向量之间的距离（分母使得这个方程式变成比率，用于预防这些向量太小或太大）：
$$
\frac{{||d\theta_{\text{approx}} -d\theta||}_{2}}{||d\theta_{\text{approx}}||_2 + ||d\theta||_2}
$$

```python
# 把原来的梯度也转换为巨大的向量
grad = gradients_to_vector(gradients)

numerator = np.linalg.norm(gradapprox - grad)   # 分子                                   
denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)  # 分母                    
difference = numerator / denominator  
```

