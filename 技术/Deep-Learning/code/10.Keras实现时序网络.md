Keras中的LSTM输入$X$的shape为$(m, T_x, n_{values})$，输出$Y$的shape为$(T_y, m, n_{values})$

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/LSTM.png" style="width:600;height:400px;">

在Keras中，序列生成涉及==定义具有共享权重的层==，然后针对不同的时间步长$1，...，T_x$重复这些层，所以LSTM_cell只初始化一次，在不同$t$时刻使用的都是统一LSTM_cell

```python
reshapor = Reshape((1, 78))  # 将输入的内容reshape为(1, 78)    
# 必须传入的是输出a的大小
LSTM_cell = LSTM(n_a, return_state = True)         
densor = Dense(n_values, activation='softmax') 

def djmodel(Tx, n_a, n_values):
    """
    Implement the model
    
    Arguments:
    Tx -- length of the sequence in a corpus
    n_a -- the number of activations used in our model
    n_values -- number of unique values in the music data 
    
    Returns:
    model -- a keras model with the 
    """
    
    # 定义model的输入，不用指定样本数m
    X = Input(shape=(Tx, n_values)) # X shape (?, Tx, n_values)
    # 定义LSTM的输入a0和c0
    a0 = Input(shape=(n_a,), name='a0') # a0 shape (?, n_a)
    c0 = Input(shape=(n_a,), name='c0')
    
    # previous hidden state
    a = a0 
    # previous cell state
    c = c0 
    
    # Step 1: Create empty list to append the outputs while you iterate (≈1 line)
    outputs = []
    
    # Step 2: Loop
    for t in range(Tx):
        
        # Step 2.A: 选取X的Tx时刻的输入样本 
        x = Lambda(lambda x: X[:,t,:])(X) # shape: (?, 78)
        # Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line)
        x = reshapor(x) # shape: (?, 1, 78)
        # Step 2.C: Perform one step of the LSTM_cell
        # 在Keras中，序列生成涉及定义具有共享权重的层，然后针对不同的时间步长1，...，Tx重复这些层。
        # 不同Tx时刻的LSTM_cell是相同的，所以此处不是直接调用LSTM(n_a, return_state = True)重新初始化
        # 而是执行已经初始化好的LSTM_cell
        a, _, c = LSTM_cell(x, initial_state=[a, c])
        # Step 2.D: Apply densor to the hidden state output of LSTM_Cell
        # densor同理，Wya在不同时刻t都是一样的，所以初始化一次
        out = densor(a)
        # Step 2.E: add the output to "outputs"
        outputs.append(out)
        
    # Step 3: Create model instance
    model = Model(input = [X, a0, c0], outputs = outputs)
    
    return model
```



###LSTM与词嵌入的应用

假设输入一个长度为10的句子，词汇表大小为10000，则经过one_hot之后变为$10000 \times 10$的大小。假设词嵌入的特征个数为50，Keras的嵌入层`Embedding(10000, 50, input_length=10)`输出一个$? \times 10 \times 50$大小的矩阵，对应刚好是LSTM输入大小$(m, T_x, n_{values})$.

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/%E5%A4%9A%E5%B1%82LSTM.png" style="width:700px;height:400px;">

​    

```python
def Emojify_V2(input_shape, word_to_vec_map, word_to_index):
    """
    Function creating the Emojify-v2 model's graph.
    
    Arguments:
    input_shape -- shape of the input, usually (max_len,)
    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation
    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)

    Returns:
    model -- a model instance in Keras
    """
    
    ### START CODE HERE ###
    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).
    sentence_indices = Input(input_shape, dtype='int32') # (None, 10)  
    
    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)
    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)
    
    # 加载Embedding嵌入层 
    # sentence_indices 指定了embedding_layer输出的时长Tx = 10
    embeddings = embedding_layer(sentence_indices) 
    # embeddings shape: (None, 10, 50)  输入到LSTM中的大小为Tx = 10, n_values = 50
    
    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state
    # Be careful, the returned output should be a batch of sequences.
    X = LSTM(128, return_sequences=True)(embeddings) # X shape (None, 10, 128) 
    # Add dropout with a probability of 0.5
    X = Dropout(0.5)(X)
    # Propagate X trough another LSTM layer with 128-dimensional hidden state
    # Be careful, the returned output should be a single hidden state, not a batch of sequences.
    # return_sequences = False 只输出最后一个输出，所以输出X shape (None, 128) 
    X = LSTM(128, return_sequences=False)(X) #
    # Add dropout with a probability of 0.5
    X = Dropout(0.5)(X)
    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.
    X = Dense(5)(X) # (None, 5)  
    # Add a softmax activation
    X = Activation('softmax')(X)
    
    # Create Model instance which converts sentence_indices into X.
    model = Model(inputs=sentence_indices, outputs=X)
    
    return model
```
Keras嵌入层embedding_layer的作用就是将词汇表转换为词嵌入。

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/embedding.png" style="width:700px;height:250px;">

上图有点问题，右边做下转置，即输入一个$m \times maxlen$矩阵，经过embedding_layer的作用，可以将其转换为$m \times maxlen \times e\_length$大小的矩阵，对应刚好是LSTM输入大小$(m, T_x, n_{values})$

关于上面方法中的`pretrained_embedding_layer`函数：

```python
def pretrained_embedding_layer(word_to_vec_map, word_to_index):
    """
    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.
    
    Arguments:
    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.
    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)

    Returns:
    embedding_layer -- pretrained layer Keras instance
    """
    
    vocab_len = len(word_to_index) + 1 # 词汇表的大小，按照API要求,需要+1
    # define dimensionality of your GloVe word vectors (= 50)
    emb_dim = word_to_vec_map["cucumber"].shape[0] # 每个词的词嵌入向量的大小     
    
    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)
    emb_matrix = np.zeros(shape=(vocab_len, emb_dim))
    
    # Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary
    for word, index in word_to_index.items():
        emb_matrix[index, :] = word_to_vec_map[word] # index是从1开始的
. 
    # input_dim 词汇表大小，最大整数 index + 1
    # output_dim: 词向量的维度作为输出，
    embedding_layer = Embedding(input_dim = emb_matrix.shape[0], output_dim = emb_matrix.shape[1], trainable=False)

    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".
    embedding_layer.build((None,))
    
    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.
    embedding_layer.set_weights([emb_matrix])
    
    return embedding_layer
```

