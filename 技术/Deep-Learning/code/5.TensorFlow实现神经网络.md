神经网络Tensorflow实现举例：

1. 因为$X,Y$是入参，在开始时先要用`tf.placeholder`给$X,Y$占个位：

   ```python
   # 占feature_num x ?维度的位置
   X = tf.placeholder(tf.float32, shape=(x_feature_num, None))
   origin_Y = tf.placeholder(tf.float32, shape=(y_feature_num, None))
   
   # 如果是多分类，需要对Y进行缩放
   # 如Y结果为[0, 1, 2, 1]，进行缩放后变为
   #     [[1., 0., 0., 0.],
   #      [0., 1., 0., 1.],
   #      [0., 0., 1., 0.]]
   Y = tf.one_hot(indices=origin_Y, depth=分类个数, axis=0)
   ```

2. 初始化参数$W$和$b$，在梯度下降时，$W$和$b$会不断更新，用`tf.Variable`：

   ```python
   # 使用tf.Variable时，命名相同时系统会自己处理。使用tf.get_variable()时，命名相同会报错
   # 在使用tf.variable_scope("scope1", reuse=True)开启共享时，tf.Variable() 每次都会创建新对象，对于tf.get_variable()，如果已经创建的变量对象，就把那个对象返回
   W = tf.get_variable("W", [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))
   b = tf.get_variable("b", [25, 1], initializer = tf.zeros_initializer())
   ```

   使用变量前，记得对变量进行初始化`sess.run(tf.global_variables_initializer())`。

3. 正向传播

   ```python
   Z1 = tf.add(tf.matmul(W1, X), b1)  
   A1 = tf.nn.relu(Z1)
   Z2 = tf.add(tf.matmul(W2, A1), b2)
   ```

4. 计算代价函数：
   $$
   J = - \frac{1}{m}  \sum_{i = 1}^m  \large ( \small y^{(i)} \log a^{ [2] (i)} + (1-y^{(i)})\log (1-a^{ [2] (i)} )\large )\small
   $$

   ```python
   # tf.nn.softmax_cross_entropy_with_logits()默认是按照最后一个维度进行运算，这里即列方向（是对所有行方向压缩产生列），为了适应需要对Z,Y转置一下
   logits = tf.transpose(Z2) # 转置
   labels = tf.transpose(Y)
   
   # tf.nn.softmax_cross_entropy_with_logits只求了交叉熵，最后还需要平均求值
   cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z2, labels = Y))
   ```

5. 反向传播

   使用Tensorflow框架时，不需要自己手动实现反向传播，框架可以自己进行梯度下降的优化：

   ```python
   # 每次执行时，梯度下降会根据代价函数去进行求导运算，并自动更新变量W，b
   optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)
   ```

6. 运行

   ```python
   with tf.Session() as sess:
       # 根据cost函数，执行梯度下降
       _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})
   ```

**总结：**

关于使用Tensorflow框架：

1. 在Tensorflow中主要的对象类就是`Tensors` 和`Operators`
2. 编码时，创建包含`Tensor`（`Variable`，`placeholder`...）和`Operators`（`tf.matmul`，`tf.add`，...）的`Graph`
3. 创建并初始化会话`Session`
4. 在执行会话`Session`中的优化器`optimizer`时，框架会自动执行反向传播梯度下降
5. 别忘记关闭`Session`

