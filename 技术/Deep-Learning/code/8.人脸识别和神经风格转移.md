##人脸识别

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB.jpg" style="width:480px;height:200px;">

CNN就是将图片经过一系列卷积、池化、全连接、最后sorfmax分类，但在这不进行最后的分类，只是到全连接为止。要验证的两张图片经过编码后的全连接层分别叫作$f(x1), f(x2)$。最后计算 ![d(x_1,x_2)=||f(x_1)-f(x_2)||_2^2](https://www.zhihu.com/equation?tex=d%28x_1%2Cx_2%29%3D%7C%7Cf%28x_1%29-f%28x_2%29%7C%7C_2%5E2) ，如果distance小于设定的阈值就算同一个人。

三元组损失函数的定义基于三张图片，假如三张图片$A$、$P$、$N$，即Anchor样本、Positive样本和Negative样本，其中Positive图片和Anchor图片是同一个人，但是Negative图片和Anchor不是同一个人。其损失函数：
$$
L( A,P,N) = max(|| f( A) - f( P)||^{2} -|| f( A) - f( N)||^{2} + \alpha,0)
$$

```python
def triplet_loss(y_true, y_pred, alpha = 0.2):
    """
    Implementation of the triplet loss as defined by formula (3)
    
    Arguments:
    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.
    y_pred -- python list containing three objects:
            anchor -- the encodings for the anchor images, of shape (None, 128)
            positive -- the encodings for the positive images, of shape (None, 128)
            negative -- the encodings for the negative images, of shape (None, 128)
    
    Returns:
    loss -- real number, value of the loss
    """
    
    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]
    
    # Step 1: Compute the (encoding) distance between the anchor and the positive
    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)))
    # Step 2: Compute the (encoding) distance between the anchor and the negative
    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)))
    # Step 3: subtract the two previous distances and add alpha.
    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)
    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.
    loss = tf.reduce_sum(tf.maximum(basic_loss,0))
    
    return loss
```

使用技巧：不需要每次都计算已有图片的这些特征，可以提前计算好放在database中，那么当一个待验证员工走近时，可以使用卷积网络来计算新输入的编码，然后和预先计算好的编码进行比较，最后输出预测值：

```python
FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])
load_weights_from_FaceNet(FRmodel) # 加载已经训练好的模型权重

def who_is_it(image_path, database, model):
    """
    Implements face recognition for the happy house by finding who is the person on the image_path image.
    
    Arguments:
    image_path -- path to an image
    database -- database containing image encodings along with the name of the person on the image
    model -- your Inception model instance in Keras
    
    Returns:
    min_dist -- the minimum distance between image_path encoding and the encodings from the database
    identity -- string, the name prediction for the person on image_path
    """
    
    ## Step 1: img_to_encoding()就是对图片通过model后的结果
    encoding = img_to_encoding(image_path, model)
    
    ## Step 2: Find the closest encoding ##
    
    # Initialize "min_dist" to a large value, say 100 (≈1 line)
    min_dist = 100
    
    # Loop over the database dictionary's names and encodings.
    for (name, db_enc) in database.items():      
        # 通过求矩阵的范数，来计算向量的间隔距离
        dist = np.linalg.norm(encoding - db_enc)

        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)
        if dist < min_dist:
            min_dist = dist
            identity = name
    
    if min_dist > 0.7:
        print("Not in the database.")
    else:
        print ("it's " + str(identity) + ", the distance is " + str(min_dist))
        
    return min_dist, identity
```



## 风格转换

![](http://www.ai-start.com/dl2017/images/7b75c69ef064be274c82127a970461cf.png)

为了实现神经风格迁移，先要了解下CNN到底在学什么，先将CNN的层可视化

![img](https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/CNN%E6%AF%8F%E5%B1%82%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9.jpg)

前几层一般都是边缘、条纹等初级特征，层越深它的特征也越明显，如部分、组件、物体。

在了解了CNN的特征提取后，就能更好的理解风格转换。为了提取特征会用到迁移学习，也就是采用了一个预训练模型，直接提取图片在每一层的特征，这样就避免了大量运算。

最后要做的是定义一个关于$G$的代价函数$J$用来评判某个生成图像的好坏，然后使用梯度下降法去最小化$J(G)$，以便于生成这个图像。
$$
J( G) = \alpha J_{\text{content}}( C,G) + \beta J_{\text{style}}(S,G)
$$
两个超参数$\alpha$和$\beta$来确定，内容代价和风格代价两者之间的权重用两个超参数来确定。

### 内容代价函数

在实际中，通常影藏层$l$会选择在网络的中间层，既不太浅也不很深。然后用一个预训练的卷积模型，可以是VGG网络（其他的网络也可以），两个图片$C$和$G$在$l$层的激活函数值，一个代表内容图片$ a^{(C)}$，另一个代码生成图片$ a^{(G)}$。

$J_{\text{content}}(C,G)$用来度量生成图片$G$的内容与内容图片$C$的内容有多相似：
$$
J_{content}(C,G) = \frac{1}{2}|| a^{[l][C]} - a^{[l][G]}||^{2} =  \frac{1}{4 \times n_H \times n_W \times n_C}\sum _{ \text{all entries}} (a^{(C)} - a^{(G)})^2
$$
为了计算方便，将$n_H \times n_W \times n_C$转化为成二维对象：

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/NST_LOSS.png" style="width:500px;height:200px;">

```python
def compute_content_cost(a_C, a_G):
    """
    Computes the content cost
    
    Arguments:
    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C 
    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G
    
    Returns: 
    J_content -- scalar that you compute using equation 1 above.
    """
    
    # Retrieve dimensions from a_G (≈1 line)
    m, n_H, n_W, n_C = a_G.get_shape().as_list()
    
    # Reshape a_C and a_G (≈2 lines)
    a_C_unrolled = tf.reshape(a_C, [n_H * n_W, n_C])
    a_G_unrolled = tf.reshape(a_G, [n_H * n_W, n_C])
    
    # compute the cost with tensorflow (≈1 line)
    J_content = tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled, a_G_unrolled))) / (4 * n_H * n_W * n_C)
    
    return J_content
```



### 风格代价函数

####相关性系数：

要计算Style Cost，首先先要尝试从计算机的角度理解图片的"Style"，这里把它理解为CNN中的activations（激活层）在channels维度上的相关性系数，**相关性系数描述的就是当图片某处出现这种垂直纹理时，该处又同时是橙色的可能性**。

为了能够测量出刚才所说的相关性系数，需要计算出这张图像的风格矩阵。用$a_{i,j,k}^{[l]}$来记录相应位置的激活项，也就是$l$层中的$i,j,k$位置，所以$i$代表高度，$j$代表宽度，$k$代表着$l$层中的不同通道。

==风格矩阵$G^{[l]}$，是个$n_{c} \times n_{c}$的矩阵，也就是一个方阵。用$G_{\text{kk}^{'}}^{[l]}$表示$k$通道与$k'$通道中的激活项之间的相关系数，$k$和$k'$会在1到$n_{c}$之间取值，$n_{c}$就是$l$层中通道的总数量，所以是一个$n_{c} \times n_{c}$的矩阵==。

在这个矩阵中$k$和$k'$元素被用来描述$k$通道和$k'$通道之间的相关系数，只代表一种元素，具体地：
$$
G_{kk^{'}}^{[l]( S)} = \sum_{i = 1}^{n_{H}^{[l]}}{\sum_{j = 1}^{n_{W}^{[l]}}{a_{i,\ j,\ k}^{[l](S)}a_{i,\ j,\ k^{'}}^{[l](S)}}}
$$
所以需要创造一个style matrix，也叫Gram matrix：

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/GramMatrix.jpg" style="width:500px;height:200px;">

```python
def gram_matrix(A):
    """
    Argument:
    A -- matrix of shape (n_C, n_H*n_W)
    
    Returns:
    GA -- Gram matrix of A, of shape (n_C, n_C)
    """
    
    GA = tf.matmul(A, tf.transpose(A))
    
    return GA
```



#### 代价函数：

$J_{\text{style}}(S,G)$本质上是检测的生成图片$G$和图片$S$的**相关性差值**：
$$
J_{style}^{[l]}(S,G) = \frac{1}{4 \times {n_C}^2 \times (n_H \times n_W)^2} \sum _{i=1}^{n_C}\sum_{j=1}^{n_C}(G^{(S)}_{ij} - G^{(G)}_{ij})^2
$$

```python
def compute_layer_style_cost(a_S, a_G):
    """
    Arguments:
    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S 
    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G
    
    Returns: 
    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)
    """

    # Retrieve dimensions from a_G (≈1 line)
    m, n_H, n_W, n_C = a_G.get_shape().as_list()
    
    # Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines)
    a_S = tf.reshape(a_S, [n_H * n_W, n_C])
    a_G = tf.reshape(a_G, [n_H * n_W, n_C])

    # Computing gram_matrices for both images S and G (≈2 lines)
    GS = gram_matrix(tf.transpose(a_S)) # n_C x n_C, 所以需要对a_S做转置
    GG = gram_matrix(tf.transpose(a_G))

    # Computing the loss (≈1 line)
    J_style_layer = tf.reduce_sum(tf.square((tf.subtract(GS,GG)))) / (4 * (n_H**2) * (n_W**2) * (n_C**2))
    
    return J_style_layer
```

到目前为止，只是从某一个图层中捕获了样式。 如果将几个不同的层“合并”样式成本，我们将获得更好的结果。对多个层做加权：
$$
J_{style}(S,G) = \sum_{l} \lambda^{[l]} J^{[l]}_{style}(S,G)
$$

```python
def compute_style_cost(model, STYLE_LAYERS):
    """
    Computes the overall style cost from several chosen layers
    
    Arguments:
    model -- our tensorflow model
    STYLE_LAYERS -- A python list containing:
                        - the names of the layers we would like to extract style from
                        - coeff 不同隐藏层的J_style
    
    Returns: 
    J_style -- tensor representing a scalar value, style cost defined above by equation (2)
    """
    
    # initialize the overall style cost
    J_style = 0

    for layer_name, coeff in STYLE_LAYERS:

        # 根据指定的层，选择一个输出
        out = model[layer_name]
        # 通过sess.run(out)计算出激活值a_S
        a_S = sess.run(out)

        # a_G references model[layer_name] and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that when we run the session, this will be the activations drawn from the appropriate layer, with G as input.
        a_G = out
        
        # Compute style_cost for the current layer
        J_style_layer = compute_layer_style_cost(a_S, a_G)
        # Add coeff * J_style_layer of this layer to overall style cost
        J_style += coeff * J_style_layer

    return J_style
```



### 汇总

$$
J( G) = \alpha J_{\text{content}}( C,G) + \beta J_{\text{style}}(S,G)
$$

```python
def total_cost(J_content, J_style, alpha = 10, beta = 40):
    """
    Computes the total cost function
    
    Arguments:
    J_content -- content cost coded above
    J_style -- style cost coded above
    alpha -- hyperparameter weighting the importance of the content cost
    beta -- hyperparameter weighting the importance of the style cost
    
    Returns:
    J -- total cost as defined by the formula above.
    """

    J = alpha * J_content + beta * J_style
    
    return J
```



最后，输入图片C、S，载入VGG16 model，开启会话后，进行训练，迭代出新的G:

```python
# define optimizer (1 line)
optimizer = tf.train.AdamOptimizer(2.0)
# define train_step (1 line)
train_step = optimizer.minimize(J)

def model_nn(sess, input_image, num_iterations = 200):
    
    # Initialize global variables (you need to run the session on the initializer)
    ### START CODE HERE ### (1 line)
    sess.run(tf.global_variables_initializer())
    ### END CODE HERE ###
    
    # Run the noisy input image (initial generated image) through the model. Use assign().
    ### START CODE HERE ### (1 line)
    generated_image=sess.run(model['input'].assign(input_image))
    ### END CODE HERE ###
    
    for i in range(num_iterations):
    
        # Run the session on the train_step to minimize the total cost
        sess.run(train_step)
        
        # Compute the generated image by running the session on the current model['input']
        generated_image = sess.run(model['input'])

        # Print every 20 iteration.
        if i%20 == 0:
            Jt, Jc, Js = sess.run([J, J_content, J_style])
            print("Iteration " + str(i) + " :")
            print("total cost = " + str(Jt))
            print("content cost = " + str(Jc))
            print("style cost = " + str(Js))
            
            # save current generated image in the "/output" directory
            save_image("output/" + str(i) + ".png", generated_image)
    
    # save last generated image
    save_image('output/generated_image.jpg', generated_image)
    
    return generated_image
```

