[TOC]

机器学习（ML）策略（1）（ML strategy（1））
---

这门课程能够学到如何更快速高效地优化机器学习系统。

假设一个机器学习系统达到了90%准确率，还不够好，这时有很多想法去改善系统。比如：

- 收集更多的训练数据。
- 可能训练集(train-set)的多样性还不够，应该收集更多样化的反例集。
- 再用梯度下降训练算法，训练久一点。
- 尝试用一个完全不同的优化算法，比如**Adam**优化算法。
- 尝试使用规模更大或者更小的神经网络。
- 试试**dropout**或者$L2$正则化。
- 修改网络的架构
  - 修改激活函数
  - 改变隐藏单元的数目
  - and so on

有很多想法可以去试，问题在于，如果做出了错误的选择，可能白费几个月的时间，往错误的方向前进。如何快速有效判断哪些想法是靠谱的，或者甚至提出新的想法，判断哪些是值得一试的想法，哪些是可以放心舍弃的？

这里会讲一些策略，一些分析机器学习问题的方法，可以指引我们朝着最有希望的方向前进。事实上，机器学习策略在深度学习的时代也在变化，因为现在对于深度学习算法来说能够做到的事情，比上一代机器学习算法大不一样。

### 1. 正交化（Orthogonalization）

==这里的正交化是指一次操作只影响一件事。==

例如：老式电视图片，有很多旋钮可以用来调整图像的各种性质，如一个旋钮来调整高度，一个旋钮调整宽度，一个旋钮调整梯形角度，以此类推。

相比之下，假设其中一个旋钮调的是$0.1x$表示图像高度，$+0.3x$表示图像宽度，$-1.7x$表示梯形角度，$+0.8x$表示图像在水平轴上的坐标之类的。如果调整其中一个旋钮，那么图像的高度、宽度、梯形角度、平移位置全部都会同时改变，那几乎不可能把电视调好，让图像显示在区域正中。

在这种情况下，正交化指的是设计师设计的旋钮，**每个旋钮都只调整一个性质**，这样调整电视图像就容易得多，就可以把图像调到正中。

那么这与机器学习有什么关系呢？要弄好一个监督学习系统，通常需要调系统的旋钮。类似地：

- 如果算法在成本函数上不能很好地拟合训练集(train-set)，想要一个旋钮，这样可以用来可以调整你的算法，让它很好地拟合训练集(train-set)。所以用来调试的旋钮是可以训练更大的网络，或者可以切换到更好的优化算法，比如**Adam**优化算法，等等。
- 如果发现算法对开发集(dev-set)的拟合很差，那么应该有独立的一组旋钮。比如说，算法在训练集(train-set)上做得很好，但在开发集(dev-set)不行。增大训练集(train-set)可以是另一个可用的旋钮，它可以帮助学习算法更好地归纳开发集(dev-set)的规律。
- 如果系统在开发集(dev-set)上做的很好，但测试集(test-set)上做得不好呢？如果在开发集(dev-set)上做的不错，但测试集(test-set)不行这可能意味着对开发集(dev-set)过拟合了，需要往回退一步，使用更大的开发集(dev-set)。
- 如果它在测试集(test-set)上做得很好，但无法在应用中提供良好的体验，这意味着需要回去，改变开发集(dev-set)或成本函数。因为如果根据某个成本函数，系统在测试集(test-set)上做的很好，但它无法反映算法在现实世界中的表现，这意味着要么开发集(dev-set)分布设置不正确，要么是成本函数测量的指标不对。

对于正交化的理解，就像看电视图像一样。如果电视图像太宽，就调整宽度旋钮；如果太高了，要调整高度旋钮。在机器学习中，如果可以观察你的系统，然后说这一部分是错的，它在训练集(train-set)上做的不好，或者它在测试集(test-set)上做的不错。**必须弄清楚到底是哪块地方出问题了**，然后我们刚好有对应的旋钮，或者一组对应的旋钮，刚好可以解决那个限制了机器学习系统性能的问题。

### 2. 单一数字评估指标（Single number evaluation metric）

无论是调整超参数，或者是尝试不同的学习算法，如果有一个单实数评估指标，你的进展会快得多。它可以快速告诉你，新尝试的手段比之前的手段好还是差。所以当团队开始进行机器学习项目时，推荐他们为问题设置一个单实数评估指标。

![](http://www.ai-start.com/dl2017/images/24144bd15bb01226dc596457e6e6cfcb.png)

比如猫分类器，之前搭建了某个分类器$A$，现在又训练出来了一个新的分类器B，所以评估你的分类器的一个合理方式是观察它的查准率和查全率。

查准率（**precision**）：分类器汇总标记为猫的，有多少是真猫。如果分类器$A$有95%的查准率，这意味着分类器说这图是猫的时候，有95%的机会真的是猫。
查全率（**recall**）：样本中真的猫，有多少被识别出来。如果分类器$A$查全率是90%，这意味着对于所有的图像，分类器$A$准确地分辨出了其中的90%。

但使用查准率和查全率作为评估指标的时候，有个问题：如分类器$A$在查全率上表现更好，分类器$B$在查准率上表现更好，就无法判断哪个分类器更好。
如果有两个评估指标，就很难去快速地二中选一或者多选一，所以并不推荐使用两个评估指标，查准率和查全率来选择一个分类器。所以需要找到一个新的评估指标，能够结合查准率和查全率。

在机器学习文献中，结合查准率和查全率的标准方法是所谓的$F_1$分数，可以认为查准率$P$和查全率$R$的平均值。正式来看，$F_1$分数的定义是这个公式：$\frac{2}{\frac{1}{P} + \frac{1}{R}}=2\frac{PR}{P+R}$。在数学中，这个函数叫做查准率$P$和查全率$R$的调和平均数。这个指标在权衡查准率和查全率时有一些优势。

这个例子中，可以马上看出，分类器$A$的$F_1$分数更高。假设$F_1$分数是结合查准率和查全率的合理方式，可以快速选出分类器$A$，淘汰分类器$B$。

很多机器学习团队就是这样，有一个定义明确的开发集(dev-set)用来测量查准率和查全率，再加上这样一个单一数值评估指标，有时叫单实数评估指标，能够快速判断分类器$A$或者分类器$B$更好。所以有这样一个开发集(dev-set)，加上单实数评估指标，你的迭代速度肯定会很快，它可以加速改进您的机器学习算法的迭代过程。

![](http://www.ai-start.com/dl2017/images/d16788ba017b04733720dfe7969ef838.png)

另一个例子，假设开发一个猫应用来服务四个地理大区的爱猫人士，分类器在不同市场和地理大区中的表现应该是有用的，但是通过跟踪四个数字，很难扫一眼这些数值就快速判断算法$A$或算法$B$哪个更好。所以在这个例子中，除了跟踪分类器在四个不同的地理大区的表现，也要算算平均值。假设平均表现是一个合理的单实数评估指标，通过计算平均值，就可以快速判断。

### 3. 满足和优化指标（Satisficing and optimizing metrics）

顾及到有些事情组合成单实数评估指标并不容易，这种情况下，设立满足和优化指标是很重要的。

![](http://www.ai-start.com/dl2017/images/f4574276832597fbc9d5977401896d15.png)

假设我们除了看重猫分类器的分类准确度，可以是$F_1$分数或者用其他衡量准确度的指标。但除了准确度之外，还需要考虑运行时间，就是需要多长时间来分类一张图。

可以这么做，将准确度和运行时间组合成一个整体评估指标。比如说，总体成本是$cost= accuracy - 0.5 \times\text{runningTime}$，只用这样的公式来组合准确度和运行时间，两个数值的线性加权求和，这种组合方式可能太刻意。

还可以有其他方式，比如：满足运行时间的要求下，选择最优准确度。所以在这种情况下，**准确度是一个优化指标，因为你想要准确度最大化，你想做的尽可能准确；运行时间就是我们所说的满足指标，它只需要小于100毫秒，达到之后就不在乎这指标有多好**，所以这是一个相当合理的权衡方式。

通过定义优化和满足指标，就可以提供一个明确的方式，去选择“最好的”分类器。==**更一般地说，如果要考虑$N$个指标，有时候选择其中一个指标做为优化指标是合理的。所以尽量优化那个指标，然后剩下$N-1$个指标都是满足指标，意味着只要它们达到一定阈值，就不在乎它超过那个门槛之后的表现，但它们必须达到这个门槛**==。

另一个例子，假设正在构建一个系统来检测唤醒语，也叫唤醒词。当有人说出其中的唤醒词时，有多大概率可以唤醒设备。可能需要顾及假阳性（**false positive**）的数量，就是没有人在说这个触发词时，它被随机唤醒的概率有多大。

在这种情况下，组合这两种评估指标的合理方式可能是最大化精确度。当某人说出唤醒词时，设备被唤醒的概率最大化，然后必须满足24小时内最多只能有1次假阳性。所以在这种情况下，准确度是优化指标，然后每24小时发生一次的假阳性是满足指标，只要每24小时最多有一次假阳性就满足了。

总结一下，如果需要顾及多个指标，比如说，有一个优化指标，想尽可能优化的，然后还有一个或多个满足指标，需要满足的，需要达到一定的门槛。满足优化指标就是一个全自动的方法，在观察多个成本大小时，选出"最好的"那个。

### 4. 训练/开发/测试集划分（Train/dev/test distributions）

设立训练集(train-set)，开发集(dev-set)和测试集(test-set)的方式也会影响团队在建立机器学习应用方面取得进展的速度。

**分布选择：**

如果开发集(dev-set)和测试集(test-set)来自不同的分布。在开发集(dev-set)上设立一个单实数评估指标，这就是像是定下目标，这就是要瞄准的靶心。因为一旦建立了这样的开发集(dev-set)和指标，团队就可以快速迭代，尝试不同的想法，跑实验，可以很快地使用开发集(dev-set)和指标去评估不同分类器，然后尝试选出最好的那个。在几个月工作之后发现，测试的时候，相当于把靶心移到不同的位置。

为了避免这种情况，==**将所有数据随机洗牌，放入开发集(dev-set)和测试集(test-set)，所以开发集(dev-set)和测试集(test-set)都含有来自不同区域的数据，并且开发集(dev-set)和测试集(test-set)都来自同一分布**==。

**数据集大小选择：**

在大数据时代旧的经验规则，70/30不再适用了。现在流行的是把大量数据分到训练集(train-set)，然后少量数据分到开发集(dev-set)和测试集(test-set)，特别是当有一个非常大的数据集时。在现代深度学习时代，有时我们拥有大得多的数据集，所以使用小于20%的比例或者小于30%比例的数据作为开发集(dev-set)和测试集(test-set)也是合理的。

关于测试集(test-set)，目的是完成系统开发之后，测试集(test-set)可以帮助我们评估投产系统的性能。方针就是，令测试集(test-set)足够大，能够以高信度评估系统整体性能。所以除非需要对最终投产系统有一个很精确的指标，一般来说测试集(test-set)不需要上百万个例子。对一般应用程序，也许有10,000个例子就能给出足够的置信度来给出性能指标了，这数目可能远远小于比如说整体数据集的30%，取决于有多少数据。

对于某些应用，也许不需要对系统性能有置信度很高的评估，也许只需要训练集(train-set)和开发集(dev-set)。不单独分出一个测试集(test-set)也是可以的。不建议在搭建系统时省略测试集(test-set)，因为有个单独的测试集(test-set)比较令人安心。因为可以使用这组不带偏差的数据来测量系统的性能。但如果开发集(dev-set)非常大，这样就不会对开发集(dev-set)过拟合得太厉害，这种情况，只有训练集(train-set)和测试集(test-set)也不是完全不合理的，不过一般不建议这么做。

**什么时候该改变开发/测试集和指标：**

之前已经讲过如何设置开发集(dev-set)和评估指标，就像是把目标定在某个位置，让团队瞄准。但有时候在项目进行途中，可能意识到，目标的位置放错了。这种情况下，应该移动你的目标。

看一个例子，假设构建了一个猫分类器，决定使用的指标是分类错误率。算法$A$和$B$分别有3％错误率和5％错误率，所以算法$A$似乎做得更好。但实际试一下这些算法，算法$A$由于某些原因，把很多色情图像分类成猫了。如果部署算法$A$，那么用户就会看到更多猫图，但它同时也会给用户推送一些色情图像，这是公司完全不能接受的。相比之下，算法$B$有5％的错误率，这样分类器就得到较少的图像，但它不会推送色情图像。所以从公司的角度来看，算法$B$实际上是一个更好的算法。

在这个例子中，发生的事情就是，算法A在评估指标上做得更好，它的错误率达到3%。但用户更倾向于使用算法$B$，因为它不会将色情图像分类为猫。所以当这种情况发生时，==**评估指标无法正确衡量算法之间的优劣排序时，原来的指标错误地预测算法$A$是更好的算法这就发出了信号，就应该改变评估指标了，或者要改变开发集(dev-set)或测试集(test-set)**==。

之前分类错误率指标可以写成这样：
$$
Error = \frac{1}{m_{{dev}}}\sum_{i = 1}^{m_{{dev}}}{I\{ y_{{pred}}^{(i)} \neq y^{(i)}\}}
$$
其中修改评估指标的一个方法是，在误差项上加权重项，即：
$$
Error = \frac{1}{\sum_{}^{}w^{(i)}}\sum_{i = 1}^{m_{{dev}}}{w^{(i)}I\{ y_{{pred}}^{(i)} \neq y^{(i)}\}}
$$
权重项称为$w^{\left( i \right)}$，如果图片$x^{(i)}$不是色情图片，则$w^{\left( i \right)} = 1$；如果$x^{(i)}$是色情图片，$w^{(i)}$可能就是10甚至100，这样就赋予了色情图片更大的权重，让算法将色情图分类为猫图时，错误率这个项快速变大。实际上要使用这种加权，必须自己过一遍开发集(dev-set)和测试集(test-set)，在开发集(dev-set)和测试集(test-set)里，把色情图片标记出来，这样才能使用这个加权函数。

但粗略的结论是，**如果对旧的错误率指标不满意，那就不要一直沿用不满意的错误率指标，而应该尝试定义一个新的指标**，能够更加符合偏好，定义出实际更适合的算法。

重新定义评估指标实际上是一个正交化的例子：

1. 第一步就是设定目标，重新要定义要瞄准的目标，这是完全独立的一步。
2. 第二步要做别的事情，在逼近目标的时候，为了修正目标，引入这些权重。

如何定义$J$并不重要，关键在于正交化的思路，把设立目标定为第一步，然后瞄准和射击目标是独立的第二步。在定义了指标之后，才能想如何优化系统来提高这个指标评分，比如改变你神经网络要优化的成本函数$J$。

再讲一个例子。假设猫分类器$A$和$B$，分别有用开发集(dev-set)评估得到3%的错误率和5%的错误率。因为一直在用从网上下载的高质量图片训练，但当部署到手机应用时，算法作用到用户上传的图片时，那些图片取景不专业，也许图像很模糊。所以在部署算法产品时，发现算法$B$看起来表现更好。

这是另一个指标和开发集(dev-set)测试集(test-set)出问题的例子。如果指标在当前开发集(dev-set)或者开发集(dev-set)和测试集(test-set)分布中表现很好，但实际应用程序，关注的地方表现不好，那么就需要修改指标或者开发测试集。

换句话说，如果开发测试集都是这些高质量图像，但在开发测试集上做的评估无法预测应用实际的表现。因为应用处理的是低质量图像，那么就应该改变开发测试集
，让数据更能反映实际需要处理好的数据。总体方针就是，如果当前的指标和当前用来评估的数据和真正关心必须做好的事情关系不大，那就应该更改指标或者开发测试集，让它们能更够好地反映算法需要处理好的数据。

有一个评估指标这真的可以加速团队迭代的速度。建议：**即使无法定义出一个很完美的评估指标和开发集(dev-set)，需要直接快速设立出来，然后使用它们来驱动团队的迭代速度。如果在这之后，发现选的不好，或有更好的想法，那么完全可以马上改。**对于大多数团队，建议最好不要在没有评估指标和开发集(dev-set)时跑太久，因为那样可能会减慢团队迭代和改善算法的速度。

### 5. 为什么是人的表现？（Why human-level performance?）

在过去的几年里，更多的机器学习团队一直在讨论如何比较机器学习系统和人类的表现，为什么呢？

有两个主要原因:

1. 首先是因为深度学习系统的进步，机器学习算法突然变得更好了。在许多机器学习的应用领域已经开始见到算法已经可以威胁到人类的表现了。
2. 其次，当试图让机器做人类能做的事情时，可以精心设计机器学习系统的工作流程，让工作流程效率更高，所以在这些场合，比较人类和机器是很自然的。

![](http://www.ai-start.com/dl2017/images/f44d03275801ce5ec97503851eb22ad5.png)

上图中蓝线表示人类水平，当开始往人类水平努力时，进展是很快的。但是过了一段时间，当这个算法表现比人类更好时，那么进展和精确度的提升就变得更慢了。也许它还会越来越好，但是在超越人类水平之后，它还可以变得更好，但性能增速，准确度上升的速度这个斜率，会变得越来越平缓。随着时间的推移，继续训练算法时，可能模型越来越大，数据越来越多，但是性能无法超过某个理论上限，这就是所谓的贝叶斯最优错误率（**Bayes optimal error**）。所以贝叶斯最优错误率一般认为是理论上可能达到的最优错误率，就是说没有任何办法设计出一个$x$到$y$的函数，让它能够超过一定的准确度。

事实证明，机器学习的进展往往相当快，直到超越人类的表现之前一直很快，当超越人类的表现时，有时进展会变慢。主要有两个原因：

1. 人类水平在很多任务中离贝叶斯最优错误率已经不远了，所以，当超越人类的表现之后也许没有太多的空间继续改善了。
2. 只要表现比人类的表现更差，那么实际上可以使用某些工具来提高性能。一旦超越了人类的表现，这些工具就没那么好用了。

只要机器学习算法比人类差，就可以从让人帮忙标记数据这样就有更多的数据可以喂给学习算法、进行人工错误率分析、也可以更好地分析偏差和方差。只要算法仍然比人类糟糕，就有这些重要策略可以改善算法。而一旦算法做得比人类好，这三种策略就很难利用了。所以这可能是另一个和人类表现比较的好处，特别是在人类做得很好的任务上。

### 6. 可避免偏差（Avoidable bias）

我们一般希望学习算法能在训练集(train-set)上表现良好，但有时实际上并不想做得太好。得知道人类水平的表现是怎样的，可以确切告诉我们算法在训练集(train-set)上的表现到底应该有多好，或者有多不好。

![](http://www.ai-start.com/dl2017/images/ac8eb51425d5dbf663d050398f7e8af8.png)

在猫分类器的例子中，假设学习算法达到8%的训练错误率和10%的开发错误率。分两种情况讨论：

- 人类具有近乎完美的准确度，假设人类水平的错误是1%。在这种情况下，算法在训练集(train-set)上的表现和人类水平的表现有很大差距，说明算法对训练集(train-set)的拟合并不好。所以从减少偏差和方差的工具这个角度看，在这种情况下，把重点放在减少偏差上。需要做的是，比如说训练更大的神经网络，或者跑久一点梯度下降，就试试能不能在训练集(train-set)上做得更好。
- 假设人类水平错误实际上是7.5%，也许数据集中的图像非常模糊，即使人类都无法判断这张照片中有没有猫。在这种情况下，系统在训练集(train-set)上的表现还好，它只是比人类的表现差一点点。这种情况下，减少学习算法的方差，可以试试正则化，让开发错误率更接近训练错误率。

人类实际上是非常擅长计算机视觉任务的，所以人类能做到的水平和贝叶斯错误率相差不远。

在这两种情况下，具有同样的训练错误率和开发错误率：

1. 当你认为贝叶斯错误率是1%时（这里我们使用人类水平错误率来替代贝叶斯错误率），8%的训练错误率真的很高，可以认为把它降到1%，那么减少偏差的手段可能有效。
2. 如果你认为贝叶斯错误率是7.5%，就知道没有太多改善的空间了，不能继续减少训练错误率了，你也不会希望它比7.5%好得多，因为这种目标只能通过可能需要提供更进一步的训练。可以将这个2%的差距缩小一点，使用减少方差的手段应该可行，比如正则化，或者收集更多的训练数据。

贝叶斯错误率和训练错误率之间的差值称为可避免偏差，可能希望一直提高训练集(train-set)表现，直到接近贝叶斯错误率。

而右图中这个2%是方差的指标，所以要减少这个2%比减少这个0.5%空间要大得多。而在左边的例子中，这7%衡量了可避免偏差大小，而2%衡量了方差大小。所以在左边这个例子里，专注减少可避免偏差可能潜力更大。

所以在这个例子中，当理解了人类水平错误率，理解了贝叶斯错误率，就可以在不同的场景中专注于不同的策略。在训练时如何考虑人类水平表现来决定工作着力点，具体怎么做还有更多微妙的细节……

### 7. 理解人的表现（Understanding human-level performance）

假设要观察放射科图像，然后作出分类诊断。假设一个普通的人类，在此任务上达到3%的错误率；普通的医生，能达到1%的错误率；经验丰富的医生做得更好，错误率为0.7%；还有一队经验丰富的医生团队，错误率为0.5%。应该如何界定人类水平错误率？人类水平错误率3%,1%, 0.7%还是0.5%？

思考人类水平错误率最有用的方式之一是，把它作为贝叶斯错误率的替代或估计。就是如果想要替代或估计贝叶斯错误率，我们知道贝叶斯错误率小于等于0.5%，最优错误率必须在0.5%以下。

为了发表研究论文或者部署系统，也许人类水平错误率的定义可以不一样，可以使用1%，只要超越了一个普通医生的表现，如果能达到这种水平，那系统已经达到实用了。

要点是：**在定义人类水平错误率时，要弄清楚目标所在**，如果要表明可以超越单个人类，那么就有理由在某些场合部署你的系统，也许这个定义是合适的。但是如果目标是替代贝叶斯错误率，那么0.5%才合适。

![](http://www.ai-start.com/dl2017/images/2bbcd885234523921d26848bd165e096.png)

为什么理解这个很重要，看一个错误率分析的例子。比方说，在医学图像诊断例子中，训练错误率是5%，开发错误率是6%。而人类表现水平，将它看成是贝叶斯错误率的替代品，可能会用1%或0.7%或0.5%。==**可避免偏差可以衡量现在应该是选择减少偏差还是减少误差**==。

第一个例子中，可避免偏差大概是4%，方差是1%。明显都比方差问题更大，在这种情况下，应该专注于减少偏差的技术。

第二个例子，方差是4%，人类水平表现是1%或0.7%或0.5%。不管使用哪一个定义，4%差距比任何一种定义的可避免偏差都大。所以主要使用减少方差的工具。

在人类可以做得很好的任务中，可以估计人类水平的错误率，可以使用人类水平错误率来估计贝叶斯错误率。所以你到贝叶斯错误率估计值的差距，告诉你可避免偏差问题有多大，而训练错误率和开发错误率之间的差值告诉你方差上的问题有多大，你的算法是否能够从训练集(train-set)泛化推广到开发集(dev-set)。

在之前的课程中，我们测量的是训练错误率，然后观察的是训练错误率比0%高多少，就用这个差值来估计偏差有多大。而事实证明，对于贝叶斯错误率几乎是0%的问题这样可行，例如识别猫，人类表现接近完美，所以贝叶斯错误率也接近完美。但数据噪点很多时，比如背景声音很嘈杂的语言识别，有时几乎不可能听清楚说的是什么，并正确记录下来。对于这样的问题，更好的估计贝叶斯错误率很有必要，可以帮助我们更好地估计可避免偏差和方差，这样就能更好的做出决策，选择减少偏差的策略，还是减少方差的策略。

**超过人的表现：**

机器学习进展，会在接近或者超越人类水平的时候变得越来越慢。举例谈谈为什么会这样：

- 假设一组人类专家充分讨论辩论之后，达到0.5%的错误率，单个人类专家错误率是1%。然后训练出来的算法有0.6%的训练错误率，0.8%的开发错误率。在这种情况下，0.5%是对贝叶斯错误率的估计，所以可避免偏差就是0.1%，然后方差是0.2%。和减少可避免偏差比较起来，减少方差可能空间更大。

  但假设算法可以得到0.3%训练错误率，还有0.4%开发错误率。现在，可避免偏差是什么呢？事实上训练错误率是0.3%，这是否意味着过拟合了0.2%，或者说贝叶斯错误率其实是0.1%呢？或者也许贝叶斯错误率是0.2%？或者贝叶斯错误率是0.3%呢？你真的不知道。基于本例中给出的信息，没有足够的信息来判断优化算法时应该专注减少偏差还是减少方差，这样取得进展的效率就会降低。

- 如果错误率已经比一群充分讨论辩论后的人类专家更低，那么依靠人类直觉去判断算法还能往什么方向优化就很难了。所以在这个例子中，一旦超过这个0.5%的门槛，要进一步优化机器学习问题就没有明确的选项和前进的方向了。

  这并不意味着不能取得进展，你仍然可以取得重大进展。但现有的一些工具帮助你指明方向的工具就没那么好用了。

现在，机器学习有很多问题已经可以大大超越人类水平了。主要集中在不是自然感知问题的问题上。人类在自然感知任务中往往表现非常好，所以有可能对计算机来说在自然感知任务的表现要超越人类要更难一些。

### 8. 改善你的模型的表现（Improving your model performance）

现在已经学过正交化，如何设立开发集(dev-set)和测试集(test-set)，用人类水平错误率来估计贝叶斯错误率以及如何估计可避免偏差和方差。总结一下写成一套指导方针：

1. 如果想提升机器学习系统的性能，先看看训练错误率和贝叶斯错误率估计值之间的距离，知道可避免偏差有多大。换句话说，就是还能做多好，对训练集(train-set)的优化还有多少空间。

   如果想减少可避免偏差，试试这样的策略：使用规模更大的模型，这样算法在训练集(train-set)上的表现会更好，或者训练更久。使用更好的优化算法，比如说加入**momentum**或者**RMSprop**，比如**Adam**。还可以试试寻找更好的新神经网络架构，或者说更好的超参数。可以改变激活函数，改变层数或者隐藏单位数，虽然这么做可能会让模型规模变大。或者试用其他模型，其他架构，如循环神经网络和卷积神经网络。

2. 看看开发错误率和训练错误率之间的距离，就知道方差问题有多大。换句话说，应该做多少努力让算法表现能够从训练集(train-set)推广到开发集(dev-set)。

   当发现方差是个问题时，可以试用以下这些技巧：可以收集更多数据，因为收集更多数据去训练可以帮你更好地推广到系统看不到的开发集(dev-set)数据。可以尝试正则化，包括$L2$正则化，**dropout**正则化或者我们在之前课程中提到的数据增强。同时你也可以试用不同的神经网络架构，超参数搜索，看看能不能帮助你，找到一个更适合你的问题的神经网络架构。

这些偏差、可避免偏差和方差的概念是容易上手，难以精通的。如果能系统全面地应用本周课程里的概念，实际上会比很多现有的机器学习团队更有效率、更系统、更有策略地系统提高机器学习系统的性能。