[TOC]

目标检测（Object detection）
---

### 1. 目标定位（Object localization）

在构建对象检测之前，需要先了解一下对象定位。构建神经网络的另一个问题，是定位分类问题。这意味着，不仅要用算法判断图片中是不是一辆汽车，还要在图片中标记出它的位置，这就是定位分类问题。其中“定位”的意思是判断目标在图片中的具体位置。

![](http://www.ai-start.com/dl2017/images/0107af10b33fcb955cc3c588dfb78d49.png)

在对象检测问题中，图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象。因此，图片分类的思路可以帮助学习分类定位，而对象定位的思路又有助于学习对象检测，所以先从分类和定位开始讲起。

图片分类问题已经并不陌生了，例如，输入一张图片到多层卷积神经网络，它会输出一个特征向量，并反馈给softmax单元来预测图片类型。

![](http://www.ai-start.com/dl2017/images/21b37dcb413e7c86464f88484796420c.png)

假设正在构建汽车自动驾驶系统，那么对象可能包括以下几类：行人、汽车、摩托车和背景，这四个分类就是softmax函数可能输出的结果，这就是标准的分类过程。

如果想定位图片中汽车的位置，可以让神经网络多输出几个单元，输出一个边界框。具体说就是让神经网络再多输出4个数字，标记为$b_{x}$,$b_{y}$,$b_{h}$和$b_{w}$，这四个数字是被检测对象的边界框的参数化表示。

约定图片左上角的坐标为$(0,0)$，右下角标记为$(1,1)$。要确定边界框的具体位置，需要指定红色方框的中心点，表示为($b_{x}$,$b_{y}$)，边界框的高度为$b_{h}$，宽度为$b_{w}$。

因此训练集不仅包含神经网络要预测的对象分类标签，还要包含表示边界框的四个数字，接着采用监督学习算法，输出一个分类标签。

目标标签$y$的定义如下：$y= \ \begin{bmatrix} p_{c} \\ b_{x} \\ b_{y} \\ b_{h} \\ b_{w} \\ c_{1} \\ c_{2}\\ c_{3} \\\end{bmatrix}$

它是一个向量，第一个组件$p_{c}$表示是否含有对象，如果对象属于前三类（行人、汽车、摩托车），则$p_{c}= 1$，如果是背景，则图片中没有要检测的对象，则$p_{c} =0$。如果检测到对象，就输出被检测对象的边界框参数$b_{x}$、$b_{y}$、$b_{h}$和$b_{w}$。

如果存在某个对象，那么$p_{c}=1$，同时输出$c_{1}$、$c_{2}$和$c_{3}$，表示该对象属于1-3类中的哪一类。如果图片中没有检测对象，$p_{c} =0$，$y$的其它参数将变得毫无意义，全部写成?，表示“毫无意义”的参数。因为图片中不存在检测对象，所以不用考虑网络输出中边界框的大小，也不用考虑图片中的对象是属于$c_{1}$、$c_{2}$和$c_{3}$中的哪一类。

针对给定的被标记的训练样本，不论图片中是否含有定位对象，构建输入图片$x$和分类标签$y$的具体过程都是如此，这些数据最终定义了训练集。

最后，关于神经网络的损失函数，其参数为类别$y$和网络输出$\hat{y}$，采用平方误差策略：

- 如果图片中存在定位对象，损失值等于每个元素相应差值的平方和，$L\left(\hat{y},y \right) = \left( \hat{y_1} - y_{1} \right)^{2} + \left(\hat{y_2} - y_{2}\right)^{2} + \ldots\left( \hat{y_8} - y_{8}\right)^{2}$
- 另一种情况是，$y_{1} = 0$，也就是$p_{c} = 0$，损失值$L\left(\hat{y},y \right) = \left(\hat{y_1} - y_{1}\right)^{2}$，因为对于这种情况，不用考虑其它元素，只需要关注神经网络输出$p_{c}$的准确度。

为了让大家了解对象定位的细节，这里用平方误差简化了描述过程。实际应用中，可以不对$c_{1}$、$c_{2}$、$c_{3}$和softmax激活函数应用对数损失函数，并输出其中一个元素值。通常做法是对边界框坐标应用平方差或类似方法，对$p_{c}$应用逻辑回归函数，甚至采用平方预测误差也是可以的。

以上就是利用神经网络解决对象分类和定位问题的详细过程，结果证明，利用神经网络输出批量实数来识别图片中的对象是个非常有用的算法。下节课分享另一种思路，就是把神经网络输出的实数集作为一个回归任务，这个思想也被应用于计算机视觉的其它领域，也是非常有效的。

### 2. 特征点检测（Landmark detection）

神经网络也可以通过输出图片上特征点的$(x,y)$坐标来实现对**目标特征的识别**。

![](http://www.ai-start.com/dl2017/images/100b265aefc4b0170fb39ed339e5181a.png)

假设构建一个人脸识别应用，出于某种原因，希望算法可以给出眼角的具体位置。也许除了这个特征点，还想得到更多的特征点输出值，图中眼眶上的红色特征点都是眼睛的特征点。还可以根据嘴部的关键点输出值来确定嘴的形状，从而判断人物是在微笑还是皱眉，也可以提取鼻子周围的关键特征点。为了便于说明，假设脸部有64个特征点，有些点甚至可以帮助我们定义脸部轮廓或下颌轮廓。选定特征点个数，并生成包含这些特征点的标签训练集，然后利用神经网络输出脸部关键特征点的位置。

具体做法：准备一个卷积网络和一些特征集，将人脸图片输入卷积网络，输出1或0，1表示有人脸，0表示没有人脸，然后输出（$l_{1x}$，$l_{1y}$）……直到（$l_{64x}$，$l_{64y}$）。用$l$代表一个特征，这里共有129个输出单元，由此实现对图片的人脸检测和定位。这只是一个识别脸部表情的基本构造模块，为了构建这样的网络，需要准备一个标签训练集，也就是图片$x$和标签$y$的集合，这些点都是人为辛苦标注的。

![](http://www.ai-start.com/dl2017/images/fb7fffc14dc60f98003a8ce20f527ab8.png)

最后一个例子，如果对人体姿态检测感兴趣，还可以定义一些关键特征点，如胸部的中点，左肩，左肘，腰等等。然后通过神经网络标注人物姿态的关键特征点，再输出这些标注过的特征点，就相当于输出了人物的姿态动作。

当然，要实现这个功能，需要设定这些关键特征点，从胸部中心点($l_{1x}$，$l_{1y}$)一直往下，直到($l_{32x}$，$l_{32y}$)。一旦了解如何用二维坐标系定义人物姿态，操作起来就相当简单了，批量添加输出单元，用以输出要识别的各个特征点的$(x,y)$坐标值。

要明确一点，特征点的特性在所有图片中必须保持一致，就好比，特征点1始终是右眼的外眼角，特征点2是右眼的内眼角，特征点3是左眼内眼角，特征点4是左眼外眼角等等，**标签在所有图片中必须保持一致**。假如雇用他人标记了一个足够大的数据集，那么神经网络便可以输出上述所有特征点，可以利用它们实现其他有趣的效果，比如判断人物的动作姿态，识别图片中的人物表情等等。

### 3. 目标检测（Object detection）

这节课学习通过卷积网络进行对象检测，采用的是基于滑动窗口的目标检测算法。

![](http://www.ai-start.com/dl2017/images/2f4e567978bb62fcbec093887de37783.png)

假如想构建一个汽车检测算法，步骤是，首先创建一个标签训练集，也就是$x$和$y$表示适当剪切的汽车图片样本。出于对这个训练集的期望，一开始可以使用适当剪切的图片，就是整张图片$x$几乎都被汽车占据。有了这个标签训练集，就可以开始训练卷积网络了。输入这些适当剪切过的图片，卷积网络输出$y=$0或1表示图片中有汽车或没有汽车。训练完这个卷积网络，就可以用它来实现滑动窗口目标检测，具体步骤如下：

![](http://www.ai-start.com/dl2017/images/c55f22f302899d5f9d77bef958465660.png)

- 假设有一张测试图片，首先选定一个特定大小的窗口，比如图片中红色窗口，将红色小方块中内容输入卷积神经网络，卷积网络开始进行预测，即判断红色方框内有没有汽车。

- 然后滑动窗口，目标检测算法接下来继续处理第二个图像，并输入给卷积网络，因此输入给卷积网络的只有红色方框内的区域。

- 再次运行卷积网络，然后处理第三个图像，依次重复操作，直到这个窗口滑过图像的每一个角落。

  思路是==**以固定步幅移动窗口，遍历图像的每个区域，把这些剪切后的小图像输入卷积网络，对每个位置按0或1进行分类**==，这就是所谓的图像滑动窗口操作。

  ![](http://www.ai-start.com/dl2017/images/c14524aa0534ed78c433e1cd0a8dff50.png)

- 重复上述操作，不过这次选择一个更大的窗口，截取更大的区域，并输入给卷积神经网络处理，可以根据卷积网络对输入大小调整这个区域，然后输入给卷积网络，输出0或1。
- 再以某个固定步幅滑动窗口，重复以上操作，遍历整个图像，输出结果。
- 然后第三次重复操作，这次选用更大的窗口。

这样做，不论汽车在图片的什么位置，总有一个窗口可以检测到它。这种算法叫作滑动窗口目标检测，以某个步幅滑动这些方框窗口遍历整张图片，对这些方形区域进行分类，判断里面有没有汽车。

**滑动窗口目标检测算法也有很明显的缺点，就是计算成本，因为在图片中剪切出太多小方块，卷积网络要一个个地处理。如果选用的步幅很大，显然会减少输入卷积网络的窗口个数，但是粗糙间隔尺寸可能会影响性能。反之，如果采用小粒度或小步幅，传递给卷积网络的小窗口会特别多，这意味着超高的计算成本**。

所以在神经网络兴起之前，人们通常采用更简单的分类器进行对象检测，比如通过采用手工处理工程特征的简单的线性分类器来执行对象检测。至于误差，因为每个分类器的计算成本都很低，它只是一个线性函数，所以滑动窗口目标检测算法表现良好，是个不错的算法。然而，卷积网络运行单个分类人物的成本却高得多，像这样滑动窗口太慢。除非采用超细粒度或极小步幅，否则无法准确定位图片中的对象。

### 4. 卷积的滑动窗口实现（Convolutional implementation of sliding windows）

上节课，学习了通过卷积网络实现滑动窗口对象检测算法，但效率很低。庆幸的是，计算成本问题已经有了很好的解决方案，大大提高了卷积层上应用滑动窗口目标检测器的效率。

为了构建滑动窗口的卷积应用，首先要知道如何把神经网络的全连接层转化成卷积层。

**全连接层转卷积层：**

假设对象检测算法输入一个14×14×3的图像，过滤器大小为5×5，数量是16，14×14×3的图像在过滤器处理之后映射为10×10×16。然后通过参数为2×2的最大池化操作，图像减小到5×5×16。然后添加一个连接400个单元的全连接层，接着再添加一个全连接层，最后通过softmax单元输出$y$，输出的是4个分类出现的概率。

![](http://www.ai-start.com/dl2017/images/38be387e37d131e44aff9d7fc9e3488a.png)

现在演示把这些全连接层转化为卷积层：它的前几层和之前的一样，而**对于全连接层，可以用5×5的过滤器来实现**，数量是400个，过滤器大小实际上是5×5×16，因为在卷积过程中，过滤器会遍历这16个通道，所以这两处的通道数量必须保持一致。

假设应用400个5×5×16过滤器，输出维度就是1×1×400，我们不再把它看作一个含有400个节点的集合，而是一个1×1×400的输出层。从数学角度看，它和全连接层是一样的，因为这400个节点中每个节点都有一个5×5×16维度的过滤器，所以每个值都是上一层这些5×5×16激活值经过某个任意线性函数的输出结果。

再添加另外一个卷积层，这里用的是1×1卷积，假设有400个1×1的过滤器，在这400个过滤器的作用下，下一层的维度是1×1×400，它其实就是上个网络中的全连接层。

最后经由1×1过滤器的处理，得到一个softmax激活值，通过卷积网络，最终得到这个1×1×4的输出层，而不是4个数字。

**卷积实现滑动窗口对象检测算法：**

![](http://www.ai-start.com/dl2017/images/00c4fb1a1af9b50f0fd0bcf5eacca6ff.png)

假设向滑动窗口卷积网络输入14×14×3的图片，和前面一样，神经网络最后的输出层，即softmax单元的输出是1×1×4。

![](http://www.ai-start.com/dl2017/images/ad1743ff113f9d30080f63a16c74ed64.png)

假设测试集图片是16×16×3，现在给这个输入图片加上黄色条块。在最初的滑动窗口算法中，把蓝色区域输入卷积网络生成0或1分类。接着滑动窗口，步幅为2个像素，向右滑动2个像素，将绿框区域输入给卷积网络，运行整个卷积网络，得到另外一个标签0或1。继续将橘色区域输入给卷积网络，卷积后得到另一个标签，最后对右下方的紫色区域进行最后一次卷积操作。

我们在这个16×16×3的小图像上滑动窗口，卷积网络运行了4次，于是输出了了4个标签。结果发现，这4次卷积操作中很多计算都是重复的。执行滑动窗口的卷积时使得卷积网络在这4次前向传播过程中共享很多计算，尤其是在编号1操作中，卷积网络运行同样的参数，使得相同的5×5×16过滤器进行卷积操作，得到12×12×16的输出层。然后执行同样的最大池化，输出结果6×6×16，and so on... 

最终，在输出层这4个子方块中，蓝色的是图像左上部分14×14的输出（红色箭头标识），右上角方块是图像右上部分（绿色箭头标识）的对应输出，左下角方块是输入层左下角（橘色箭头标识），右下角方块是卷积网络处理输入层右下角14×14区域(紫色箭头标识)的结果。最后的输出层就是这个14×14区域经过卷积网络处理后的结果。

如果想了解具体的计算步骤，以绿色方块为例，传递给卷积网络，第一层的激活值就是对应的绿色阴影块，最大池化后的下一层的激活值是这块的绿色阴影块，这块区域对应着后面几层输出的右上角方块（编号4，5，6）。

==所以该卷积操作的原理是我们不需要把输入图像分割成四个子集，分别执行前向传播，而是**把它们作为一张图片输入给卷积网络进行计算，其中的公共区域可以共享很多计算**==，就像这里我们看到的这个4个14×14的方块一样。

![](http://www.ai-start.com/dl2017/images/5fd2f8d039a3bfc5187dfe33f5276235.png)

再看一个更大的图片样本，假如对一个28×28×3的图片应用滑动窗口操作，如果以同样的方式运行前向传播，最后得到8×8×4的结果。跟上一个范例一样，以14×14区域滑动窗口，首先在最开始的区域应用滑动窗口，其结果对应输出层的左上角部分。接着以大小为2的步幅不断地向右移动窗口，直到第8个单元格，得到输出层的第一行。然后向图片下方移动，最终输出这个8×8×4的结果。因为最大池化参数为2，相当于以大小为2的步幅在原始图片上应用神经网络。

![](http://www.ai-start.com/dl2017/images/84a6a0505acc165c6600d4b6f03d5e3c.png)

总结一下滑动窗口的实现过程，在图片上剪切出一块区域，假设它的大小是14×14，把它输入到卷积网络。继续输入下一块区域，大小同样是14×14，重复操作，直到某个区域识别到汽车。

但是正如上面所讲的，我们不能依靠连续的卷积操作来识别图片中的汽车，可以直接对大小为28×28的整张图片进行卷积操作，一次得到所有预测值，如果足够幸运，神经网络便可以识别出汽车的位置。

不过这种算法仍然存在一个缺点，就是边界框的位置可能不够准确。

### 5. Bounding Box预测（Bounding box predictions）

学习如何在滑动窗口法的卷积实现中得到更精准的边界框。

![](http://www.ai-start.com/dl2017/images/e4bebc707829a1610572f43f8e0995c9.png)

其中一个能得到更精准边界框的算法是YOLO算法，YOLO(You only look once)意思是你只看一次。

**实现：**

比如输入图像是100×100的，然后在图像上放一个网格。为了介绍起来简单一些，用3×3网格，实际实现时会用更精细的网格，可能是19×19。基本思路是使用图像分类和定位算法（在9个格子中逐一应用第一节课的图像分类和定位算法）。

更具体一点，对于9个格子中的每一个指定一个标签$y$，$y$是8维的，$y= \ \begin{bmatrix} p_{c} \\ b_{x} \\ b_{y} \\ b_{h} \\ b_{w} \\ c_{1} \\ c_{2}\\ c_{3} \\\end{bmatrix}$，$p_{c}$等于0或1取决于格子中是否有图像。然后$b_{x}$、$b_{y}$、$b_{h}$和$b_{w}$作用就是，如果那个格子里有对象，那么就给出边界框坐标。然后$c_{1}$、$c_{2}$和$c_{3}$就是想要识别的三个类别，$c_{1}$、$c_{2}$和$c_{3}$可以是行人、汽车和摩托车类别。这张图里有9个格子，所以对于每个格子都有这么一个向量。

![](http://www.ai-start.com/dl2017/images/fb08477cd3937f7df0deddc1de1d2920.png)



**取对象的中点，然后将这个对象分配给包含对象中点的格子**。所以左边的汽车就分配到编号4格子上，然后右边这辆分配给编号6格子。所以即使编号5格子同时有两辆车的一部分，我们就假装中心格子没有任何我们感兴趣的对象，分类标签$y= \ \begin{bmatrix} 0 \\ ? \\ ? \\ ? \\ ? \\ ? \\ ? \\ ? \\\end{bmatrix}$。而对于编号4格子，目标标签$y= \begin{bmatrix} 1 \\ b_{x} \\ b_{y} \\ b_{h} \\ b_{w} \\ 0 \\ 1 \\0 \\\end{bmatrix}$。右边编号6格子也是类似的，$y=\begin{bmatrix} 1 \\ b_{x} \\ b_{y} \\ b_{h} \\ b_{w} \\ 0 \\ 1 \\0 \\ \end{bmatrix}$作为目标向量。

所以对于这里9个格子中任何一个，都会得到一个8维输出向量$y$，因为这里是3×3的网格，总的输出尺寸是3×3×8。

![](http://www.ai-start.com/dl2017/images/98633e9df22fd06cfc21af2e7d39bbb6.png)

对于这个例子中，左上格子是1×1×8，对应的是9个格子中左上格子的输出向量。所以对于这3×3中每一个位置而言，都对应一个8维输出目标向量$y$，如果这里没有对象的话，其中一些值可以是don‘t cares（即？）。所以总的目标输出，这个图片的输出标签尺寸就是3×3×8。

**应用：**

如果要训练一个输入为100×100×3的神经网络，进行普通的卷积网络，卷积层，最大池化层等等，**最后就映射到一个3×3×8输出尺寸**。所以要做的是，有一个输入$x$，然后有这些3×3×8的目标标签$y$，用反向传播训练神经网络时，将任意输入$x$映射到这类输出向量$y$。

![](http://www.ai-start.com/dl2017/images/a6d32959543c502ee18765cf20495bc2.png)

这个算法的优点在于神经网络可以输出精确的边界框，所以测试的时候，是喂入输入图像$x$，然后跑正向传播，直到得到这个输出$y$。只要每个格子中对象数目没有超过1个，这个算法应该是没问题的。一个格子中存在多个对象的问题，实践中可能会使用更精细的19×19网格，所以输出就是19×19×8。这样的网格精细得多，那么多个对象分配到同一个格子得概率就小得多。

**==把对象分配到一个格子的过程是，观察对象的中点，然后将这个对象分配到其中点所在的格子，即使对象可以横跨多个格子==**，也只会被分配到9个格子其中之一，或者19×19网络的其中一个格子。在19×19网格中，两个对象的中点（图中蓝色点所示）处于同一个格子的概率就会更低。

YOLO算法能让神经网络输出边界框，可以具有任意宽高比，并且能输出更精确的坐标，不会受到滑动窗口分类器的步长大小限制。其次，这是一个卷积实现，并没有在3×3网格上跑9次算法。相反，这是单次卷积实现，使用了一个卷积网络，有很多共享计算步骤，在处理这3×3计算中很多计算步骤是共享的，所以这个算法效率很高。

事实上YOLO算法有一个好处，也是它受欢迎的原因，因为这是一个卷积实现，实际上它的运行速度非常快，可以达到实时识别。

**编码边界框：**

![](http://www.ai-start.com/dl2017/images/395c57c64bb792a813a904b135423937.png)

在YOLO算法中，约定每个格子左上角的点是$(0,0)$，右下点是$(1,1)$，要指定橙色中点的位置：
$b_{x}$大概是0.4，因为它的位置大概是水平长度的0.4，$b_{y}$大概是0.3。
边界框的高度用格子总体宽度的比例表示，所以$b_{h}$是0.9，$b_{w}$就是0.5。

换句话说，$b_{x}$、$b_{y}$、$b_{h}$和$b_{w}$单位是相对于格子尺寸的比例，所以==**$b_{x}$和$b_{y}$必须在0和1之间**==，因为从定义上看，橙色点位于对象分配到格子的范围内，如果它不在0和1之间，如果它在方块外，那么这个对象就应该分配到另一个格子上。==**$b_{h}$和$b_{w}$可能会大于1**==，特别是如果有一辆汽车的边界大于格子大小，那么边界框的宽度和高度有可能大于1。指定边界框的方式有很多，但这种约定是比较合理的，如果去读YOLO的研究论文，有其他参数化的方式，可能效果会更好，这里只给出了一个合理的约定，用起来应该没问题。

### 6. 交并比（Intersection over union）

使用并交比函数，可以用来评价对象检测算法。

![](http://www.ai-start.com/dl2017/images/38eea69baa46091d516a0b7a33e5379e.png)

在对象检测任务中，希望能够同时定位对象，所以如果实际边界框刚好是紫色的边界框，那么这个结果是好还是坏？

**交并比（loU）函数做的是计算两个边界框交集和并集之比**。两个边界框的并集是绿色阴影区域，而交集就是黄色阴影区域，那么交并比就是：黄色阴影的交集大小除以绿色阴影的并集面积。

一般约定，如果$loU≥0.5$，就说检测正确，如果预测器和实际边界框完美重叠，loU就是1，因为交集就等于并集。但一般来说只要$loU≥0.5$，那么结果是可以接受的。

一般约定，0.5是阈值，用来判断预测的边界框是否正确。也可以将loU定得更高，loU越高，边界框越精确。这是衡量定位精确度的一种方式，很少见到有人将阈值降到0.5以下。

定义loU这个概念是为了评价对象定位算法是否精准，但更一般地说，loU衡量了两个边界框重叠地相对大小，也可以判断两个边界框是否相似。这个工具可以让YOLO算法输出效果更好。

### 7. 非极大值抑制（Non-max suppression）

对象检测算法可能对同一个对象做出多次检测，非极大值抑制这个方法可以确保算法对每个对象只检测一次。

![](http://www.ai-start.com/dl2017/images/a86a2edbb89014e193ab613a162cff58.png)

当运行对象分类和定位算法时，对于每个格子都运行一次，编号1格子可能会认为这辆车中点应该在格子内部，编号2、3格子也会这么认为。所以不仅仅是一个格子，也许其他格子也会觉得它们格子内有车。

因为要在361个格子上都运行一次图像检测和定位算法，那么可能很多格子都会举手说我的$p_{c}=1$，而不是361个格子中仅有两个格子会报告它们检测出一个对象。所以当运行算法的时候，最后可能会对同一个对象做出多次检测。非极大值抑制做的就是清理这些检测结果。这样一辆车只检测一次，而不是每辆车都触发多次检测。

![](http://www.ai-start.com/dl2017/images/78f2b2a2efdbd6aebe034ce30cda440b.png)

这个算法做的是，首先看看每次报告每个检测结果相关的概率$p_{c}$，实际上是$p_{c}$乘以$c_{1}$、$c_{2}$或$c_{3}$。$p_{c}$检测概率，首先看概率最大的那个，这个例子中右边车辆是0.9，就说这是最可靠的检测，然后就用高亮标记。

这么做之后，非极大值抑制就会逐一审视剩下的矩形，就会去掉其他loU值很高的矩形。其余两个矩形$p_{c}$分别是0.6和0.7，这两个矩形和淡蓝色矩形重叠程度很高，所以会被抑制，变暗，表示它们被抑制了。剩下高亮显示的那些，就是最后得到的两个预测结果。

所以这就是非极大值抑制，**非最大值意味着只输出概率最大的分类结果，抑制很接近但不是最大的其他预测结果**，所以这方法叫做非极大值抑制。

**算法细节：**

1. 首先在19×19网格上执行一下YOLO算法，最后会得到一个19×19×8的输出尺寸。

2. 现在要实现非极大值抑制，做的第一件事是，去掉$p_{c}$小于或等于某个阈值（比如阈值为0.6）的所有边界框。

   除非算法认为这里存在对象的概率至少有0.6，否则就抛弃，所以这就抛弃了所有概率比较低的输出边界框。思路是对于这361个位置，输出一个边界框，还有那个最好边界框所对应的概率，只是抛弃所有低概率的边界框。

3. 接下来在剩下的边界框中，选择概率$p_{c}$最高的边界框，然后把它输出成预测结果，让它高亮显示，这样就可以确定输出有一辆车的预测。

4. 最后去掉所有剩下的边界框，任何没有达到输出标准的边界框，和上一步输出边界框有很高交并比的边界框全部抛弃。

   所以while循环，在还有剩下边界框的时候，一直这么做，把没处理的都处理完，直到每个边界框都判断过了。它们有的作为输出结果，剩下的会被抛弃，它们和输出结果交并比太高，和刚刚输出存在对象结果的重叠程度过高。

如果尝试同时检测三个对象，比如说行人、汽车、摩托，事实证明，正确的做法是独立进行三次非极大值抑制，对每个输出类别都做一次。

### 8. Anchor Boxes

到目前为止，对象检测中存在的一个问题是每个格子只能检测出一个对象，如果想让一个格子检测出多个对象，使用anchor box这个概念。

![](http://www.ai-start.com/dl2017/images/49b7d68a17e89dd109f96efecc223f5a.png)

假设有一张图片，行人的中点和汽车的中点几乎在同一个地方，两者都落入到同一个格子中。对于那个格子，如果 $y$ 输出这个向量$y= \ \begin{bmatrix} p_{c} \\ b_{x} \\ b_{y} \\ b_{h} \\ b_{w} \\ c_{1} \\ c_{2}\\ c_{3} \\\end{bmatrix}$，可以检测这三个类别，行人、汽车和摩托车，它将无法输出检测结果，所以必须从两个检测结果中选一个。 

![](http://www.ai-start.com/dl2017/images/e001f5f3d2afa76a1c3710bd60bcad00.png)

而anchor box的思路是，预先定义两个不同形状的anchor box，要做的是把预测结果和这两个anchor box关联起来。一般来说，可能会用更多的anchor box。

![](http://www.ai-start.com/dl2017/images/2e357b5b92122660c550dcfb0901519c.png)

要做的是定义类别标签，用的向量不再是之前的$\begin{bmatrix} p_{c} & b_{x} &b_{y} & b_{h} & b_{w} & c_{1} & c_{2} & c_{3} \\\end{bmatrix}^{T}$，而是重复两次，$y=  \begin{bmatrix} p_{c} & b_{x} & b_{y} &b_{h} & b_{w} & c_{1} & c_{2} & c_{3} & p_{c} & b_{x} & b_{y} & b_{h} & b_{w} &c_{1} & c_{2} & c_{3} \\\end{bmatrix}^{T}$。前面的$p_{c},b_{x},b_{y},b_{h},b_{w},c_{1},c_{2},c_{3}$（绿色方框标记的参数）是和anchor box 1关联的8个参数，后面的8个参数（橙色方框标记的元素）是和anchor box 2相关联。

因为行人的形状更类似于anchor box 1的形状，编码$p_{c} =1$，代表有个行人，用$b_{x},b_{y},b_{h}$和$b_{w}$来编码包住行人的边界框，然后用$c_{1},c_{2},c_{3}$($c_{1}= 1,c_{2} = 0,c_{3} = 0$)来说明这个对象是个行人。

因为车子的边界框更像anchor box 2的形状，可以编码为($p_{c}= 1,b_{x},b_{y},b_{h},b_{w},c_{1} = 0,c_{2} = 1,c_{3} = 0$)。

**总结：**

在用anchor box之前，对于训练集图像中的每个对象，输出$y$就是3×3×8。现在用到anchor box这个概念，每个对象都分配到对象中点所在的格子中，以及分配到和对象形状交并比最高的anchor box中。

这里有两个anchor box，然后观察哪一个anchor box和实际边界框的交并比更高。不管选的是哪一个，这个对象不只分配到一个格子，而是分配到一对，所以现在输出 $y$ 就是3×3×16，如果有更多对象，那么$y$ 的维度会更高。

如果行人走开了，只有一辆车，那么anchor box 2分量还是一样的，对于anchor box 1要填的就是，里面没有任何对象，所以 $p_{c} =0$，然后剩下的就是don’t care-s(即？)。

**补充：**

还有一些额外的细节，如果有两个anchor box，但在同一个格子中有三个对象，这种情况算法处理不好，希望这种情况不会发生，但如果真的发生了，这个算法并没有很好的处理办法。对于这种情况，就只能引入一些打破僵局的默认手段。

还有这种情况，两个对象都分配到一个格子中，而且它们的anchor box形状也一样，这是算法处理不好的另一种情况，需要引入一些打破僵局的默认手段，专门处理这种情况，希望你的数据集里不会出现这种情况，其实出现的情况不多，所以对性能的影响应该不会很大。

建立anchor box这个概念，是为了处理两个对象出现在同一个格子的情况，实践中这种情况很少发生，特别是用的是19×19网格而不是3×3的网格，两个对象中点处于361个格子中同一个格子的概率很低。

最后，对于anchor box的选择，人们一般手工指定anchor box形状，可以选择5到10个anchor box形状，覆盖到多种不同的形状，可以涵盖想要检测的对象的各种形状。

还有一个更高级的版本，k-Means，可以将两类对象形状聚类，如果我们用它来选择一组anchor box，选择最具有代表性的一组anchor box，可以代表试图检测的十几个对象类别。这其实是自动选择anchor box的高级方法。

### 9. YOLO 算法（Putting it together: YOLO algorithm）

把所有组件组装在一起构成YOLO对象检测算法。

![](http://www.ai-start.com/dl2017/images/36ff927836cfcd7fee9413e2d34757d8.png)

假设要训练一个算法去检测三种对象，行人、汽车和摩托车，如果要用两个anchor box，那么输出 $y$ 就是3×3×2×8，2是anchor box的数量，8是向量维度，8实际上先是5（$p_{c},b_{x},b_{y},b_{h},b_{w}$）再加上类别的数量（$c_{1},c_{2},c_{3}$）。可以将它看成是3×3×2×8，或者3×3×16。要构造训练集，需要遍历9个格子，然后构成对应的目标向量$y$。

所以先看第一个格子，里面没什么有价值的东西，行人、车子和摩托车，三个类别都没有出现在左上格子中，所以对应那个格子目标$y= \begin{bmatrix} 0 & ? & ? & ? & ? & ? & ? & ? & 0 & ? & ? & ? & ? & ? & ? & ?\\ \end{bmatrix}^{T}$，第一个anchor box的 $p_{c}$ 是0，因为没什么和第一个anchor box有关的，第二个anchor box的 $p_{c}$ 也是0，剩下这些值是don’t care-s。

网格中大多数格子都是空的，但编号2的格子有目标向量$y =\begin{bmatrix} 0 & ? & ? & ? & ? & ? & ? & ? & 1 & b_{x} & b_{y} & b_{h} &b_{w} & 0 & 1 & 0 \\\end{bmatrix}^{T}$。

![](http://www.ai-start.com/dl2017/images/e23084f4a75246f08ea4cedef55f60ab.png)

所以这样遍历9个格子，每个格子会得到一个16维向量，所以最终输出尺寸就是3×3×16。实践中用的可能是19×19×16，或者需要用到更多的anchor box，可能是19×19×5×8，即19×19×40，用了5个**anchor box**。

有一个训练集，然后训练一个卷积网络，输入是图片，可能是100×100×3，然后你的卷积网络最后输出尺寸是，在我们例子中是3×3×16或者3×3×2×8。

**算法实现：**

![](http://www.ai-start.com/dl2017/images/140cd14e52c3f217d053ae0efa2fce81.png)

输入图像，神经网络的输出尺寸是这个3×3×2×8，对于9个格子，每个都有对应的向量。

![](http://www.ai-start.com/dl2017/images/23256c4b7b28d62d34a744f5fb5e9c3b.png)

最后要运行一下非极大值抑制，如果使用两个anchor box，那么对于9个格子中任何一个都会有两个预测的边界框，其中一个的概率$p_{c}$很低，9个格子中，每个都有两个预测的边界框。有一些边界框可以超出所在格子的高度和宽度，接下来抛弃概率很低的预测。

最后，如果有三个对象检测类别，希望检测行人，汽车和摩托车，那么要做的是，**对于每个类别单独运行非极大值抑制**，处理预测结果所属类别的边界框。运行三次来得到最终的预测结果。所以算法的输出最好能够检测出图像里所有的车子，还有所有的行人。

这就是YOLO对象检测算法，这实际上是最有效的对象检测算法之一，包含了整个计算机视觉对象检测领域文献中很多最精妙的思路。

### 10. 候选区域（选修）（Region proposals (Optional)）

如果阅读对象检测的文献，可能会看到一组概念，所谓的候选区域，这在计算机视觉领域是非常有影响力的概念。

滑动窗口法这个算法的其中一个缺点是，它在显然没有任何对象的区域浪费时间。

有一种叫做R-CNN的算法，意思是带区域的卷积网络。这个算法尝试选出一些区域，在这些区域上运行卷积网络分类器是有意义的，所以这里不再针对每个滑动窗运行检测算法，而是只选择一些窗口，在少数窗口上运行卷积网络分类器。

![](http://www.ai-start.com/dl2017/images/e78e4465af892d0965e2b0863263ef8c.png)

选出候选区域的方法是运行图像分割算法，找出可能存在对象的区域。比如说，分割算法得到一个色块，然后在这个色块上运行分类器。在这种情况下，如果在蓝色色块上（编号3）运行分类器，希望能检测出一个行人，在青色色块(编号4)上运行算法，也许可以发现一辆车。

这个细节就是所谓的分割算法，先找出可能2000多个色块，将这2000个色块上放置边界框，然后在这2000个色块上运行分类器，这样需要处理的位置可能要少的多，可以减少卷积网络分类器运行时间，比在图像所有位置运行一遍分类器要快。

现在不仅是在方形区域（编号5）中运行卷积网络，还会在高高瘦瘦（编号6）的区域运行，尝试检测出行人，然后在很宽很胖的区域（编号7）运行，尝试检测出车辆，同时在各种尺度运行分类器。

这就是R-CNN的特色概念，现在看来R-CNN算法还是很慢的。所以有一系列的研究工作去改进这个算法，**R-CNN的基本算法是使用某种算法求出候选区域，然后对每个候选区域运行一下分类器，每个区域会输出一个标签，并输出一个边界框**，这样就能在确实存在对象的区域得到一个精确的边界框。

R-CNN算法不会直接信任输入的边界框，它也会输出一个边界框$b_{x}$，$b_{y}$，$b_{h}$和$b_{w}$，这样得到的边界框比较精确，比单纯使用图像分割算法给出的色块边界要好，所以它可以得到相当精确的边界框。

![](http://www.ai-start.com/dl2017/images/e6ed1aa3263107d4e189dd75adc060b4.png)

这些年来有一些对R-CNN算法的改进工作，有一个Fast R-CNN算法，它基本上是R-CNN算法，不过用卷积实现了滑动窗法。最初的算法是逐一对区域分类的，所以快速R-CNN用的是滑动窗法的一个卷积实现，这和卷积的滑动窗口实现中的大致相似，这显著提升了R-CNN的速度。

Fast R-CNN算法的其中一个问题是得到候选区域的聚类步骤仍然非常缓慢。还有一个Faster R-CNN使用的是卷积神经网络，而不是更传统的分割算法来获得候选区域色块，结果比Fast R-CNN算法快得多。现在来说大多数Faster R-CNN的算法实现还是比YOLO算法慢很多。

候选区域的概念在计算机视觉领域的影响力相当大，所以可以了解一下这些算法。这个方法需要两步，**首先得到候选区域，然后再分类**，相比之下，能够一步做完，类似于YOLO这个算法，是长远而言更有希望的方向。可能会碰到其他人在用，所以这也是值得了解的。