[TOC]

## 深度卷积网络实例探究

首先来看看一些卷积神经网络的实例分析，为什么要看这些实例分析呢？

事实上，过去几年计算机视觉研究中的大量研究都集中在如何把卷积层、池化层以及全连接层这些组件组合起来，形成有效的卷积神经网络。最直观的方式之一就是去看一些案例，就像很多人通过看别人的代码来学习编程一样，通过研究别人构建有效组件的案例是个不错的办法。

![](http://www.ai-start.com/dl2017/images/7fb4f4ae7f3dcd200fcaacd5a3188d51.png)

实际上在计算机视觉任务中表现良好的神经网络框架往往也适用于其它任务，也许你的任务也不例外。也就是说，如果有人已经训练或者计算出擅长识别猫、狗、人的神经网络或者神经网络框架，而你的计算机视觉识别任务是构建一个自动驾驶汽车，完全可以借鉴别人的神经网络框架来解决自己的问题。

### 1. 经典网络（Classic networks）

- **LeNet-5**

  ![](http://www.ai-start.com/dl2017/images/a3931d10222ebaf43490935d391eeceb.png)

  假设有一张32×32×1的图片，LeNet-5可以识别图中的手写数字。**LeNet-5是针对灰度图片训练的**，所以图片的大小只有32×32×1。

  使用6个5×5的过滤器，步幅为1，padding为0，输出结果为28×28×6，图像尺寸从32×32缩小到28×28，然后进行池化操作。在这篇论文写成的那个年代，人们更喜欢使用平均池化，而现在我们可能用最大池化更多一些。在这个例子中，我们进行平均池化，过滤器的宽度为2，步幅为2，图像的尺寸，高度和宽度都缩小了2倍，输出结果是一个14×14×6的图像。

  接下来是卷积层，用一组16个5×5的过滤器，新的输出结果有16个通道。LeNet-5的论文是在1998年撰写的，当时人们并不使用padding，或者总是使用valid卷积，这就是为什么每进行一次卷积，图像的高度和宽度都会缩小，所以这个图像从14到14缩小到了10×10。然后又是池化层，高度和宽度再缩小一半，输出一个5×5×16的图像。将所有数字相乘，乘积是400。

  后面是全连接层，在全连接层中，有400个节点，每个节点有120个神经元，这里已经有了一个全连接层。但有时还会从这400个节点中抽取一部分节点构建另一个全连接层，如图有2个全连接层。

  最后一步就是利用这84个特征得到最后的输出，还可以在最后加一个节点用来预测$\hat{y}$的值，$\hat{y}$有10个可能的值，对应识别0-9这10个数字。在现在的版本中则使用softmax函数输出十种分类结果，而在当时，LeNet-5网络在输出层使用了另外一种，现在已经很少用到的分类器。

  相比现代版本，这里得到的神经网络会小一些，只有约6万个参数。而现在，我们经常看到含有一千万到一亿个参数的神经网络，比这大1000倍的神经网络也不在少数。

  不管怎样，如果从左往右看，随着网络越来越深，图像的高度和宽度在缩小，从最初的32×32缩小到28×28，再到14×14、10×10，最后只有5×5。与此同时，随着网络层次的加深，通道数量一直在增加，从1增加到6个，再到16个。

  这个神经网络中还有一种模式至今仍然经常用到，就是一个或多个卷积层后面跟着一个池化层，然后又是若干个卷积层再接一个池化层，然后是全连接层，最后是输出，这种排列方式很常用。

  读到这篇经典论文时，可以发现，过去人们使用sigmod函数和tanh函数，而不是ReLu函数。这种网络结构的特别之处还在于，各网络层之间是有关联的，这在今天看来显得很有趣。比如说，有一个$n_{H} \times n_{W} \times n_{C}$的网络，有$n_{C}$个通道，使用尺寸为$f×f×n_{C}$的过滤器。由于在当时，计算机的运行速度非常慢，为了减少计算量和参数，经典的LeNet-5网络使用了非常复杂的计算方式，每个过滤器都采用和输入模块一样的通道数量。论文中提到的这些复杂细节，现在一般都不用了。

  经典的LeNet-5网络在池化后进行了非线性函数处理，在这个例子中，池化层之后使用了sigmod函数。如果真的去读这篇论文，这会是最难理解的部分之一，后面的课程中讲到。

  以上内容来自于原文的第二段和第三段，原文的后几段介绍了另外一种思路。文中提到的这种图形变形网络如今并没有得到广泛应用，所以在读这篇论文的时候，建议精读第二段，这段重点介绍了这种网络结构。泛读第三段，里面主要是一些有趣的实验结果。

- **AlexNet**

  ![](http://www.ai-start.com/dl2017/images/6a9d80274dc6315d9d8c0c27b81548b4.png)

  首先用一张227×227×3的图片作为输入（实际上原文中使用的图像是224×224×3，但227×227这个尺寸更好一些）。

  第一层使用96个11×11的过滤器，步幅为4，由于步幅是4，因此尺寸缩小到55×55，缩小了4倍左右。
  然后用一个3×3的过滤器构建最大池化层，$f=3$，步幅$s$为2，卷积层尺寸缩小为27×27×96。
  接着再执行一个5×5的same卷积，padding之后，输出是27×27×276。
  然后再次进行最大池化，尺寸缩小到13×13x256。
  再执行一次same卷积，相同的padding，384个过滤器，得到的结果是13×13×384。
  再做一次same卷积，再做一次同样的操作，最后再进行一次最大池化，尺寸缩小到6×6×256。

  6×6×256等于9216，将其展开为9216个单元，然后是一些全连接层。最后使用softmax函数输出识别的结果，看它究竟是1000个可能的对象中的哪一个。

  实际上，这种神经网络与LeNet有很多相似之处，不过AlexNet要大得多。前面讲到的LeNet大约有6万个参数，而AlexNet包含约6000万个参数。当用于训练图像和数据集时，**AlexNet能够处理非常相似的基本构造模块**，这些模块往往包含着大量的隐藏单元或数据，这一点AlexNet表现出色。**AlexNet比LeNet表现更为出色的另一个原因是它使用了ReLu激活函数**。

  这篇论文在写的时候，GPU的处理速度还比较慢，所以AlexNet采用了非常复杂的方法在两个GPU上进行训练。大致原理是，这些层分别拆分到两个不同的GPU上，同时还专门有一个方法用于两个GPU进行交流。

  ![](http://www.ai-start.com/dl2017/images/1c258ebaf8c400984a0bc777232fe1d6.png)

  论文还提到，经典的AlexNet结构还有另一种类型的层，叫作“局部响应归一化层”（Local Response Normalization），即LRN层，这类层应用得并不多，所以并没有专门讲。局部响应归一层的基本思路是，假如这是网络的一块，比如是13×13×256，LRN要做的就是选取一个位置，如上所画，从这个位置穿过整个通道，能得到256个数字，并进行归一化。进行局部响应归一化的动机是，对于这张13×13的图像中的每个位置来说，可能并不需要太多的高激活神经元。但是后来，很多研究者发现LRN起不到太大作用因为并不重要，而且我们现在并不用LRN来训练网络。

  正是通过这篇论文，计算机视觉群体开始重视深度学习，并确信深度学习可以应用于计算机视觉领域。此后，深度学习在计算机视觉及其它领域的影响力与日俱增。

- **VGGNet**

  ![](http://www.ai-start.com/dl2017/images/4a5b3f4c8b34eb8cdca6c36b46af142d.png)

  VGG-16网络没有那么多超参数，这是一种只需要专注于构建卷积层的简单网络。

  首先用64个3×3，步幅为1的过滤器，进行same卷积，卷积两次。然后用一个2×2，步幅为2的过滤器构建最大池化层，减少到112×112×64。
  然后使用128个过滤器，进行same卷积，卷积两次，变为112×112×128。然后进行池化变为56×56×128。
  接着再用256个相同的过滤器进行三次卷积操作，然后再池化，
  然后再卷积三次，再池化。如此进行几轮操作后，将最后得到的7×7×512的特征图进行全连接操作，得到4096个单元，然后进行进行了两次卷积激活，输出从1000个对象中识别的结果。

  VGG-16的这个数字16，就是指在这个网络中包含16个卷积层和全连接层。确实是个很大的网络，总共包含约1.38亿个参数，即便以现在的标准来看都算是非常大的网络。但VGG-16的结构并不复杂，这点非常吸引人，而且这种**网络结构很规整，都是几个卷积层后面跟着可以压缩图像大小的池化层，池化层缩小图像的高度和宽度**。同时，**卷积层的过滤器数量变化存在一定的规律，由64翻倍变成128，再到256和512**。作者可能认为512已经足够大了，所以后面的层就不再翻倍了。无论如何，每一步都进行翻倍，或者说在每一组卷积层进行过滤器翻倍操作，正是设计此种网络结构的另一个简单原则。这种相对一致的网络结构对研究者很有吸引力，而它的主要缺点是需要训练的特征数量非常巨大。

  文中揭示了，随着网络的加深，图像的高度和宽度都在以一定的规律不断缩小，每次池化后刚好缩小一半，而通道数量在不断增加，而且刚好也是在每组卷积操作后增加一倍。也就是说，图像缩小的比例和通道数增加的比例是有规律的。从这个角度来看，这篇论文很吸引人。

如果对这些论文感兴趣，建议从AlexNet的论文开始，然后就是VGG的论文，最后是LeNet的论文。虽然有些晦涩难懂，但对于了解这些网络结构很有帮助。

### 2. 残差网络（Residual Networks (ResNets)）

因为存在梯度消失和梯度爆炸问题，所以非常深的神经网络是很难训练的。跳跃连接（Skip connection），它可以从某一层网络层获取激活，然后迅速反馈给另外一层，甚至是神经网络的更深层。可以利用跳跃连接构建训练深度网络的ResNets，有时深度能够超过100层。

ResNets是由残差块（Residual block）构建的。

![](http://www.ai-start.com/dl2017/images/f0a8471f869d8062ba59598c418da7fb.png)

关于残差块，假如上图描述的是一个两层神经网络，在$L$层进行激活，得到$a^{\left\lbrack l + 1 \right\rbrack}$，再次进行激活，两层之后得到$a^{\left\lbrack l + 2 \right\rbrack}$。

计算过程是从$a^{[l]}$开始，首先进行线性激活，根据这个公式：$z^{\left\lbrack l + 1 \right\rbrack} = W^{\left\lbrack l + 1 \right\rbrack}a^{[l]} + b^{\left\lbrack l + 1 \right\rbrack}$，通过$a^{[l]}$算出$z^{\left\lbrack l + 1 \right\rbrack}$。然后通过ReLU非线性激活函数得到$a^{\left\lbrack l + 1 \right\rbrack} =g(z^{\left\lbrack l + 1 \right\rbrack})$。
接着再次进行线性激活，依据等式$z^{\left\lbrack l + 2 \right\rbrack} = W^{\left\lbrack 2 + 1 \right\rbrack}a^{\left\lbrack l + 1 \right\rbrack} + b^{\left\lbrack l + 2 \right\rbrack}$，最后再次进行ReLu非线性激活，即$a^{\left\lbrack l + 2 \right\rbrack} = g(z^{\left\lbrack l + 2   \right\rbrack})$。

换句话说，信息流从$a^{\left\lbrack l   \right\rbrack}$到$a^{\left\lbrack l + 2  \right\rbrack}$需要经过以上所有步骤，即这组网络层的主路径。

在残差网络中有一点变化，直接将$a^{[l]}$向后拷贝到神经网络的深层，在ReLU非线性激活函数前加上$a^{[l]}$，这是一条捷径。$a^{[l]}$的信息直接到达神经网络的深层，不再沿着主路径传递。

这就意味着最后的等式$a^{\left\lbrack l + 2 \right\rbrack} = g(z^{\left\lbrack l + 2 \right\rbrack})$去掉了，取而代之的是另一个ReLU非线性函数，仍然对$z^{\left\lbrack l + 2 \right\rbrack}$进行$g$函数处理，但这次要加上$a^{[l]}$，即：$\ a^{\left\lbrack l + 2 \right\rbrack} = g\left(z^{\left\lbrack l + 2 \right\rbrack} + a^{[l]}\right)$，也就是加上的这个$a^{[l]}$产生了一个残差块。除了捷径，还会听到另一个术语“跳跃连接”，就是指$a^{[l]}$跳过一层或者好几层，从而将信息传递到神经网络的更深层。

发明者他们发现使用残差块能够训练更深的神经网络，所以构建一个ResNet网络就是通过将很多这样的残差块堆积在一起，形成一个很深神经网络。

![](http://www.ai-start.com/dl2017/images/6077958a616425d76284cecb43c2f458.png)

正如上图看到的，每两层增加一个捷径，构成一个残差块，5个残差块连接在一起构成一个残差网络。

如果使用标准优化算法训练一个普通网络，比如说梯度下降法，或者其它热门的优化算法。如果没有残差，没有这些捷径或者跳跃连接，可以发现随着网络深度的加深，训练错误会先减少，然后增多。理论上网络深度越深越好，但实际上，如果没有残差网络，对于一个普通网络来说，深度越深意味着用优化算法越难训练。实际上，随着网络深度的加深，训练误差会越来越多。

但有了ResNets就不一样了，即使网络再深，训练的表现却不错，比如说训练误差减少，就算是训练深达100层的网络也不例外。让$x$的激活，或者这些中间的激活能够到达网络的更深层，这种方式确实有助于解决梯度消失和梯度爆炸问题。在训练更深网络的同时，又能保证良好的性能。也许从另外一个角度来看，随着网络越来深，网络连接会变得臃肿，但是ResNet确实在训练深度网络方面非常有效。

### 3. 残差网络为什么有用？（Why ResNets work?）

一个网络深度越深，它在训练集上训练的效率就会有所减弱，这也是有时候我们不希望加深网络的原因。而在训练ResNets网络时，并非完全如此。

![](http://www.ai-start.com/dl2017/images/2af8c71686dc45ced8d66c011714343f.png)

假设有一个大型神经网络，其输入为$X$，输出激活值$a^{[l]}$。假如想增加这个神经网络的深度，用Big NN表示，输出为$ a^{\left\lbrack l\right\rbrack}$。再给这个网络额外添加两层，依次添加两层，最后输出为$a^{\left\lbrack l + 2 \right\rbrack}$，可以把这两层看作一个ResNets块，即具有捷径连接的残差块。为了方便说明，假设在整个网络中使用ReLU激活函数，所以激活值都大于等于0，包括输入$X$的非零异常值。因为ReLU激活函数输出的数字要么是0，要么是正数。

看一下$a^{\left\lbrack l + 2\right\rbrack}$的值，即$a^{\left\lbrack l + 2\right\rbrack} = g(z^{\left\lbrack l + 2 \right\rbrack} + a^{\left\lbrack l\right\rbrack})$，添加项$a^{\left\lbrack l\right\rbrack}$是刚添加的跳跃连接的输入。展开这个表达式$a^{\left\lbrack l + 2 \right\rbrack} = g(W^{\left\lbrack l + 2 \right\rbrack}a^{\left\lbrack l + 1 \right\rbrack} + b^{\left\lbrack l + 2 \right\rbrack} + a^{\left\lbrack l\right\rbrack})$，其中$z^{\left\lbrack l + 2 \right\rbrack} = W^{\left\lbrack l + 2 \right\rbrack}a^{\left\lbrack l + 1 \right\rbrack} + b^{\left\lbrack l + 2\right\rbrack}$。

注意一点，如果使用L2正则化或权重衰减，它会压缩$W^{\left\lbrack l + 2\right\rbrack}$的值，如果对$b$应用权重衰减也可达到同样的效果。这里的$W$是关键项，如果$W^{\left\lbrack l + 2 \right\rbrack} = 0$，为方便起见，假设$b^{\left\lbrack l + 2 \right\rbrack} = 0$，最后$ a^{\left\lbrack l + 2 \right\rbrack} = \ g\left( a^{[l]} \right) $ 。因为假定使用ReLU激活函数，并且所有激活值都是非负的，$g\left(a^{[l]} \right)$是应用于非负数的ReLU函数，所以最后$a^{[l+2]} =a^{[l]}$。

结果表明，残差块学习这个恒等式函数并不难，跳跃连接使我们很容易得出$ a^{\left\lbrack l + 2 \right\rbrack} = a^{\left\lbrack l\right\rbrack}$。这意味着，即使给神经网络增加了这两层，它的效率也并不逊色于简单的神经网络，因为学习恒等函数对它来说很简单。尽管它多了两层，也只把$a^{[l]}$的值赋值给$a^{\left\lbrack l + 2 \right\rbrack}$。所以给大型神经网络增加两层，不论是把残差块添加到神经网络的中间还是末端位置，都不会影响网络的表现。

**残差网络起作用的主要原因就是这些残差块学习恒等函数非常容易，网络性能不会受到影响**，很多时候甚至可以提高效率，因此创建类似残差网络可以提升网络性能。

对于普通网络，层数越多，如果初始化权重$w$在0附近，导致梯度消失，如果$w$大于1，会产生梯度爆炸：
$$
\begin{eqnarray*}
\frac{\partial J(w,b)}{\partial w^1} &=& \frac{\partial J(w,b)}{\partial a^4}\frac{\partial a^4}{\partial z^4}\frac{\partial z^4}{\partial a^3}\frac{\partial a^3}{\partial z^3}\frac{\partial z^3}{\partial a^2}\frac{\partial a^2}{\partial z^2}\frac{\partial z^2}{\partial a^1}\frac{\partial a^1}{\partial w^1} \\
&=&\frac{\partial J(w,b)}{\partial a^4}\sigma'(z^{4})w^{4}\sigma'(z^{3})w^{3}\sigma'(z^{2})w^{2}\sigma'(z^{1})x^{1} 
\end{eqnarray*}
$$
普通网络函数假设为$a=F(x^l)$ ，使用残差网络后变为$a=F(x^l) + x^l$ ，对输入$x^l$求偏导：
$$
\begin{eqnarray*}
\frac{\partial a}{\partial x^l} &=& \frac{\partial F(x^l)}{\partial x^l} + 1
\end{eqnarray*}
$$
因为使用了跳跃连接，使得梯度下降通过“1”流回任意浅层$l$ ，避免了经过多层造成的梯度爆炸，所以ResNets网络深度可以很深。

![](http://www.ai-start.com/dl2017/images/cefcaece17927e14eb488cb52d99aaef.png)

除此之外，关于残差网络，另一个值得探讨的细节是，假设$ z^{\left\lbrack l + 2\right\rbrack}$与$a^{[l]}$具有相同维度，所以ResNets使用了许多same卷积，所以$a^{\left\lbrack l\right\rbrack}$的维度等于输出层的维度。之所以能实现跳跃连接是因为same卷积保留了维度，所以很容易得出这个捷径连接，并输出这两个相同维度的向量。

如果输入和输出有不同维度，比如$ a^{\left\lbrack l \right\rbrack}$输入的维度是128，$ a^{\left\lbrack l + 2\right\rbrack}$的维度是256。再增加一个矩阵，标记为$W_{s}$，$W_{s}$是一个256×128维度的矩阵，所以$W_{s}a^{\left\lbrack l\right\rbrack}$的维度是256，这个新增项是256维度的向量。不需要对$W_{s}$做任何操作，它是网络通过学习得到的矩阵或参数，它是一个固定矩阵，padding值为0，用0填充$a^{[l]}$，其维度为256。

![](http://www.ai-start.com/dl2017/images/70062fa97916ab79c7ad37282ba1a5f4.png)

用ResNets进行图片识别，就是给普通网络添加跳跃连接，就实现了ResNets。这个网络有很多层3×3卷积，而且它们大多都是same卷积，这就是添加等维特征向量的原因，这也解释了添加项$ z^{\left\lbrack l + 2 \right\rbrack} + a^{\left\lbrack l\right\rbrack}$（维度相同所以能够相加）。

ResNets类似于其它很多网络，也会有很多卷积层，其中偶尔会有池化层或类池化层的层。不论这些层是什么类型，只需要调整矩阵$W_{s}$的维度。普通网络和ResNets网络常用的结构是：卷积层-卷积层-卷积层-池化层-卷积层-卷积层-卷积层-池化层……依此重复。直到最后，有一个通过ResNets进行预测的全连接层。

### 4. 网络中的网络以及 1×1 卷积（Network in Network and 1×1 convolutions）

在架构内容设计方面，其中一个比较有帮助的想法是使用1×1卷积。也许好奇，1×1的卷积能做什么呢？不就是乘以数字么？听上去挺好笑的，结果并非如此。

如下图过滤器为1×1，输入一张6×6的图片，然后对它做卷积，结果相当于把这个图片乘以数字2，所以单元格分别是2、4、6等等。用1×1x1的过滤器进行卷积，似乎用处不大，只是对输入矩阵乘以某个数字。但这仅仅是对于6×6×1的一个通道图片来说，1×1卷积效果不佳。

![](http://www.ai-start.com/dl2017/images/84c62c1d9bdf0a14f20cbeb02e1cec1a.png)

如果是一张6×6×32的图片，那么使用1×1过滤器进行卷积效果更好。因为1×1卷积所实现的功能是遍历36个单元格，计算左图中32个数字和过滤器中32个数字的元素积之和，然后应用ReLU非线性函数。

![](http://www.ai-start.com/dl2017/images/70eba35d0705dc681c40f09a0926061a.png)

以其中一个单元为例，在输入层上的做切片，1×1×32过滤器中的32个数字可以理解为：一个神经元的输入是32个数字（即通道中的32个数字），乘以32个权重（将过滤器中的32个数理解为权重），然后应用ReLU非线性函数输出相应的结果。就好像有多个输入单元，其输入内容为一个切片上所有数字。

一般来说，如果过滤器不止一个，而是多个，输出结果是6×6x过滤器数量。所以1×1卷积可以从根本上理解为对这32个不同的位置都应用一个全连接层，全连接层的作用是输入32个数字（过滤器数量标记为$n_{C}^{\left\lbrack l + 1\right\rbrack}$，在这36个单元上重复此过程）,输出结果是6×6×\#filters（过滤器数量），以便在输入层上实施一个非平凡（**non-trivial**）计算。

这种方法通常称为1×1卷积，有时也被称为Network in Network，很多神经网络架构都受到它的影响，包括后面的Inception网络。

1×1卷积层就是通过给神经网络添加了一个非线性函数，从而**减少或保持输入层中的通道数量不变**，当然也可以增加通道数量。后面可以发现这对构建Inception网络很有帮助。

### 5. 谷歌 Inception 网络简介（Inception network motivation）

构建卷积层时，过滤器的大小究竟是用1×1，3×3还是5×5，或者要不要添加池化层。而Inception网络的作用就是代替我们决定，虽然网络架构因此变得更加复杂，但网络表现却非常好。

例如，输入层是28×28×192维度的，Inception网络的作用就是代替人工来确定卷积层中的过滤器类型，或者确定是否需要创建卷积层或池化层。

![](http://www.ai-start.com/dl2017/images/46cf8f4fe80f47de754d0e0f13945941.png)

如果使用1×1卷积，假设输出为28×28×64，并且这里只有一个层。

如果使用3×3的过滤器，那么输出是28×28×128。然后把第二个值堆积到第一个值上，为了匹配维度，应用same卷积。

或许希望提升网络的表现，用5×5过滤器或许会更好，输出变成28×28×32，再次使用same卷积，保持维度不变。

或许不想要卷积层，那就用池化操作，得到一些不同的输出结果，把它也堆积起来，这里的池化输出是28×28×32。为了匹配所有维度，需要对最大池化使用padding，它是一种特殊的池化形式，因为如果输入的高度和宽度为28×28，则输出的相应维度也是28×28，步幅为1。

有了这样的Inception模块，就可以输入某个量，因为它累加了所有数字，这里的最终输出为32+32+128+64=256。Inception模块的输入为28×28×192，输出为28×28×256。这就是Inception网络的核心内容，**==基本思想是Inception网络不需要人为决定使用哪个过滤器或者是否需要池化，而是由网络自行确定这些参数，可以给网络添加这些参数的所有可能值，然后把这些输出连接起来，让网络自己学习它需要什么样的参数，采用哪些过滤器组合==**。

不难发现，现在的Inception层有一个问题，就是计算成本。

![](http://www.ai-start.com/dl2017/images/27894eae037f4fd859d33ebdda1cac9a.png)

我们把重点集中在其中的5×5的过滤器，一个28×28×192的输入块，执行一个5×5的same卷积，有32个过滤器，输出为28×28×32。

现在计算这个28×28×32输出的计算成本，因为输出有32个通道，每个过滤器大小为5×5×192，输出大小为28×28×32，所以要计算28×28×32个数字。对于输出中的每个数字来说，都需要执行5×5×192次乘法运算，所以乘法运算的总次数=每个输出值所需要执行的乘法运算次数（5×5×192）x 输出值个数（28×28×32），结果等于1.2亿(120422400)。即使在现在，用计算机执行1.2亿次乘法运算，成本也是相当高的。

![](http://www.ai-start.com/dl2017/images/1fec66d984a3c8c47ff459775d411e71.png)

还有另外一种架构，其输入为28×28×192，输出为28×28×32。

对于输入层，使用1×1卷积把输入值从192个通道减少到16个通道。然后对这个较小层运行5×5卷积，得到最终输出。

我们要做的就是把左边这个大的输入层压缩成这个较小的的中间层，它只有16个通道，而不是192个，然后再运行5×5卷积。**即先缩小网络表示，然后再扩大它**。

接下来看这个计算成本，应用1×1卷积，过滤器个数为16，每个过滤器大小为1×1×192，输出28×28×192中每个元素都做192次乘法，相乘结果约等于240万。

第二个卷积层的输出，28×28×32，对每个输出值应用一个5×5×16维度的过滤器，计算结果为1000万。

所以需要乘法运算的总次数是这两层的计算成本之和，也就是1204万，计算成本从1.2亿下降到了原来的十分之一，即1204万。所需要的加法运算与乘法运算的次数近似相等，所以只统计了乘法运算的次数。

总结一下，**如果在构建神经网络层的时候，不想决定池化层是使用1×1，3×3还是5×5的过滤器，那么Inception模块就是最好的选择。可以应用各种类型的过滤器，只需要把输出连接起来**。通过使用1×1卷积来构建瓶颈层，来缩小网络，从而大大降低计算成本。

事实证明，只要合理构建瓶颈层，既可以显著缩小表示层规模，又不会降低网络性能，从而节省了计算。

### 6. Inception 网络（Inception network）

学习如何将这些模块组合起来，构筑自己的Inception网络。

![](http://www.ai-start.com/dl2017/images/16a042a0f2d3866909533d409ff2ce3b.png)

Inception模块会将之前层的激活或者输出作为它的输入，作为前提，这是一个28×28×192的输入。

上一章的例子是，先通过一个1×1的层，再通过一个5×5的层，1×1的层可能有16个通道，而5×5的层输出为28×28×32，共32个通道。

为了在3×3的卷积层中节省运算量，也可以做相同的操作，这样的话3×3的层将会输出28×28×128。

或许还想将其直接通过一个1×1的卷积层，这时就不必在后面再跟一个1×1的层了，这样的话过程就只有一步，假设这个层的输出是28×28×64。

最后是池化层，为了能在最后将这些输出都连接起来，使用same类型的padding来池化，使得输出的高和宽依然是28×28，这样才能将它与其他输出连接起来。但注意，如果进行了最大池化，其输出将会是28×28×192，所以看起来它会有很多通道。实际要做的就是再加上一个1×1的卷积层，将通道的数量缩小，缩小到28×28×32。这样就避免了最后输出时，池化层占据所有的通道。

最后，将这些方块全都连接起来。在这过程中，把得到的各个层的通道都加起来，最后得到一个28×28×256的输出。通道连接实际就是最开始讲的，把所有方块连接在一起的操作。这就是一个Inception模块，而Inception网络最后所做的就是将这些模块都组合到一起。

![](http://www.ai-start.com/dl2017/images/1f2a024a28f664aa704be53cea7ca6f8.png)

这是一张关于Inception网络的图片，可以发现图中有许多重复的模块，可能整张图看上去很复杂，但如果只截取其中一个环节，就会发现这是Inception模块。

所以Inception网络只是很多这些模块在不同的位置重复组成的网络，所以如果理解了之前的Inception模块，也就理解Inception网络。

![](http://www.ai-start.com/dl2017/images/5315d2dbcc3053b0146cabd79304ef1d.png)

如果读过论文原文，就会发现，这里其实还有一些分支（图中绿色框部分）。在网络的最后几层，通常称为全连接层，在它之后是一个softmax层（编号1）来做出预测，这些分支所做的就是通过隐藏层（编号2、3、4）来做出预测，这些其实都是一个softmax输出。

**隐藏层部分的全连接层应该把它看做Inception网络的一个细节，它确保了即便是隐藏单元和中间层也参与了特征计算，它们也能预测图片的分类。它在Inception网络中，起到一种调整的效果，并且能防止网络发生过拟合**。

### 7. 迁移学习（Transfer Learning）

如果看到一篇研究论文想应用它的成果，应该考虑做一件事，就是在网络上寻找一个开源的实现。如果能得到作者的实现，通常要比从头开始实现要快得多，虽然从零开始实现肯定可以是一个很好的锻炼。

如果在开发一个计算机视觉应用，一个常见的工作流程是，先选择一个喜欢的架构，接着寻找一个开源实现，从GitHub下载下来，以此基础开始构建。这样做的优点在于，这些网络通常都需要很长的时间来训练。

计算机视觉的研究社区非常喜欢把许多数据集上传到网上，比如ImageNet，或者MS COCO，或者Pascal类型的数据集，有大量的计算机视觉研究者已经用这些数据集训练过他们的算法了。有时候这些训练过程需要花费好几周，并且需要很多的GPU，其它人已经做过了，并且经历了非常痛苦的寻最优过程。

这就意味着可以直接下载花费了别人好几周甚至几个月而做出来的开源的权重参数，把它当作一个很好的初始化用在自己的神经网络上。用迁移学习把公共的数据集的知识迁移到自己的问题上。

![](http://www.ai-start.com/dl2017/images/8f0e69f085991cfc74726983418f6569.png)

如果自己的训练集很小，建议从网上下载一些神经网络开源的实现，不仅把代码下载下来，也把权重下载下来。有许多训练好的网络，都可以下载。举个例子，ImageNet数据集，有1000个不同的类别，因此这个网络会有一个Softmax单元，它可以输出1000个可能类别之一。

可以去掉原本的Softmax层，创建自己的Softmax单元，用来输出自己需要的类别。就网络而言，建议把所有的层看作是冻结的，只需要训练和自己Softmax层有关的参数。

![](http://www.ai-start.com/dl2017/images/ac520bf9e9facfc026db46b187b513bd.png)

另一个技巧，也许对一些情况有用，由于前面的层都冻结了，相当于一个固定的函数，不需要改变。取输入图像$X$，然后把它映射到softmax的前一层的激活函数。这个能加速训练的技巧就是，如果先计算softmax的前一层，计算特征或者激活值，然后把它们存到硬盘里。需要做的就是用这个固定的函数，在神经网络的前半部分（softmax层之前的所有层视为一个固定映射），取任意输入图像$X$，然后计算它的某个特征向量，这样训练的就是一个很浅的softmax模型，用这个特征向量来做预测。

**对计算有用的一步就是对训练集中所有样本的这一层的激活值进行预计算，然后存储到硬盘里，然后在此之上训练softmax分类器。所以，存储到硬盘或者说预计算方法的优点就是，不需要每次遍历训练集再重新计算这个激活值**。

![](http://www.ai-start.com/dl2017/images/e7079af956d884d6184c4bde62271175.png)

要有一个更大的训练集，根据经验，这种情况下，应该冻结更少的层，然后训练后面的层。如果输出层的类别不同，那么需要构建自己的输出单元。有很多方式可以实现，比如可以取后面几层的权重，用作初始化，然后从这里开始梯度下降。

或者可以直接去掉后面几层，换成自己的隐藏单元和自己的softmax输出层，这些方法值得一试。但是有一个规律，如果有越来越多的数据，需要冻结的层数越少，能够训练的层数也就越多。

**如果有一个更大的数据集，也许有足够多的数据，那么不要单单训练一个softmax单元，而是考虑训练中等大小的网络，包含最终要用的网络的后面几层。**

最后，**如果有大量数据，应该做的就是用开源的网络和它的权重，把所有的权重当作初始化，然后训练整个网络**。再次注意，如果这是一个1000节点的softmax，而自己只有三个输出，需要自己的softmax输出层来输出想要的标签。

事实上，对于很多计算机视觉的应用，如果下载其他人的开源的权重，并用作自己问题的初始化，可以做的更好。在所有深度学习不同的应用中，计算机视觉是一个经常用到迁移学习的领域，除非有非常非常大的数据集，可以从头开始训练所有的东西。总之，迁移学习是非常值得考虑的，除非自己有一个极其大的数据集和非常大的计算量预算来从头训练网络。

### 8. 数据扩充（Data augmentation）

大部分的计算机视觉任务使用很多的数据，所以数据扩充是经常使用的一种技巧来提高计算机视觉系统的表现。当下在计算机视觉方面，计算机视觉的主要问题是没有办法得到充足的数据。对大多数机器学习应用，这不是问题，但是对计算机视觉，数据就远远不够。![](http://www.ai-start.com/dl2017/images/f92337ae2e50a0896d42d45cc7951e43.png)

最简单的数据扩充方法就是垂直镜像对称，假如，训练集中有这张图片，然后将其翻转得到右边的图像。

![](http://www.ai-start.com/dl2017/images/709aa552b6a5f4715620047bacf64753.png)

另一个经常使用的技巧是随机裁剪，给定一个数据集，然后开始随机裁剪，可以得到不同的图片放在数据集中，训练集中有不同的裁剪。随机裁剪并不是一个完美的数据扩充的方法，例如随机裁剪红色方框标记部分。但在实践中，这个方法还是很实用的，随机裁剪构成了很大一部分的真实图片。

镜像对称和随机裁剪是经常被使用的。当然，理论上，也可以使用旋转，剪切（此处并非裁剪的含义，图像仅水平或垂直坐标发生变化）图像，可以对图像进行这样的扭曲变形，引入很多形式的局部弯曲等等。当然使用这些方法并没有坏处，尽管在实践中，因为太复杂了所以使用的很少。

![](http://www.ai-start.com/dl2017/images/a5bcde6f0d2c2326be700c0ca441c934.png)

第二种经常使用的方法是彩色转换，有这样一张图片，然后给R、G和B三个通道上加上不同的失真值。

其中一种影响颜色失真的算法是PCA，即主成分分析。PCA颜色增强的大概含义是，如果图片呈现紫色，即主要含有红色和蓝色，绿色很少，然后PCA颜色增强算法就会对红色和蓝色增减很多，绿色变化相对少一点，所以使总体的颜色保持一致。网上也能找到PCA颜色增强的开源实现方法，然后直接使用它。

![](http://www.ai-start.com/dl2017/images/5ee17d350497cb8cf52881f14cb0d9e8.png)

可能有存储好的数据，存在硬盘上，使用圆桶这个符号来表示。如果有一个小的训练数据，可以做任何事情，这些数据集就够了。

如果有特别大的训练数据，可以会使用**CPU**线程，然后它不停的从硬盘中读取数据，可以用CPU线程来实现这些失真变形，可以是随机裁剪、颜色变化，或者是镜像。

与此同时，CPU线程持续加载数据，然后实现任意失真变形，从而构成mini-batch，这些数据持续的传输给其他线程或者其他的进程，然后开始训练，可以在CPU或者GPU上实现训一个大型网络的训练。

**常用的实现数据扩充的方法是使用一个线程或者是多线程，用来加载数据，实现变形失真，然后传给其他的线程或者其他进程，来训练网络，可以并行实现**。

在数据扩充过程中也有一些超参数，比如说颜色变化了多少，以及随机裁剪的时候使用的参数。与计算机视觉其他部分类似，一个好的开始可能是使用别人的开源实现，了解他们如何实现数据扩充。当然如果想获得更多的不变特性，而其他人的开源实现并没有实现这个，自己可以去调整这些参数。希望可以使用数据扩充使计算机视觉应用效果更好。

### 9. 计算机视觉现状（The state of computer vision）

深度学习已经成功地应用于计算机视觉、自然语言处理、语音识别、在线广告、物流还有其他许多问题。在计算机视觉的现状下，深度学习应用于计算机视觉应用有一些独特之处。

![](http://www.ai-start.com/dl2017/images/e6701cf4129576648941bfd593a13c77.png)

可以认为大部分机器学习问题是介于少量数据和大量数据范围之间的。举个例子，今天我们有相当数量的语音识别数据，虽然现在图像识别或图像分类方面有相当大的数据集，因为图像识别是一个复杂的问题，通过分析像素并识别出它是什么，感觉即使在线数据集非常大，如超过一百万张图片，我们仍然希望我们能有更多的数据。

还有一些问题，比如物体检测，我们拥有的数据更少。图像识别其实是如何看图片的问题，而对象检测则是看一幅图，画一个框，告诉图片里的物体，比如汽车等等。因为获取边框的成本比标记对象的成本更高，所以我们进行对象检测的数据往往比图像识别数据要少。

所以，观察一下机器学习数据范围图谱，可以发现**当有很多数据时，人们倾向于使用更简单的算法和更少的手工工程，因为我们不需要为这个问题精心设计特征**。当有大量的数据时，只要有一个大型的神经网络，甚至一个更简单的架构，可以是一个神经网络，就可以去学习它想学习的东西。相反当没有那么多的数据时，会看到人们从事更多的是手工工程。

![](http://www.ai-start.com/dl2017/images/c87f6cc9ec9c45ad57a049b6baf0b86d.png)

学习算法有两种知识来源，一个来源是被标记的数据，就像$(x，y)$应用在监督学习。第二个知识来源是手工工程，有很多方法去建立一个手工工程系统，它可以是源于精心设计的特征，手工精心设计的网络体系结构或者是系统的其他组件。所以没有太多标签数据时，需要更多地考虑手工工程。

**幸运的是，当有少量的数据时，有一件事可以很有帮助，那就是迁移学习。**

在想好了想要的神经网络之后，可以独立训练几个神经网络，并平均它们的输出。比如说随机初始化三个、五个或者七个神经网络，然后训练所有这些网络，然后平均它们的输出。

另外对它们的输出$\hat y$进行平均计算是很重要的，不要平均它们的权重，这是行不通的。看看7个神经网络，它们有7个不同的预测，然后平均它们，这可能会让我们在基准上提高1%，2%或者更好。但因为集成意味着要对每张图片进行测试，可能需要在从3到15个不同的网络中运行一个图像，因为这3到15个网络可能会让运行时间变慢，所以技巧之一的集成是人们在基准测试中表现出色和赢得比赛的利器，但这几乎不用于生产服务于客户的，除非有一个巨大的计算预算而且不介意在每个用户图像数据上花费大量的计算。

![](http://www.ai-start.com/dl2017/images/6027faa79b81f9940281ea36ca901504.png)

Multi-crop是一种将数据扩充应用到测试图像中的一种形式。

举个例子，把猫的图片复制四遍，包括它的两个镜像版本。有一种叫作10-crop的技术（crop理解为裁剪的意思）。它基本上说的是，假设取中心区域裁剪，然后通过分类器去运行它，然后取左上角区域，运行分类器，右上角用绿色表示，左下方用黄色表示，右下方用橙色表示，通过分类器来运行它。然后对镜像图像做同样的事情，取中心的crop，然后取四个角落的crop。

如果把这些加起来，就会有10种不同的图像的crop，因此命名为10-crop。要做的就是，通过分类器来运行这十张图片，然后对结果进行平均。如果有足够的计算预算，可以这么做，这可能会在生产系统中获得更好的性能。这是另一种技术，它在基准测试上的应用，要比实际生产系统中好得多。

集成的一个大问题是需要保留所有这些不同的神经网络，这就占用了更多的计算机内存。对于multi-crop，可以只保留一个网络，所以它不会占用太多的内存，但它仍然会让运行时间变慢。这些小技巧，研究论文也可以参考这些，Andrew NG并不倾向于在构建生产系统时使用这些方法，尽管它们在基准测试和竞赛上做得很好。

