[TOC]

## 深层神经网络

![深层神经网络](http://www.ai-start.com/dl2017/images/9927bcb34e8e5bfe872937fccd693081.png)

上图是一个四层的神经网络，有三个隐藏层。我们用L表示层数，上图：$L=4$，输入层的索引为“0”，第一个隐藏层${n}^{[1]}=5$,表示有5个隐藏神经元，同理${n}^{[2]}=5$，${n}^{[3]}=3$，${{n}^{[4]}}$=${{n}^{[L]}}=1$（输出单元为1）。而输入层，${n}^{[0]}={n}_{x}=3$。

在不同层所拥有的神经元的数目，对于每层*l*都用${a}^{[l]}$来记作*l*层激活后结果，我们会在后面看到在正向传播时，最终能你会计算出${{a}^{[l]}}$。

通过用激活函数 $g$ 计算${z}^{[l]}$，激活函数也被索引为层数$l$，然后我们用${w}^{[l]}$来记作在*l*层计算${z}^{[l]}$值的权重。类似的，${{z}^{[l]}}$里的方程${b}^{[l]}$也一样。

最后总结下符号约定：
输入的特征记作$x$，但是$x$同样也是0层的激活函数，所以$x={a}^{[0]}$。最后一层的激活函数，所以${a}^{[L]}$是等于这个神经网络所预测的输出结果。

### 1. 前向传播和反向传播（Forward and backward propagation）

实现了这两个函数（正向和反向），神经网络的计算过程会是这样的：

![深层神经网络正反向传播](http://www.ai-start.com/dl2017/images/be2f6c7a8ff3c58e952208d5d59b19ce.png)

- 对于正向传播：输入${a}^{[l-1]}$，输出是${a}^{[l]}$，缓存为${z}^{[l]}$；==从实现的角度来说我们可以缓存下${w}^{[l]}$和${b}^{[l]}$，这样更容易在不同的环节中调用函数==。  
  后面几层以此类推，直到最后算出了$a^{[L]}$，第$L$层的最终输出值$\hat y$。在这些过程里我们缓存了所有的$z$值，这就是正向传播的步骤。

$$
\begin{eqnarray*}
  {z}^{[l]} &=& {W}^{[l]}\cdot {A}^{[l-1]}+{b}^{[l]} \tag{1}\\
  {A}^{[l]} &=& {g}^{[l]}({Z}^{[l]}) \tag{2}
  \end{eqnarray*}
$$


- 对反向传播的步骤而言，需要算一系列的反向迭代，就是这样反向计算梯度。
  首先需要$da^{[L]}$的值，然后经由方块会给出${da}^{[L-1]}$的值，以此类推，直到得到${da}^{[2]}$和${da}^{[1]}$，你还可以计算多一个输出值，就是${da}^{[0]}$，但这其实是你的输入特征的导数，并不重要，起码对于训练监督学习的权重不算重要，你可以止步于此。反向传播步骤中也会输出$dW^{[l]}$和$db^{[l]}$。
  $$
  \begin{eqnarray*}
  d{{Z}^{[l]}}&=&d{{A}^{[l]}}*{{g}^{\left[ l \right]}}'\left({{Z}^{[l]}} \right)~~ \tag{1}\\
  d{{W}^{[l]}}&=&\frac{1}{m}\text{}d{{Z}^{[l]}}\cdot {{A}^{\left[ l-1 \right]T}} \tag{2}\\
  d{{b}^{[l]}}&=&\frac{1}{m}\text{ }np.sum(d{{z}^{[l]}},axis=1,keepdims=True) \tag{3}\\
  d{{A}^{[l-1]}}&=&{{W}^{\left[ l \right]T}}.d{{Z}^{[l]}}\tag{4}
    \end{eqnarray*}
  $$



神经网络的一步训练包含了，从$a^{[0]}$开始，也就是 $x$ 然后经过一系列正向传播计算得到$\hat y$，之后再用输出值计算第二行最后方块，再实现反向传播。现在就有所有的导数项了，$W$也会在每一层被更新为$W=W-αdW$，$b$也一样，$b=b-αdb$，反向传播就都计算完毕，就有了所有的导数值，那么这是神经网络一个梯度下降循环。

### 2. 核对矩阵的维数（Getting your matrix dimensions right）

当实现深度神经网络的时候，其中一个常用的检查代码是否有错的方法就是拿出一张纸过一遍算法中矩阵的维数。

${Z}^{[l]}$可以看成由每一个单独的${Z}^{[l]}$叠加而得到，${Z}^{[l]}=({{z}^{[l][1]}}，{{z}^{[l][2]}}，{{z}^{[l][3]}}，…，{{z}^{[l][m]}})$，$m$为训练集大小
所以${Z}^{[l]}$的维度是$({{n}^{[l]}},m)$。${A}^{[l]}$：$({n}^{[l]},m)$，${A}^{[0]} = X =({n}^{[0]},m)$

$W^{[l]}$的维度是（下一层的维数 X 前一层的维数），即${{W}^{[l]}}$: (${{n}^{[l]}}$,${{n}^{[l-1]}}$)；$b^{[l]}$的维度是（下一层的维数，1），即:${{b}^{[l]}}$ : (${{n}^{[l]}},1)$；${{dW}^{[l]}}$和${{W}^{[l]}}$维度相同，${{db}^{[l]}}$和${{b}^{[l]}}$维度相同。

在做深度神经网络的反向传播时，一定要确认所有的矩阵维数是前后一致的，可以大大提高代码通过率。

### 3. 为什么使用深层表示？（Why deep representations?）

我们都知道深度神经网络能解决好多问题，其实并不需要很大的神经网络，但是得有深度，得有比较多的隐藏层，这是为什么呢？

深度神经网络的这许多隐藏层中，较早的前几层能学习一些低层次的简单特征，等到后几层，就能把简单的特征结合起来，去探测更加复杂的东西。比如你录在音频里的单词、词组或是句子，然后就能运行语音识别了。同时我们所计算的之前的几层，也就是相对简单的输入函数，比如图像单元的边缘什么的。到网络中的深层时，你实际上就能做很多复杂的事，比如探测面部或是探测单词、短语或是句子。

深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。

关于神经网络为何有效的理论，来源于电路理论，它和你能够用电路元件计算哪些函数有着分不开的联系。根据不同的基本逻辑门，譬如与门、或门、非门。在非正式的情况下，这些函数都可以用相对较小，但很深的神经网络来计算，小在这里的意思是隐藏单元的数量相对比较小，但是如果你用浅一些的神经网络计算同样的函数，也就是说在我们不能用很多隐藏层时，会需要成指数增长的单元数量才能达到同样的计算结果。

![逻辑门神经网络计算](http://www.ai-start.com/dl2017/images/b409b7c0d05217ea37f0036691c891ca.png)

### 4. 参数VS超参数（Parameters vs Hyperparameters）

**什么是超参数？**
比如算法中的**learning rate** （学习率）、**iterations**(梯度下降法循环的数量)、**hidden layers**（隐藏层数目）、**hidden unity**（隐藏层单元数目）、**choice of activation function**（激活函数的选择）都需要来设置，这些数字实际上控制了最后的参数和的值，所以它们被称作超参数。

实际上深度学习有很多不同的超参数，之后也会介绍一些其他的超参数，如**momentum**、**mini batch size**、**regularization parameters**等等。

**如何寻找超参数的最优值？**
走**Idea—Code—Experiment—Idea**这个循环，尝试各种不同的参数，实现模型并观察是否成功，然后再迭代。

今天的深度学习应用领域，还是很经验性的过程，通常你有个想法，比如你可能大致知道一个最好的学习率值，可能说最好，我会想先试试看，然后你可以实际试一下，训练一下看看效果如何。然后基于尝试的结果你会发现，你觉得学习率设定再提高到0.05会比较好。如果你不确定什么值是最好的，你大可以先试试一个学习率，再看看损失函数J的值有没有下降。然后你可以试一试大一些的值，然后发现损失函数的值增加并发散了。然后可能试试其他数，看结果是否下降的很快或者收敛到在更高的位置。你可能尝试不同的并观察损失函数这么变了，试试一组值，然后可能损失函数变成这样，这个值会加快学习过程，并且收敛在更低的损失函数值上（箭头标识），我就用这个值了。

应用深度学习领域，一个很大程度基于经验的过程，凭经验的过程通俗来说，就是试直到你找到合适的数值。