[TOC]

循环序列模型（Recurrent Neural Networks）
---

### 1. 为什么选择序列模型？（Why Sequence Models?）

循环神经网络（RNN）之类的模型在语音识别、自然语言处理和其他领域中引起变革。

![](http://www.ai-start.com/dl2017/images/ae2970d80a119cd341ef31c684bfac49.png)

这些问题都可以被称作使用标签数据 $(X,Y)$作为训练集的监督学习，但从这一系列例子中可以看出序列问题有很多不同类型。有些问题里，输入数据 $X$和输出数据$Y$都是序列，但就算在这种情况下，$X$和$Y$有时也会不一样长。或者像上图编号1所示和上图编号2的$X$和$Y$有相同的数据长度。在另一些问题里，只有 $X$或者只有$Y$是序列。

### 2. 数学符号（Notation）

比如想要建立一个序列模型，它的输入语句是这样的：“Harry Potter and Herminoe Granger invented a new spell.”。假如想要建立一个能够自动识别句中人名位置的序列模型，那么这就是一个命名实体识别问题，这常用于搜索引擎，比如说索引过去24小时内所有新闻报道提及的人名，用这种方式就能够恰当地进行索引。命名实体识别系统可以用来查找不同类型的文本中的人名、公司名、时间、地点、国家名和货币名等等。

这个输入数据是9个单词组成的序列，所以最终会有9个特征集和来表示这9个单词，并按序列中的位置进行索引，$x^{<1>}$、$x^{<2>}$、$x^{<3>}$等等一直到$x^{<9>}$来索引不同的位置，==用$x^{<t>}$来索引这个序列的中间位置==。$t$意味着它们是时序序列，不论是否是时序序列，都将用$t$来索引序列中的位置。

输出数据也是一样，用$y^{<1>}$、$y^{<2>}$、$y^{<3>}$一直到$y^{<9>}$来表示输出数据。同时==用$T_{x}$来表示输入序列的长度==，==$T_{y}$来表示输出序列的长度==。在这个例子里$T_{x} =T_{y}$，$T_{x}$和$T_{y}$可以有不同的值。

之前用$x^{(i)}$来表示第$i$个训练样本，所以为了指代==训练样本中第$i$个样本的第$t$个元素用$x^{\left(i \right) <t>}$来表示==，同理==$T_{x}^{(i)}$就代表第$i$个训练样本的输入序列长度==。同样$y^{\left( i \right) < t>}$代表第$i$个训练样本中第$t$个元素，$T_{y}^{(i)}$就是第$i$个训练样本的输出序列的长度。

在自然语言NLP中，需要我们事先决定的一件事是怎样表示一个序列里单独的单词，如怎样表示像Harry这样的单词，$x^{<1>}$实际应该是什么？

![](http://www.ai-start.com/dl2017/images/a45c8066f935c6f29d00a95e36cb6662.png)

想要表示一个句子里的单词，第一件事是做一张词表，有时也称为词典，意思是列一列你的表示方法中用到的单词。这个词表中的第一个词是a，第二个单词是Aaron，4075这个位置是Harry，and so on。在这个例子中用了10,000个单词大小的词典，对于商业应用来说，30,000到50,000词大小的词典比较常见，有些大型互联网公司会用百万词，甚至更大的词典。接下来可以用one-hot表示法来表示词典里的每个单词：

![](http://www.ai-start.com/dl2017/images/8deca8a84f06466155d2d8d53d26e05d.png)

比如这里$x^{<1>}$表示Harry这个单词，它第4075行是1，其余值都是0的向量，刚好对应其在这个词典里的位置。目的是用这样的表示方式表示$X$，用序列模型在$X$和目标输出$Y$之间学习建立一个映射。

把它当作监督学习的问题，确信会带有$(x，y)$标签的数据。如果遇到了一个不在词表中的单词，创建一个新的标记，也就是一个叫做Unknow Word的伪造单词，用\<UNK\>作为标记，来表示不在词表中的单词，之后会讨论更多有关这个的内容。

### 3. 循环神经网络模型（Recurrent Neural Network Model）

建立一个神经网络来学习$X$到$Y$的映射，可以尝试的方法之一是使用标准神经网络，在之前的例子中有9个输入单词，是9个one-hot向量，然后将它们输入到一个标准神经网络中，经过一些隐藏层，最终会输出9个值为0或1的项，它表明每个输入单词是否是人名的一部分。

![](http://www.ai-start.com/dl2017/images/1653ec3b8eb718ca817d3423ae3ca643.png)

但这个方法并不好，主要有两个问题：

1. 不是所有的例子都有着同样输入长度$T_{x}$或是同样输出长度的$T_{y}$。即使能够使用零填充（zero pad）使每个输入语句都达到最大长度，但仍然看起来不是一个好的表达方式。

   $x^{<1>}$……$x^{<t>}$……$x^{< T_{x}>}$）都是10,000维的one-hot向量，因此这会是十分庞大的输入层。如果总的输入大小是最大单词数乘以10,000，那么第一层的权重矩阵就会有着巨量的参数。

2. 一个这样单纯的神经网络结构，它并不共享从文本的不同位置上学到的特征。

循环网络就没有上述问题，那么什么是循环神经网络？

**循环神经网络定义：**

![](http://www.ai-start.com/dl2017/images/cb041c33b65e17600842ebf87174c4f2.png)

如果从左到右的顺序读句子，第一个单词假如说是$x^{<1>}$，我们要做的就是将第一个词输入一个神经网络层，可以让神经网络尝试预测输出，判断这是否是人名的一部分。

循环神经网络做的是，当它读到句中的第二个单词$x^{<2>}$时，它不是仅用$x^{<2>}$就预测出${\hat{y}}^{<2>}$，也会输入一些来自时间步1的信息。具体而言，时间步1的激活值就会传递到时间步2。然后，在下一个时间步，循环神经网络输入了单词$x^{<3>}$，然后它尝试预测输出了预测结果${\hat{y}}^{<3>}$，一直到最后一个时间步，输入了$x^{<T_{x}>}$，然后输出了${\hat{y}}^{< T_{y} >}$。

至少在这个例子中$T_{x} =T_{y}$，同时如果$T_{x}$和$T_{y}$不相同，这个结构会需要作出一些改变。所以**==在每一个时间步中，循环神经网络传递一个激活值到下一个时间步中用于计算==**。

有些论文中会用上图编号2的图形来表示，在每一个时间步中，输入$x^{<t>}$然后输出$y^{<t>}$。为了表示循环连接，有时人们会画个圈，表示输回网络层，有时会画一个黑色方块，表示在这个黑色方块处会延迟一个时间步。

![](http://www.ai-start.com/dl2017/images/140529e4d7531babb5ba21778cd88bc3.png)

要开始整个流程，在零时刻需要构造一个激活值$a^{<0>}$，这通常是零向量（有些研究人员会用其他方法随机初始化$a^{<0>}$）。

循环神经网络是从左向右扫描数据，同时**每个时间步的参数也是共享的**。用$W_{\text{ax}}$来表示管理着从$x^{<1>}$到隐藏层的连接的一系列参数，每个时间步使用的都是相同的参数$W_{\text{ax}}$。而激活值也就是水平联系是由参数$W_{aa}$决定的，同时每一个时间步都使用相同的参数$W_{aa}$。同样的输出结果由$W_{\text{ya}}$决定。

在这个循环神经网络中，它的意思是在预测${\hat{y}}^{< 3 >}$时，不仅要使用$x^{<3>}$的信息，还要使用来自$x^{<1>}$和$x^{<2>}$的信息，因为来自$x^{<1>}$的信息可以通过上图编号1所示的路径来帮助预测${\hat{y}}^{<3>}$。

**这个循环神经网络的一个缺点就是它只使用了序列中之前的信息来做出预测**，尤其当预测${\hat{y}}^{<3>}$时，它没有用到$x^{<4>}$，$x^{<5>}$，$x^{<6>}$等等的信息。这就有一个问题，如果给定了这个句子，“Teddy Roosevelt was a great President.”，为了判断Teddy是否是人名的一部分，仅仅知道句中前两个词是完全不够的，还需要知道句中后部分的信息，不可能只看前三个单词就能分辨出其中的区别（这样特定的神经网络结构的一个限制是它在某一时刻的预测仅使用了从序列之前的输入信息并没有使用序列中后部分的信息，之后会用双向循环神经网络（BRNN）处理这个问题）。

**循环神经网络计算：**

![](http://www.ai-start.com/dl2017/images/19cbb2d356a2a6e0f35aa2a946b23a2a.png)

具体看一下这个神经网络计算了些什么：

先约定符号，举个例子$W_{\text{ax}}$，第二个下标意味着$W_{\text{ax}}$要乘以某个$x$类型的量，然后第一个下标$a$表示它是用来计算某个$a$类型的变量。同样的，$W_{\text{ya}}$乘上了某个$a$类型的量，用来计算出某个$\hat {y}$类型的量。

开始先输入$a^{<0>}$，它是一个零向量。接着就是前向传播过程，先计算激活值$a^{<1>}$，然后再计算$y^{<1>}$。
$$
\begin{split}
a^{<1>} &= g_{1}(W_{{aa}}a^{< 0 >} + W_{{ax}}x^{< 1 >} + b_{a}) \\
\hat y^{< 1 >} &= g_{2}(W_{{ya}}a^{< 1 >} + b_{y})
\end{split}
$$
![](http://www.ai-start.com/dl2017/images/rnn-f.png)

循环神经网络的第一个激活函数经常是tanh，有时候也会用ReLU，但是tanh是更通常的选择。选用哪个激活函数是取决于输出$y$，如果是一个二分问题，会用sigmoid函数作为激活函数，如果是$k$类别分类问题的话，那么可以选用softmax作为激活函数。对于命名实体识别来说$y$只可能是0或者1，所以第二个激活函数$g$可以是sigmoid激活函数。更一般的情况下，在$t$时刻：
$$
\begin{split}
a^{< t >} &= g_{1}(W_{aa}a^{< t - 1 >} + W_{ax}x^{< t >} + b_{a}) \\
\hat y^{< t >} &= g_{2}(W_{{ya}}a^{< t >} + b_{y})
\end{split}
$$
这些等式定义了神经网络的前向传播，从零向量$a^{<0>}$开始，然后用$a^{<0>}$和$x^{<1>}$来计算出$a^{<1>}$和$\hat y^{<1>}$，然后用$x^{<2>}$和$a^{<1>}$一起算出$a^{<2>}$和$\hat y^{<2>}$等等，从左到右完成前向传播。

**公式简化：**
$$
a^{<t>} =g(W_{a}\left\lbrack a^{< t-1 >},x^{<t>} \right\rbrack +b_{a})
$$
为了简化这些符号，将$W_{\text{aa}}a^{<t -1>} +W_{\text{ax}}x^{<t>}$写成$W_{a}\left\lbrack a^{< t-1 >},x^{} \right\rbrack$。

$W_{a}$的方式是将矩阵$W_{aa}$和矩阵$W_{{ax}}$水平并列放置，$[ {{W}_{aa}} \ \vdots \ {{W}_{ax}}]=W_{a}$。举个例子，如果$a$是100维的向量，$x$是10000维的向量，那么$W_{aa}$就是个$100\times100$维的矩阵，$W_{ax}$就是个$100\times10000$维的矩阵。因此如果将这两个矩阵堆起来，$W_{a}$就是个$100\times10100$维的矩阵。

$\left\lbrack a^{< t - 1 >},x^{< t >}\right\rbrack$的意思是将这两个向量堆在一起，即$\begin{bmatrix}a^{< t-1 >} \\ x^{< t >} \\\end{bmatrix}$，最终这就是个10100维的向量。

矩阵$[ {{W}_{aa}}\ \vdots \ {{W}_{ax}}] \times \begin{bmatrix} a^{< t - 1 >} \\ x^{< t >} \\ \end{bmatrix}$，刚好等于$W_{{aa}}a^{<t-1>} + W_{{ax}}x^{<t>}$。这种记法的好处是可以不使用两个参数矩阵$W_{{aa}}$和$W_{{ax}}$，而是将其压缩成一个参数矩阵$W_{a}$，当我们建立更复杂模型时这就能够简化要用到的符号。

同样对于$\hat y^{<t>} = g(W_{ya}a^{<t>} +b_{y})$，重写为：$\hat y^{< t >} = g(W_{y}a^{< t >} +b_{y})$。现在$W_{y}$和$b_{y}$符号仅有一个下标，它表示在计算时会输出什么类型的量。

$W_{y}$表明它是计算$y$类型的量的权重矩阵，而上面的$W_{a}$和$b_{a}$则表示这些参数是用来计算$a$类型或者说是激活值的。

### 4. 通过时间的反向传播（Backpropagation through time）

![](http://www.ai-start.com/dl2017/images/71a0ed918704f6d35091d8b6d60793e4.png)

前向传播的计算：输入序列，$x^{<1>}$，$x^{<2>}$，$x^{<3>}$一直到$x^{< T_{x} >}$，然后用$x^{<1>}$还有$a^{<0>}$计算出时间步1的激活项$a^{<1>}$，再用$x^{<2>}$和$a^{<1>}$计算出$a^{<2>}$，然后计算$a^{<3>}$等等，一直到$a^{< T_{x} >}$。

为了计算出$a^{<1>}$，需要一些参数，$W_{a}$和$b_{a}$，用它们来计算出$a^{<1>}$。**这些参数在之后的每一个时间步都会被用到**，会继续用这些参数计算$a^{<2>}$，$a^{<3>}$等等，所有的这些激活项都要取决于参数$W_{a}$和$b_{a}$。有了$a^{<1>}$，神经网络就可以计算第一个预测值$\hat y^{<1>}$，接着到下一个时间步，继续计算出$\hat y^{<2>}$，$\hat  y^{<3>}$，等等，一直到$\hat y^{<T_{y}>}$。为了计算出${\hat{y}}$，需要参数$W_{y}$和$b_{y}$，它们将被用于所有$y$节点。

为了计算反向传播，需要定义一个损失函数，先定义一个元素损失函数：
$$
L^{< t >}(\hat  y^{< t >},y^{< t >}) = - y^{<t>}\log\hat  y^{<t>}-( 1-\hat y^{<t>})log(1-\hat y^{<t>})
$$
将它定义为标准逻辑回归损失函数，也叫交叉熵损失函数（Cross Entropy Loss），它和之前我们在二分类问题中看到的公式很像。所以这是关于单个位置上或者说某个时间步$t$上某个单词的预测值的损失函数。现在定义整个序列的损失函数，将$L$定义为：
$$
L(\hat y,y) = \ \sum_{t = 1}^{T_{x}}{L^{< t >}(\hat  y^{< t >},y^{< t >})}
$$
在这个计算图中，通过$\hat y^{<1>}$可以计算对应的损失函数，于是计算出上图编号3的第一个时间步的损失函数，然后计算出第二个时间步的损失函数，然后是第三个时间步，一直到最后一个时间步。最后为了计算出总体损失函数，把它们都加起来，通过$L(\hat y,y)$计算出最后的损失函数，也就是把每个单独时间步的损失函数都加起来。

反向传播算法和之前一样需要在相反的方向上进行计算与传递信息，最终要做的就是把前向传播的箭头都反过来，然后就可以计算出所有合适的量，最后可以通过导数相关的参数，用梯度下降法来更新参数。

在这个反向传播的过程中，最重要的递归运算就是上图红色从右到左的运算，这个算法有一个很别致的名字，叫做“通过（穿越）时间反向传播（backpropagation through time）”。对于反向传播，需要从右到左进行计算，就像时间倒流，“通过时间反向传播”，就像穿越时光。

RNN反向传播示意图：

![nn_cell_backpro](http://www.ai-start.com/dl2017/images/rnn_cell_backprop.png)

### 5. 不同类型的循环神经网络（Different types of RNNs）

对于其他一些应用，$T_{x}$和$T_{y}$并不一定相等。比如音乐生成这个例子，$T_{x}$可以是长度为1甚至为空集。再比如电影情感分类，输出可以是1到5的整数，而输入是一个序列。在命名实体识别中，输入长度和输出长度是一样的。还有一些情况，输入长度和输出长度不同，他们都是序列但长度不同，比如机器翻译，一个法语句子和一个英语句子不同数量的单词却能表达同一个意思。

![](http://www.ai-start.com/dl2017/images/1daa38085604dd04e91ebc5e609d1179.png)

- 上图编号1所示是“一对一”的结构，当去掉$a^{<0>}$时它就是一种标准类型的神经网络。

- 上图编号2所示是“一对多”的结构，比如音乐生成或者序列生成。

  这样的神经网络结构，首先是输入$x$，得到RNN的输出，然后就没有输入了，再得到第二个输出，接着输出第三个值等等，一直到合成这个音乐作品的最后一个音符。这里也可以写上输入$a^{<0>}$，有一个技术细节，**==当生成序列时通常会把前一个合成的输出也喂给下一层==**。

- 还有上图编号3所示的“多对一”，如情感分类的例子，首先读取输入，一个电影评论的文本，然后判断他们是否喜欢电影还是不喜欢。

  比如做情感分析，输入$x^{<1>}$，$x^{<2>}$，一次输入一个单词，如果输入文本是“These is nothing to like in this movie”。不再在每个时间上都有输出了，而是让这个RNN网络读入整个句子，然后在最后一个时间上得到输出。因为它有很多输入，很多的单词，然后只输出一个数字。

- 还有上图编号4所示“多对多”的结构，比如命名实体识别的例子，其中$T_{x}=T_{y}$。

- 最后还有一种上图编号5所示的“多对多”结构的版本，对于像机器翻译这样的应用，$T_{x}$和$T_{y}$就可以不同了。

  比如语言翻译，翻译后两个句子的长度可能不同，所以还需要一个新的网络结构。首先读入这个句子，读入这个输入，读完之后，这个网络就会输出翻译结果。有了这种结构$T_{x}$和$T_{y}$就可以是不同的长度了。这个网络的结构有两个不同的部分，第一部分是一个编码器，获取输入，比如要翻译的句子；后面的是解码器，它会读取整个句子，然后输出翻译成其他语言的结果。

### 6. 语言模型和序列生成（Language model and sequence generation）

什么是语言模型？比如一个语音识别系统，听到一个句子，“the apple and pear（pair） salad was delicious.”，pear和pair是近音词，所以究竟说了什么？一个好的语音识别系统要帮助输出的东西，即使这两句话听起来是如此相似。而让语音识别系统去选择第二个句子的方法就是使用一个语言模型，它能计算出这两句话各自的可能性。举个例子，一个语音识别模型可能算出第一句话的概率是$P( \text{The apple  and  pair  salad}) = 3.2 \times 10^{-13}$，而第二句话的概率是$P\left(\text{The apple  and  pear salad} \right) = 5.7 \times 10^{-10}$，比较这两个概率值，第二句话的概率比第一句高出1000倍以上，这就是为什么语音识别系统能够在这两句话中作出选择。

所以语言模型所做的就是，它会告诉你某个特定的句子它出现的概率是多少。**语言模型是两种系统的基本组成部分，一个刚才所说的语音识别系统，还有机器翻译系统，它要能正确输出最接近的句子**。而语言模型做的最基本工作就是输入一个句子（文本序列），$y^{<1>}$，$y^{<2>}$一直到$y^{<T_{y}>}$。对于语言模型来说，用$y$来表示这些序列比用$x$来表示要更好，然后语言模型会估计某个句子序列中各个单词出现的可能性。

![](http://www.ai-start.com/dl2017/images/986226c39270a1e14643e8658fe6c374.png)

那么如何建立一个语言模型呢？为了使用RNN建立出这样的模型，首先需要一个训练集，包含一个很大的英文文本语料库（corpus）或者其它的语言。语料库是自然语言处理的一个专有名词，意思就是数量众多的英文句子组成的文本。

假如说得到这么一句话，“Cats average 15 hours of sleep a day.”，要做的第一件事就是将这个句子标记化，意思就是建立一个字典，然后将每个单词都转换成对应的one-hot向量，也就是字典中的索引。可能还有一件事就是要定义句子的结尾，一般的做法就是增加一个额外的标记，叫做\<EOS\>，表示句子的结尾，这样能够搞清楚一个句子什么时候结束。这句话就会有9个输入，有$y^{<1>}$，$y^{<2>}$一直到$y^{<9>}$。在标记化的过程中，可以自行决定要不要把标点符号看成标记，也可以将句号也加入字典中。

还有一个问题如果训练集中有一些词不在字典里，在这种情况下，可以把未知单词替换成一个叫做\<UNK\>的未知词标志，只针对\<UNK\>建立概率模型，而不是针对具体的未知词。

![](http://www.ai-start.com/dl2017/images/8b901fc8fcab9e16b1fe26b92f4ec546.png)

现在来建立RNN模型：

1. 在第0个时间步，要计算激活项$a^{<1>}$，它是以$x^{<1 >}$作为输入的函数，而$x^{<1>}$会被设为全为0的集合，也就是0向量。在之前的$a^{<0>}$按照惯例也设为0向量，于是$a^{<1>}$要做的就是通过softmax进行一些预测来计算出第一个词可能会是什么，其结果就是$\hat y^{<1>}$。这一步其实就是通过一个softmax层来预测字典中的任意单词会是第一个词的概率。比如说第一个词是$a$的概率有多少，第一个词是cats的概率又有多少，这样一直到Zulu是第一个词的概率是多少。还有第一个词是\<UNK\>的概率有多少，第一个词是句子结尾标志的概率有多少，表示不必阅读。所以$\hat y^{<1>}$的输出是softmax的计算结果，它只是预测第一个词的概率，而不去管结果是什么。在这个例子中，最终会得到单词Cats。因为字典中有10,000个词，所以softmax层输出10,000种结果。
2. 然后RNN进入下个时间步，在下一时间步中，仍然使用激活项$a^{<1>}$，在这步要做的是计算出第二个词会是什么。现在依然给它传正确的第一个词，告诉它第一个词就是Cats，也就是$\hat y^{<1>}$，这就是为什么$ x^{<2>} = \hat y^{<1>}$。然后在第二个时间步中，输出结果同样经过softmax层进行预测，RNN的职责就是预测这些词的概率，而不会去管结果是什么，可能是Cats或者Zulu或者\<UNK\>或者\<EOS\>或者其他词，它只会考虑之前得到的词。
3. 然后再进行RNN的下个时间步，现在要计算$a^{<3>}$。为了预测第三个词，也就是15。现在给它之前两个词，告诉它Cats average是句子的前两个词，所以这是下一个输入，$x^{<3>} = \hat y^{<2>}$。输入average以后，现在要计算出字典中每一个词的概率，在这种情况下，正确结果会是15，以此类推。
4. 一直到最后，会停在第9个时间步，然后把$x^{<9>}$也就是$\hat y^{<8>}$传入，会输出$\hat y^{<9>}$，最后的得到结果会是\<EOS\>标志。在这一步中，通过前面这些得到的单词，不管它们是什么，都希望能预测出EOS句子结尾标志的概率会很高。

所以RNN中的每一步都会考虑前面得到的单词，这就是RNN如何学习从左往右地每次预测一个词。

接下来为了训练这个网络，要定义代价函数。于是，在某个时间步$t$，如果真正的词是$y^{<t>}$，而神经网络的softmax层预测结果值是$y^{<t>}$：
$$
L\left( \hat y^{<t>},y^{<t>}\right) = - \sum_{i}{y_{i}^{<t>}\log\hat y_{i}^{<t>}}
$$
而总体损失函数：
$$
L = \sum_{t}^{}{L^{< t >}\left( \hat y^{<t>},y^{<t>} \right)}
$$
如果用很大的训练集来训练这个RNN，就可以通过开头一系列单词像是Cars average 15等来预测之后单词的概率。现在有一个新句子，它是$y^{<1>}$，$y^{<2>}$，$y^{<3>}$，现在要计算出整个句子中各个单词的概率，方法就是第一个softmax层会输出$y^{<1>}$的概率，然后第二个softmax层会输出在考虑$y^{<1>}$的情况下$y^{<2>}$的概率，最后第三个softmax层考虑在$y^{<1>}$和$y^{<2>}$的情况下$y^{<3>}$的概率。把这三个概率相乘，最后得到这个含3个词的整个句子的概率：
$$
P(y^{<1>},y^{<2>},y^{<3>})= P(y^{<1>}) \cdot P(y^{<2>}|y^{<1>}) \cdot P(y^{<3>}|y^{<1>},y^{<2>})
$$

### 7. 对新序列采样（Sampling novel sequences）

训练一个序列模型之后，要想了解到这个模型学到了什么，一种非正式的方法就是进行一次新序列采样。

记住一个序列模型模拟了任意特定单词序列的概率，现在要做的就是对这些概率分布进行采样来生成一个新的单词序列。

![](http://www.ai-start.com/dl2017/images/8b901fc8fcab9e16b1fe26b92f4ec546.png)

1. 第一步要做的就是对模型生成的第一个词进行采样，输入$x^{<1>} =0$，$a^{<0>} =0$，在第一个时间步得到的是经过softmax层后得到的概率，然后根据这个softmax的分布进行随机采样。使用例如numpy命令`np.random.choice`，来根据向量中这些概率的分布进行采样，这样就能对第一个词进行采样了。
2. 然后继续下一个时间步，记住第二个时间步需要$\hat y^{<1>}$作为输入。**==不管在第一个时间步得到的是什么词，都要把它传递到下一个位置作为输入==**，然后softmax层就会预测$\hat y^{<2>}$是什么。最后再次用`np.random.choice`采样函数来对$\hat y^{<2>}$进行采样。
3. 然后再到下一个时间步，无论得到什么样的用one-hot码表示的选择结果，都把它传递到下一个时间步，然后对第三个词进行采样。不管得到什么都把它传递下去，一直这样直到最后一个时间步。

知道句子结束的方法：

- 如果代表句子结尾的标识在字典中，可以一直进行采样直到得到\<EOS\>标识，代表着已经抵达结尾，可以停止采样了。

- 另一种情况是，如果字典中没有这个词，可以决定对20个或100个或其他个单词进行采样，一直将采样进行下去直到达到所设定的时间步。

  不过这种过程有时候会产生一些未知标识\<UNK\>，如果要确保算法不会输出这种标识，能做的一件事就是拒绝采样过程中产生任何未知的标识，一旦出现就继续在剩下的词中进行重采样，直到得到一个不是未知标识的词。如果不介意有未知标识产生的话，也可以完全不管它们。

这就是如何在RNN语言模型中生成一个随机选择的句子，都是基于词汇的RNN模型。

![](http://www.ai-start.com/dl2017/images/1d31771da8ced333968541fbbf67e6f1.png)

根据实际的应用，还可以构建一个基于字符的RNN结构，在这种情况下，字典仅包含从a到z的字母，可能还会有空格符，还可以有数字0到9，字母也可以区分大小写。如果建立一个基于字符的语言模型，比起基于词汇的语言模型，序列$\hat y^{<1>}$，$\hat y^{<2>}$，$\hat y^{<3>}$在训练数据中将会是单独的字符，而不是单独的词汇。

使用基于字符的语言模型有优点也有缺点，优点是不必担心会出现未知的标识。不过基于字符的语言模型一个主要缺点就是最后会得到太多太长的序列，大多数英语句子只有10到20个的单词，但却可能包含很多很多字符。

所以**基于字符的语言模型在捕捉句子中的依赖关系也就是句子较前部分如何影响较后部分不如基于词汇的语言模型那样可以捕捉长范围的关系，并且基于字符的语言模型训练起来计算成本比较高昂**。所以自然语言处理绝大多数都是使用基于词汇的语言模型，但随着计算机性能越来越高，会有更多的应用。在一些特殊情况下，会开始使用基于字符的模型。但是这确实需要更昂贵的计算力来训练，所以现在并没有得到广泛地使用，除了一些比较专门需要处理大量未知的文本或者未知词汇的应用，还有一些要面对很多专有词汇的应用。

### 8. 循环神经网络的梯度消失（Vanishing gradients with RNNs）

基本的RNN算法有一个很大的问题，就是梯度消失的问题。

我们知道如果是个很深的神经网络，从输出$\hat y$得到的梯度很难传播回去，很难影响靠前层的权重，很难影响前面层的计算。对于有同样问题的RNN，首先从左到右前向传播，然后反向传播。但是反向传播会很困难，因为同样的梯度消失的问题，后面层的输出误差很难影响前面层的计算。

这就意味着，比如“The cat, which already ate ……, was full.”。实际上很难让一个神经网络能够意识到它要记住看到的cat是单数名词还是复数名词，然后在序列后面生成依赖单复数形式的was或者were。需要长时间记住前面单词是单数还是复数，这样后面的句子才能用到这些信息。

也正是这个原因，所以基础的RNN模型会有很多局部影响，意味着输出$\hat y^{<3>}$主要受$\hat y^{<3>}$附近的值的影响，最深层的值主要与其附近的输入有关，基本很难受到序列靠前$\hat y^{<3>}$的输入的影响。这是因为不管输出是什么，深层区域都很难反向传播到序列的前面部分，也因此网络很难调整序列前面的计算。这是基础的RNN算法的一个缺点。

在梯度爆炸的情况下，因为指数级大的梯度会让参数变得极其大，以至于网络参数崩溃。所以梯度爆炸很容易发现，因为参数会大到崩溃，会看到很多NaN错误，这意味着网络计算出现了数值溢出。如果发现了梯度爆炸的问题，一个解决方法就是用梯度修剪。

梯度修剪的意思就是观察梯度向量，如果它大于某个阈值，缩放梯度向量，保证它不会太大，这就是通过一些最大值来修剪的方法。这是相对比较鲁棒的解决方法。然而梯度消失更难解决，后面会主要讲解。

### 9. GRU单元（Gated Recurrent Unit（GRU））

![](http://www.ai-start.com/dl2017/images/1521560729.png)

RNN的时间$t$处，计算激活值，就上图张图而言，就是RNN隐藏层的单元的可视化呈现。

**GRU理解：**

“The cat, which already ate……, was full.”，需要记得猫是单数的，为了确保后面是was而不是were。当从左到右读这个句子，GRU单元将会有个新的变量称为$c$，代表细胞（cell），即记忆细胞。记忆细胞的作用是提供了记忆的能力，比如说一只猫是单数还是复数，所以当它看到之后的句子的时候，它仍能够判断句子的主语是单数还是复数。于是在时间$t$处，有记忆细胞$c^{<t>}$，然后我们看的是，GRU实际上输出了激活值$a^{<t>}$，$c^{<t>} = a^{<t>}$。

这些等式表示了GRU单元的计算，**在每个时间步，将用一个候选值重写记忆细胞**，即${\tilde{c}}^{<t>}$的值：
$$
{\tilde{c}}^{<t>} =tanh(W_{c}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{c})
$$
在GRU中真正重要的思想是有一个门，这个门叫做$\Gamma_{u}$，这是一个0到1之间的值。实际上这个值是把$W_{u}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{u}$带入sigmoid函数得到的：
$$
\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{u})
$$
可以想到$\Gamma_{u}$在大多数的情况下非常接近0或1，字母$u$表示“update”。

![](http://www.ai-start.com/dl2017/images/cfa628f62f1c57ee6213793a438957a3.png)

GRU的关键部分就是上图编号3所示的等式，即用$\tilde{c}$更新$c$的等式。然后门决定是否要真的更新它。$\Gamma_{u}$的作用就是决定什么时候会更新这个值，特别是当看到词组the cat，即句子的主语猫，这就是一个好时机去更新这个值。然后当使用完它的时候，“The cat, which already ate……, was full.”，然后就不需要记住它了。

$$
c^{<t>} = \Gamma_{u}*{\tilde{c}}^{<t>} +\left( 1- \Gamma_{u} \right)*c^{<t-1>}
$$
如果$\Gamma_{u} =1$，把$c^{<t>}$设为候选值$ {\tilde{c}}^{<t>}$。如果门值$\Gamma_{u}= 0$，则$c^{<t>} =c^{<t-1>}$，$c^{<t>}$则为旧的值。最后令：
$$
a^{<t>} = c^{<t>}
$$
从左到右扫描句子，当门值一直为0的时候（$\Gamma_{u}=0$），$c^{<t>}$一直不会被更新，延用旧的值，这样即使一直处理句子到最后，$c^{<t>}$应该会一直等$c^{<t-1>}$，于是它仍然记得猫是单数的。

这是一个简化过的GRU单元，优点是通过门决定，当从左到右扫描一个句子的时候，这个时机是要更新某个记忆细胞，还是不更新（中间$\Gamma_{u}=0$一直为0，表示一直不更新）直到真的需要使用记忆细胞的时候。因为sigmoid的值，现在门很容易取到0值，只要这个值是一个很大的负数。因为$\Gamma_{u}$很接近0，使得$c^{<t>} = c^{<t-1>}$，这就不会有梯度消失的问题了。因为$\Gamma_{u}$很接近0，这就是说$c^{<t>}$几乎就等于$c^{<t-1>}$，而且$c^{<t>}$的值也很好地被维持了，即使经过很多很多的时间步。这就是缓解梯度消失问题的关键，因此允许神经网络运行在非常庞大的依赖词上。

**完整GRU:**

对于完整的GRU单元要做的一个改变，就是在计算${\tilde{c}}^{<t>}$时给记忆细胞的新候选值加上一个新的项，添加一个门$\Gamma_{r}$，可以认为$r$代表相关性（relevance）：
$$
{\tilde{c}}^{<t>} =tanh(W_{c}\left\lbrack \Gamma_r * c^{<t-1>},x^{<t>} \right\rbrack +b_{c})
$$
这个$\Gamma_{r}$门告诉候选值${\tilde{c}}^{<t>}$跟$c^{<t-1>}$有多大的相关性，计算这个门$\Gamma_{r}$需要一个新的参数矩阵$W_{r}$：
$$
\Gamma_{r}= \sigma(W_{r}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{r})
$$
为什么要有$\Gamma_{r}$？因为多年来研究者们试验过很多不同可能的方法来设计这些单元，去尝试让神经网络有更深层的连接，去尝试产生更大范围的影响，还有解决梯度消失的问题，GRU就是其中一个研究者们最常使用的版本，也被发现在很多不同的问题上也是非常健壮和实用的。

### 10. 长短期记忆（LSTM（long short term memory）unit）

上一节学的GRU（门控循环单元），能够在序列中学习非常深的连接：

$$
\begin{split}
{\tilde{c}}^{<t>} &=tanh(W_{c}\left\lbrack \Gamma_r * c^{<t-1>},x^{<t>} \right\rbrack +b_{c}) \\
\Gamma_{r} &= \sigma(W_{r}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{r}) \\
\Gamma_{u} &= \sigma(W_{u}\left\lbrack c^{<t-1>},x^{<t>} \right\rbrack +b_{u})\\
c^{<t>} &= \Gamma_{u}*{\tilde{c}}^{<t>} +\left( 1- \Gamma_{u} \right)*c^{<t-1>}\\
a^{<t>} &= c^{<t>}
\end{split}
$$
LSTM是一个比GRU更加强大和通用的版本：
$$
\begin{split}
{\tilde{c}}^{<t>} &=tanh(W_{c}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +b_{c}) \\
\Gamma_{u} &= \sigma(W_{u}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +b_{u})\\
\Gamma_{f} &= \sigma(W_{f}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +b_{f})\\
\Gamma_{o} &= \sigma(W_{o}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +b_{o})\\
c^{<t>} &= \Gamma_{u}*{\tilde{c}}^{<t>} + \Gamma_{f} *c^{<t-1>}\\
a^{<t>} &= \Gamma_{o} * tanh(c^{<t>})
\end{split}
$$
LSTM中使用${\tilde{c}}^{<t>} = tanh(W_{c}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +b_{c}$来更新它的候选值${\tilde{c}}^{<t>}$。注意，在LSTM中不再有$c^{<t-1>} = a^{<t-1>}$的情况，现在专门使用$a^{<t>}$或者$a^{<t-1>}$，而不是用$c^{<t-1>}$，也不用$\Gamma_{r}$，即相关门。

像以前GRU一样有一个更新门$\Gamma_{u}$和表示更新的参数$W_{u}$，$\Gamma_{u}= \sigma(W_{u}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +b_{u})$。LSTM的一个新特性是不只有一个更新门控制。我们将用不同的项来代替它们，要用别的项来取代$\Gamma_{u}$和$1-\Gamma_{u}$。

用遗忘门（the forget gate），叫它$\Gamma_{f}$，所以这个$\Gamma_{f} =\sigma(W_{f}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +b_{f})$；然后还有一个新的输出门，$\Gamma_{o} =\sigma(W_{o}\left\lbrack a^{<t-1>},x^{<t>} \right\rbrack +>b_{o})$。

于是记忆细胞的更新值$c^{<t>} =\Gamma_{u}*{\tilde{c}}^{<t>} + \Gamma_{f}*c^{<t-1>}$。这就给了记忆细胞选择权去维持旧的值$c^{<t-1>}$或者加上新值${\tilde{c}}^{<t>}$，所以这里用了单独的更新门$\Gamma_{u}$和遗忘门$\Gamma_{f}$，

最后$a^{<t>} = c^{<t>}​$的式子会变成$a^{<t>} = \Gamma_{o} * tanh(c^{<t>})​$。这就是LSTM主要的式子了，有三个门而不是两个，这有点复杂，它把门放到了和之前有点不同的地方。

![](http://www.ai-start.com/dl2017/images/LSTM.png)

在这上图里是用$a^{<t-1>}$， $x^{<t>}$一起来计算遗忘门$\Gamma_{f}$的值，更新门$\Gamma_{u}$以及输出门$\Gamma_{o}$。然后也经过tanh函数来计算${\tilde{c}}^{<t>}$。

![](http://www.ai-start.com/dl2017/images/LSTM_rnn.png)

这里其中一个元素很有意思，如果把它们连起来，就是把它们按时间次序连起来。输入$x^{<1>}$，然后$x^{<2>}$，$x^{<3>}$，把这些单元依次连起来，输出了上一个时间的$a$，$a$会作为下一个时间步的输入，$c$同理。

上面的连线，显示了只要正确地设置了遗忘门和更新门，LSTM是相当容易把$c^{<0>}$的值一直往下传递到右边，比如$c^{<3>} = c^{<0>}$。这就是为什么即使经过很长很长的时间步，LSTM和GRU仍然非常擅长于长时间记忆某个值。

这就是LSTM，一般使用的版本会有些不同，最常用的版本可能是门值不仅取决于$a^{<t-1>}$和$x^{<t>}$，有时候也可以偷窥一下$c^{<t-1>}$的值），这叫做“窥视孔连接”（peephole connection）。其意思就是门值不仅取决于$a^{<t-1>}$和$x^{<t>}$，也取决于上一个记忆细胞的值（$c^{<t-1>}$），然后“偷窥孔连接”就可以结合这三个门（$\Gamma_{u}$、$\Gamma_{f}$、$\Gamma_{o}$）来计算了。

LSTM反向传播计算：

- 门求偏导：

$d \Gamma_o^{\langle t \rangle} = da_{next}*\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}*(1-\Gamma_o^{\langle t \rangle})\tag{1}$

$d\tilde c^{\langle t \rangle} = dc_{next}*\Gamma_i^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * i_t * da_{next} * \tilde c^{\langle t \rangle} * (1-\tanh(\tilde c)^2) \tag{2}$

$d\Gamma_u^{\langle t \rangle} = dc_{next}*\tilde c^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * \tilde c^{\langle t \rangle} * da_{next}*\Gamma_u^{\langle t \rangle}*(1-\Gamma_u^{\langle t \rangle})\tag{3}$

$d\Gamma_f^{\langle t \rangle} = dc_{next}*\tilde c_{prev} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * c_{prev} * da_{next}*\Gamma_f^{\langle t \rangle}*(1-\Gamma_f^{\langle t \rangle})\tag{4}$

- 参数求偏导 ：


$ dW_f = d\Gamma_f^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{5} $
$ dW_u = d\Gamma_u^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{6} $
 $ dW_c = d\tilde c^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{7} $
$ dW_o = d\Gamma_o^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{8}$

为了计算$db_f, db_u, db_c, db_o$ 需要各自对$d\Gamma_f^{\langle t \rangle}, d\Gamma_u^{\langle t \rangle}, d\tilde c^{\langle t \rangle}, d\Gamma_o^{\langle t \rangle}$ 求和。最后，计算隐藏状态、记忆状态和输入的偏导数：

$ da_{prev} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c^{\langle t \rangle} + W_o^T * d\Gamma_o^{\langle t \rangle} \tag{9}$

$ dc_{prev} = dc_{next}\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c_{next})^2)*\Gamma_f^{\langle t \rangle}*da_{next} \tag{10}$

$ dx^{\langle t \rangle} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c_t + W_o^T * d\Gamma_o^{\langle t \rangle}\tag{11} $

**对比：**

GRU的优点是这是个更加简单的模型，更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，然后它可以扩大模型的规模。

LSTM更加强大和灵活，因为它有三个门而不是两个。如果想选一个使用，LSTM在历史进程上是个更优先的选择。最近几年GRU也获得了很多支持，因为它更加简单，而且还效果还不错，它更容易适应规模更加大的问题。

无论是GRU还是LSTM，都可以用它们来构建捕获更加深层连接的神经网络。

### 11. 双向循环神经网络（Bidirectional RNN）

判断一个语句中Teddy是否为人名，除了之前的单词，还需要后面的信息

![](http://www.ai-start.com/dl2017/images/48c787912f7f8daee638dd311583d6cf.png)

假设只有4个单词的句子，$x^{<1>}$到$x^{<4>}$。这个网络会有一个前向的循环单元叫做${\overrightarrow{a}}^{<1>}$，${\overrightarrow{a}}^{<2>}$，${\overrightarrow{a}}^{<3>}$还有${\overrightarrow{a}}^{<4>}$，这四个循环单元都有一个当前输入$x$输入进去，得到预测的$\hat y^{<1>}$，$\hat y^{<2>}$，$\hat y^{<3>}$和$\hat y^{<4>}$。

之后需要增加一个反向循环层，这里有个${\overleftarrow{a}}^{<1>}$，左箭头代表反向连接，${\overleftarrow{a}}^{<2>}$反向连接，${\overleftarrow{a}}^{<3>}$反向连接，${\overleftarrow{a}}^{<4>}$反向连接，所以这里的左箭头代表反向连接。

同样，把网络向上连接，这些$a$依次反向向前连接。这样，这个网络就构成了一个无环图。给定一个输入序列$x^{<1>}$到$x^{<4>}$，这个序列首先计算前向的${\overrightarrow{a}}^{<1>}$，然后计算前向的${\overrightarrow{a}}^{<2>}$，接着${\overrightarrow{a}}^{<3>}$，${\overrightarrow{a}}^{<4>}$。反向序列从计算${\overleftarrow{a}}^{<4>}$开始，反向进行计算反向的${\overleftarrow{a}}^{<3>}$。计算完了反向的${\overleftarrow{a}}^{<3>}$，可以用这些激活值计算反向的${\overleftarrow{a}}^{<2>}$，然后是反向的${\overleftarrow{a}}^{<1>}$，把所有这些激活值都计算完了就可以计算预测结果了。

![](http://www.ai-start.com/dl2017/images/053831ff43d039bd5e734df96d8794cb.png)

为了预测结果，$\hat y^{<t>} =g(W_{g}\left\lbrack {\overrightarrow{a}}^{<t>},{\overleftarrow{a}}^{<t>} \right\rbrack +b_{y})$。假如要观察时间3的预测结果，信息从$x^{<1>}$过来，流经这里，前向的${\overrightarrow{a}}^{<1>}$到前向的${\overrightarrow{a}}^{<2>}$，到前向的${\overrightarrow{a}}^{<3>}$再到$\hat y^{<3>}$，所以从$x^{<1>}$，$x^{<2>}$，$x^{<3>}$来的信息都会考虑在内。而从$x^{<4>}$来的信息会流过反向的${\overleftarrow{a}}^{<4>}$，到反向的${\overleftarrow{a}}^{<3>}$再到$\hat y^{<3>}$。这样使得时间3的预测结果不仅输入了过去的信息，还有现在的信息，这一步涉及了前向和反向的传播信息以及未来的信息。

这就是双向循环神经网络，并且这些基本单元不光可以是标准RNN单元，也可以是GRU单元或者LSTM单元。事实上，很多的NLP问题，对于大量有自然语言处理问题的文本，**有LSTM单元的双向RNN模型是用的最多的**。所以如果有NLP问题，并且文本句子都是完整的，首先需要标定这些句子，一个有LSTM单元的双向RNN模型是一个不错的首选。

**双向RNN网络模型的缺点就是需要完整的数据的序列，才能预测任意位置**。比如要构建一个语音识别系统，那么双向RNN模型需要考虑整个语音表达，需要等待这个人说完，获取到整个语音表达才能处理这段语音，并进一步做语音识别。对于实际的语音识别的应用通常会有更加复杂的模块，而不是仅仅用我们见过的标准的双向RNN模型。但是对于很多自然语言处理的应用，如果总是可以获取整个句子，这个标准的双向RNN算法实际上很高效。

### 12. 深层循环神经网络（Deep RNNs）

目前学到的不同RNN的版本，每一个都可以独当一面。但是要学习非常复杂的函数，通常会把RNN的多个层堆叠在一起构建更深的模型。

![](http://www.ai-start.com/dl2017/images/8378c2bfe73e1ac9f85d6aa79b71b5eb.png)

一个标准的神经网络，首先是输入$x$，然后堆叠上隐藏层。比如说第一层是$a^{\left\lbrack 1 \right\rbrack}$，接着堆叠上下一层，激活值$a^{\left\lbrack 2 \right\rbrack}$，可以再加一层$a^{\left\lbrack 3 \right\rbrack}$，然后得到预测值$\hat{y}$。深层的RNN网络跟这个有点像。

这里把符号稍微改了一下，不再用原来的$a^{<0 >}$表示0时刻的激活值了，而是用$a^{\lbrack 1\rbrack <0>}$来表示第一层。所以现在用$a^{\lbrack l\rbrack <t>}$来表示第$l$层的激活值，这个\<t\>表示第$t$个时间点。第一层第一个时间点的激活值$a^{\lbrack 1\rbrack <1>}$，$a^{\lbrack 1\rbrack <2>}$就是第一层第二个时间点的激活值。然后把编号4方框内所示的部分堆叠在上面，就是一个有三个隐层的新的网络。

具体看$a^{\lbrack 2\rbrack <3>}$是怎么算的：激活值$a^{\lbrack 2\rbrack <3>}$有两个输入，一个是从下面过来的输入，还有一个是从左边过来的输入，所以$a^{\lbrack 2\rbrack < 3 >} = g(W_{a}^{\left\lbrack 2 \right\rbrack}\left\lbrack a^{\left\lbrack 2 \right\rbrack < 2 >},a^{\left\lbrack 1 \right\rbrack < 3 >} \right\rbrack + b_{a}^{\left\lbrack 2 \right\rbrack})$，这就是$a^{\lbrack 2\rbrack <3>}$的激活值计算方法。参数$W_{a}^{\left\lbrack 2 \right\rbrack}$和$b_{a}^{\left\lbrack 2 \right\rbrack}$在这一层的计算里都一样，相对应地第一层也有自己的参数$W_{a}^{\left\lbrack 1 \right\rbrack}$和$b_{a}^{\left\lbrack 1 \right\rbrack}$。

对于像编号1这样标准的神经网络，网络可能很深，甚至于100层深，而对于RNN来说，有三层就已经不少了。由于时间的维度，RNN网络会变得相当大，很少会看到网络堆叠到100层。

![](http://www.ai-start.com/dl2017/images/455863a3c8c2dfaa0e5474bfa2c6824d.png)

但有一种会容易见到，就是在每一个上面堆叠循环层，把原图的$\hat y^{<1>}$，然后换成一些深的层，这些层并不水平连接，只是一个深层的网络，然后用来预测$y^{<1>}$。同样后面$y^{<T_y>}$也加上深层网络，这种类型的网络结构用的会稍微多一点，这种结构有三个循环单元，在时间上连接，接着一个网络在后面接一个网络，这是一个深层网络，但没有水平方向上的连接，所以这种类型的结构会见得多一点。

通常这些单元没必要非是标准的RNN，可以是最简单的RNN模型，也可以是GRU单元或者LSTM单元，也可以构建深层的双向RNN网络。由于深层的RNN训练需要很多计算资源，需要很长的时间，不会像卷积神经网络一样有大量的隐藏层。