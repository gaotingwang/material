[TOC]

##自然语言处理与词嵌入

### 1. 词汇表征（Word Representation）

词嵌入（word embeddings），是语言表示的一种方式，可以让算法自动的理解一些类似的词，比如男人对女人，国王对王后。通过词嵌入的概念就可以构建NLP应用了，即使模型标记的训练集相对较小。

![](http://www.ai-start.com/dl2017/images/68d7c930146724f39782cb57d33161e9.png)

我们一直用one-hot向量来表示词，比如man在词典里是第5391个，那么就可以表示成一个向量，只在第5391处为1，用$O_{5391}$代表这个量，这里的$O$代表one-hot。这种表示方法的一大缺点就是它把每个词孤立起来，这样使得算法对相关词的泛化能力不强。

举个例子，假如已经学习到了一个语言模型，即使学习算法已经学到了“I want a glass of orange juice”这样一个很可能的句子，但如果看到“I want a glass of apple \_\_\_”，因为算法不知道apple和orange的关系很接近，就像man和woman，king和queen一样。所以算法很难从已经知道的orange juice是一个常见的东西，而明白apple juice也是很常见的东西或者说常见的句子。

如果取两个向量，然后计算它们的内积，结果就是0。如果用apple和orange来计算它们的内积，结果也是0。很难区分它们之间的差别，因为这些向量内积都是一样的，所以无法知道apple和orange要比king和orange，或者queen和orange相似地多。

换一种表示方式会更好，如果不用one-hot表示，而是用特征化的表示来表示每个词。

![](http://www.ai-start.com/dl2017/images/ce30c9ae7912bdb3562199bf85eca1cd.png)

举个例子，比如想知道这些词与Gender（性别）的关系。假定男性的性别为-1，女性的性别为+1，那么man的性别值可能就是-1，而woman就是-1。最终根据经验king就是-0.95，queen是+0.97，apple和orange没有性别可言。另一个特征可以是Royal（高贵），man，woman和高贵没太关系，所以它们的特征值接近0。而king和queen很高贵，apple和orange跟高贵也没太大关系。还可以有很多的其他特征，从Size（尺寸大小），Cost（花费多少），这个东西是不是alive（活的），是不是一个Action（动作），或者是不是Noun（名词）或者是不是Verb（动词），还是其他的等等。

可以想很多的特征，为了说明，假设有300个不同的特征，这样的话就有了上图编号1所示这一列数字，这样就组成了一个300维的向量来表示man这个词。接下来，用$e_{5391}$这个符号来表示这个向量。同样用$e_{9853}$代表women这个300维的向量，其他的例子也一样。

现在，如果用这种表示方法来表示apple和orange这些词，那么apple和orange的这种表示肯定会非常相似。可能有些特征不太一样，比如颜色口味，但总的来说apple和orange的大部分特征实际上都一样，或者说都有相似的值。这样对于已经知道orange juice的算法，很大几率上也会明白apple juice这个东西，这样对于不同的单词算法会泛化的更好。

这种高维特征的表示能够比one-hot更好的表示不同的单词。最终学习的特征不会像这里一样这么好理解，不会像第一个特征就是性别，第二个特征就是高贵，第三个特征是年龄等等这些，新的特征表示的东西肯定会更难搞清楚。尽管如此，特征表示方法却能使算法更高效地发现apple和orange会比king和orange，queen和orange更加相似。

![](http://www.ai-start.com/dl2017/images/59fb45cfdf7faa53571ec7b921b78358.png)

如果能够学习到一个300维的特征向量，或者说300维的词嵌入，通常可以做一件事，把这300维的数据嵌入到一个二维空间里，这样就可以可视化了。

常用的可视化算法是t-SNE算法，如果观察这种词嵌入的表示方法，会发现man和woman这些词聚集在一块，king和queen聚集在一块，而这些都是人，所以也都聚集在一起。动物都聚集在一起，水果也都聚集在一起，像1、2、3、4这些数字也聚集在一起。如果把生物看成一个整体，他们也聚集在一起。

这种词嵌入算法对于相近的概念，学到的特征也比较类似，在对这些概念可视化的时候，这些概念就比较相似，最终把它们映射为相似的特征向量。这种表示方式用的是在300维空间里的特征表示，这叫做嵌入（embeddings）。之所以叫嵌入的原因是，可以想象一个300维的空间，现在取每一个单词比如orange，这个词就被嵌在这个300维空间里的一个点上了，apple这个词就被嵌在这个300维空间的另一个点上了。为了可视化，t-SNE算法把这个空间映射到低维空间，就可以画出一个2维图像然后观察，这就是这个术语嵌入的来源。

### 2. 使用词嵌入（Using Word Embeddings）

使用词嵌入做迁移学习的步骤：

1. 第一步，先从一个非常大的文本集中学习词嵌入，或者可以下载网上预训练好的词嵌入模型。
2. 第二步，可以用这些词嵌入模型把它迁移到新的只有少量标注训练集的任务中，比如说用300维的词嵌入来表示你的单词。这样做的一个好处就是可以用更低维度的特征向量代替原来的10000维的one-hot向量，现在可以用一个300维更加紧凑的向量。尽管one-hot向量计算很快，而用学到的词嵌入的300维的向量会更加紧凑。
3. 第三步，在任务上训练模型时，比如命名实体识别任务上，只有少量的标记数据集上，可以自己选择要不要继续微调，用新的数据调整词嵌入。实际中，只有这个第二步中有很大的数据集才会这样做，如果自己的标记数据集不是很大，通常不会在微调词嵌入上费力气。

在前面的课程学到了关于人脸编码的知识了，对于人脸识别，我们训练了一个Siamese网络结构，这个网络会学习不同人脸的一个128维表示，然后通过比较编码结果来判断两个图片是否是同一个人脸。词嵌入的意思和这个差不多。在人脸识别领域大家喜欢用编码（encoding）这个词来指代向量$f(x^{\left(i \right)})$，$f(x^{\left( j\right)})$，人脸识别领域和这里的词嵌入（embedding）有一个不同就是：在人脸识别中训练一个网络，任给一个人脸照片，甚至是没有见过的照片，神经网络都会计算出相应的一个编码结果。学习词嵌入则是有一个固定的词汇表，比如10000个单词，学习向量$e_{1}$到$e_{10000}$，学习一个固定的编码，每一个词汇表的单词的固定嵌入。人脸识别中的算法未来可能涉及到海量的人脸照片，而**自然语言处理有一个固定的词汇表**，而像一些没有出现过的单词就记为未知单词。

### 3. 词嵌入的特性（Properties of Word Embeddings）

词嵌入还有一个迷人的特性就是它还能帮助实现类比推理，尽管类比推理可能不是自然语言处理应用中最重要的，不过它能帮助人们理解词嵌入做了什么，以及词嵌入能够做什么。

man如果对应woman，我们应该都能猜到king应该对应queen。能否有一种算法来自动推导出这种关系，下面就是实现的方法。

![](http://www.ai-start.com/dl2017/images/12242657bd982acd1d80570cc090b4fe.png)

这里用一个四维向量来表示man，用$e_{5391}$来表示，不过在这节先把它称为$e_{\text{man}}$，对于其他也是用一样的表示方法。这些向量有一个有趣的特性，将向量$e_{\text{man}}$和$e_{\text{woman}}$它们进行减法运算：
$$
e_{\text{man}} - e_{\text{woman}} = \begin{bmatrix}
 - 1 \\
     0.01 \\
       0.03 \\
       0.09 \\
       \end{bmatrix} - \begin{bmatrix}
       1 \\
       0.02 \\
       0.02 \\
       0.01 \\
       \end{bmatrix} = \begin{bmatrix}
 - 2 \\
 - 0.01 \\
     0.01 \\
       0.08 \\
       \end{bmatrix} \approx \begin{bmatrix}
 - 2 \\
     0 \\
       0 \\
       0 \\
       \end{bmatrix}
$$
类似的，用$e_{\text{king}}$减去$e_{\text{queen}}$，最后也会得到一样的结果，即：
$$
e_{\text{king}} - e_{\text{queen}} = \begin{bmatrix}
 - 0.95 \\
     0.93 \\
       0.70 \\
       0.02 \\
       \end{bmatrix} - \begin{bmatrix}
       0.97 \\
       0.95 \\
       0.69 \\
       0.01 \\
       \end{bmatrix} = \begin{bmatrix}
 - 1.92 \\
 - 0.02 \\
     0.01 \\
       0.01 \\
       \end{bmatrix} \approx \begin{bmatrix}
 - 2 \\
     0 \\
       0 \\
       0 \\
       \end{bmatrix}
$$
这个结果表示，man和woman主要的差异是gender（性别）上的差异，而king和queen之间的主要差异，根据向量的表示，也是gender（性别）上的差异，这就是为什么$e_{\text{man}}- e_{\text{woman}}$和$e_{\text{king}} -  e_{\text{queen}}$结果是相同的。

所以得出这种类比推理的结论的方法就是，当算法被问及man对woman相当于king对什么时，算法所做的就是计算$e_{\text{man}}-e_{\text{woman}}$，然后找出一个向量，使得$e_{\text{man}}-e_{\text{woman}}$≈$\ e_{\text{king}}- e_{?}$ 。

如何把这种思想写成算法？词嵌入向量在一个可能有300维的空间里，为了计算当man对于woman，那么king对于什么，能做的就是找到单词$w$来使得，$e_{\text{man}}-e_{\text{woman}}≈ e_{\text{king}} - e_{w}$这个等式成立：
$$
Find\ word\ w:argMax \ Sim(e_{w},e_{\text{king}} - e_{\text{man}} + e_{\text{woman}})
$$
所做的就是等式变换，把$e_{w}$单独放到等式的一边，于是等式的另一边就会是$e_{\text{king}}- e_{\text{man}} +  e_{\text{woman}}$。有一些用于测算相似度的函数，然后通过方程找到一个使得相似度最大的单词。

列举一个最常用的相似度函数，叫做余弦相似度。在余弦相似度中，假如在向量$u$和$v$之间定义相似度:
$$
\text{sim}\left( u,v \right) = \frac{u^{T}v}{\left| \left| u \right| \right|_{2}\left| \left| v \right| \right|_{2}}
$$
先不看分母，分子其实就是$u$和$v$的内积。如果$u$和$v$非常相似，那么它们的内积将会很大，把整个式子叫做余弦相似度。其实就是因为该式是$u$和$v$的夹角的余弦值，这个夹角就是$\Theta$角，这个公式实际就是计算两向量夹角$\Theta$的余弦。所以夹角为0度时，余弦相似度就是1，当夹角是90度角时余弦相似度就是0，当它们是180度时，图像完全跑到了相反的方向，这时相似度等于-1，这就是为什么余弦相似度对于这种类比工作能起到非常好的效果。

还能用平方距离来表示:$\left| \left| u - v \right| \right|^{2}$ ，从学术上来说，比起测量相似度，这个函数更容易测量的是相异度，所以需要对其取负，这个函数才能正常工作。余弦相似度用得更多一点，这两者的主要区别是它们对$u$和$v$之间的距离标准化的方式不同。

值得注意的是这种方法真的有效，如果学习一些词嵌入，通过算法来找到使得相似度最大化的单词$w$，确实可以得到完全正确的答案。不过这取决于过程中的细节，如果查看一些研究论文不难发现，通过这种方法来做类比推理准确率大概只有30%\~75%。

![](http://www.ai-start.com/dl2017/images/012c07b7692aed382a2875292ea8e81b.png)

t-SNE算法所做的就是把这些300维的数据用一种非线性的方式映射到2维平面上，可以得知t-SNE中这种映射很复杂而且很非线性。在进行t-SNE映射之后，不能总是期望使等式成立的关系，在大多数情况下，由于t-SNE的非线性映射，就没法再指望这种平行四边形了，很多这种平行四边形的类比关系在t-SNE映射中都会失去原貌。

词嵌入的一个显著成果就是，可学习的类比关系的一般性，只要从足够大的语料库中进行学习，它就能自主地发现这些模式。可能不会自己动手构建一个类比推理系统作为一项应用，不过希望在这些可学习的类特征的表示方式能够给我们一些直观的感受。

### 4. 嵌入矩阵（Embedding Matrix）

当应用算法来学习词嵌入时，实际上是学习一个嵌入矩阵。

假设词汇表含有10,000个单词，可能还有一个未知词标记\<UNK\>，有300个特征。要做的就是学习一个嵌入矩阵$E$，它将是一个300×10,000的矩阵。

![](http://www.ai-start.com/dl2017/images/fa320bd001f9dca8ec33c7a426e20d80.png)

假设orange的单词编号是6257，代表词汇表中第6257个单词，用符号$O_{6527}$来表示这个one-hot向量，这个向量除了第6527个位置上是1，其余各处都为0，显然它是一个10,000维的列向量。

假设这个嵌入矩阵叫做矩阵$E$，注意如果用$E$去乘以右边的one-hot向量$O_{6527}$，就会得到一个300维的向量。$E$是300×10,000的，$O_{6527}$是10,000×1的，所以它们的积是300×1的，即300维的向量。最后得到的300维的列，就是单词orange下的彩色一列，用$e_{6257}$表示。

更广泛来说，假如说有某个单词$w$，那么$e_{w}$就代表单词$w$的嵌入向量，表示的是字典中单词$w$的嵌入向量。我们的目标是学习一个嵌入矩阵$E$ ，矩阵$E$它包含了词汇表中所有单词的嵌入向量。在下节中将会随机地初始化矩阵$E$ 。然后使用梯度下降法来学习这个300×10000的矩阵中的各个参数，$E$乘以这个one-hot向量会得到嵌入向量。

但当动手实现时，用大量的矩阵和向量相乘来计算它，效率是很低下的，因为one-hot向量是一个维度非常高的向量，并且几乎所有元素都是0，所以矩阵向量相乘效率太低。所以在实践中会使用一个专门的函数来单独查找矩阵$E$的某列，而不是用通常的矩阵乘法来做。但是例如在Keras中就有一个嵌入层，然后我们用这个嵌入层更有效地从嵌入矩阵中提取出你需要的列，而不是对矩阵进行很慢很复杂的乘法运算。

### 5. 学习词嵌入（Learning Word Embeddings）

在深度学习应用于学习词嵌入的历史上，人们一开始使用的算法比较复杂，但随着时间推移，研究者们不断发现他们能用更加简单的算法来达到一样好的效果，特别是在数据集很大的情况下。现在很多最流行的算法都十分简单，如果一开始就介绍这些简单的算法，可能奇怪这么简单的算法究竟是怎么起作用的。之后会对这些算法进行简化，使你能够明白即使一些简单的算法也能得到非常好的结果。

![](http://www.ai-start.com/dl2017/images/31347eca490e0ae8541140fb01c04d72.png)

下面将介绍如何建立神经网络来预测序列中的下一个单词，将$E$与单词的one-hot向量（$O_{9665}$）相乘得到嵌入向量$e_{9665}$。同样地对其他单词也这样操作。于是现在有许多300维的嵌入向量，把它们全部放进神经网络中，经过神经网络以后再通过softmax层，这个softmax也有自己的参数，这个softmax分类器会在10,000个可能的输出中预测结尾这个单词。

隐藏层有自己的参数，用$W^{\left\lbrack1 \right\rbrack}$和$b^{\left\lbrack 1\right\rbrack}$来表示，softmax层也有自己的参数$W^{\left\lbrack2 \right\rbrack}$和$b^{\left\lbrack 2\right\rbrack}$。如果它们用的是300维大小的嵌入向量，而这里有6个词，所以用6×300，所以这个输入会是一个1800维的向量。

![](http://www.ai-start.com/dl2017/images/747e619260737ded586ae51b3b4f07d6.png)

实际上更常见的是有一个固定的历史窗口，举个例子，总是想预测给定的四个单词后的下一个单词，这里的4是算法的超参数。这就是如何适应很长或者很短的句子，方法就是总是只看前4个单词，所以只用上图编号2所示这4个单词。如果一直使用一个4个词的历史窗口，这就意味着神经网络会输入一个1200维的特征变量到神经网络的隐藏藏，然后再通过softmax来预测输出。用一个固定的历史窗口就意味着可以处理任意长度的句子，因为输入的维度总是固定的。

这个模型的参数就是矩阵$E$，对所有的单词用的都是同一个矩阵$E$，而不是对应不同的位置上的不同单词用不同的矩阵。然后权重$W,b$也都是算法的参数，可以用反向传播来进行梯度下降来最大化训练集似然，通过序列中给定的4个单词去重复地预测出语料库中下一个单词什么。

事实上通过这个算法能很好地学习词嵌入，原因是在这个算法的激励下，apple和orange会学到很相似的嵌入，这样做能够让算法更好地拟合训练集，因为它有时看到的是orange juice，有时看到的是apple juice。如果只用一个300维的特征向量来表示所有这些词，算法要想最好地拟合训练集，就要使apple（苹果）、orange（橘子）、grape（葡萄）和pear（梨），还有像durian（榴莲）这种很稀有的水果，都拥有相似的特征向量。

这就是早期最成功的学习词嵌入，学习这个矩阵$E$的算法之一。如果要建立一个语言模型，那么一般选取目标词之前的几个词作为上下文。但如果目标是学习一个嵌入向量，研究人员发现可以用其他类型的上下文，它们也能得到很好的词嵌入。比如，可以把目标词左右各4个词作为上下文，来预测中间的目标词，这也可以用来学习词嵌入。或者想用一个更简单的上下文，只用目标词的前一个词，来预测该词的下一个词。

还有一个效果非常好的做法就是上下文是附近的一个单词，在下节课中把它公式化，这用的是一种Skip-Gram模型的思想。这是一个简单算法的例子，因为上下文相当的简单，比起之前4个词，现在只有1个，但是这种算法依然能工作得很好。

### 6. Word2Vec

已经见到了如何学习一个神经语言模型来得到更好的词嵌入， Word2Vec算法是一种简单而且计算时更加高效的方式。

==**在Skip-Gram模型中，要做的是抽取上下文和目标词配对，来构造一个监督学习问题。上下文不一定总是目标单词之前离得最近的四个单词，或最近的$n$个单词。要的做的是随机选一个词作为上下文词，然后要做的是随机在一定词距内选另一个词**==，比如在上下文词前后5个词内选择目标词。

假设在训练集中给定了一个这样的句子：“I want a glass of orange juice to go along with my cereal.”，比如上下文词选orange，可能正好选到了juice作为目标词，也有可能选到了前面第二个词，所以另一种配对目标词可以是glass，还可能选到了单词my作为目标词。

于是构造一个监督学习问题，它给定上下文词，要求预测在这个词正负10个词距内随机选择的某个目标词。这不是个简单的学习问题，因为在正负10个词距之间，可能会有很多不同的单词。但是构造这个监督学习的目标并不是要解决这个监督学习问题本身，而是想要使用这个学习问题来学到一个好的词嵌入模型。

![](http://www.ai-start.com/dl2017/images/4ebf216a59d46efa2136f72b51fd49bd.png)

假设使用一个10,000词的词汇表，从上下文$c$，到某个目标词，记为$t$

- 上下文词的one-hot向量，将其写作$O_{c}$，之后拿嵌入矩阵$E$乘以向量$O_{c}$，然后得到了输入的上下文词的嵌入向量$e_{c}=EO_{c}$。

- 将把向量$e_{c}$喂入一个softmax单元，softmax单元要做的就是输出$\hat y$。softmax模型预测不同目标词的概率：
  $$
  Softmax:p\left( t \middle| c \right) = \frac{e^{\theta_{t}^{T}e_{c}}}{\sum_{j = 1}^{10,000}e^{\theta_{j}^{T}e_{c}}}
  $$
  这里$\theta_{t}$是一个与输出$t$有关的参数，即某个词$t$和标签相符的概率是多少，这里省略了softmax中的偏差项，想要加上的话也可以加上。

- 最终softmax的损失函数就会像之前一样，用$y$表示目标词，这里用的$y$和$\hat y$都是用one-hot表示的，于是损失函数就会是：
  $$
  L\left( \hat y,y \right) = - \sum_{i = 1}^{10,000}{y_{i}\log \hat y_{i}}
  $$
  $y$ 就是只有一个1其他都是0的one-hot向量，如果目标词是juice，那么第4834个元素就是1，其余是0。类似的$\hat y$是一个从softmax单元输出的10,000维的向量，这个向量是所有可能目标词的概率。

实际上使用这个算法会遇到一些问题，首要的问题就是计算速度。尤其是在softmax模型中，每次想要计算这个概率，需要对词汇表中的所有词做求和计算，那么这个分母的求和操作是相当慢的，实际上10000已经是相当慢的了，所以扩大词汇表就更加困难了。

这里有一些解决方案，如分级（hierarchical）的softmax分类器和负采样（Negative Sampling）。

![](http://www.ai-start.com/dl2017/images/89743b5ade106cad1318b8f3f4547a7f.png)

在文献中会看到的一个方法是使用分级（hierarchical）的softmax分类器，意思就是说不要一下子就确定到底是属于10,000类中的哪一类。想象有一个分类器如上图编号1所示，它能告诉目标词是在词汇表的前5000个中还是在词汇表的后5000个词中，假如这个分类器告诉在前5000个词中。然后第二个分类器会告诉这个词在词汇表的前2500个词中，或者在词汇表的第二组2500个词中，诸如此类，直到最终找到一个词准确所在的分类器。

像这样有一个树形的分类器，意味着树上内部的每一个节点都可以是一个二分类器，比如逻辑回归分类器，所以不需要再为每个分类，对词汇表中所有的10000个词求和了。实际上用这样的分类树，计算成本与词汇表大小的对数成正比，而不是词汇表大小的线性函数，这个就叫做分级softmax分类器。

在实践中分级softmax分类器不会是一棵完美平衡的分类树，实际上，分级的softmax分类器会被构造成常用词在顶部，不常用的词像durian会在树的更深处，如上图编号2所示的分类树。因为更常见的词会更频繁，所以只需要少量检索就可以获得常用单词像the和of。然而更少见到的词，比如durian就更合适在树的较深处，因为一般不需要到那样的深处。有不同的经验法则可以帮助构造分类树形成分级softmax分类器，这是能在文献中见到的一个加速softmax分类的方法。

**对上下文$c$进行采样：**

一旦对上下文$c$进行采样，那么目标词$t$就会在上下文c的正负10个词距内进行采样。要如何选择上下文$c$？

一种选择是可以就对语料库均匀且随机地采样，如果这么做，会发现有一些词，像the、of、a、and、to诸如此类是出现得相当频繁的，所以上下文到目标词的映射会相当频繁地得到这些种类的词，像orange、apple或durian就不会那么频繁地出现了。这会导致需要花大部分的力气来更新这些频繁出现的单词的$e_{c}$，我们想要的是花时间来更新像durian这些更少出现的词的嵌入。

实际上词$p(c)$的分布并不是单纯的在训练集语料库上均匀且随机的采样得到的，而是采用了不同的分级来平衡更常见的词和不常见的词。

还有另一个版本的Word2Vec模型，叫做CBOW，即连续词袋模型（Continuous Bag-Of-Words Model），它获得中间词两边的的上下文，然后用周围的词去预测中间的词，这个模型也很有效，也有一些优点和缺点:

|                   CBOW                   |                Skip-Gram                 |
| :--------------------------------------: | :--------------------------------------: |
| ![](http://www.ai-start.com/dl2017/images/cbow.jpg) | ![](http://www.ai-start.com/dl2017/images/skipgram.jpg) |
|                对小型数据库比较合适                |                在大型语料中表现更好                |



### 7. 负采样（Negative Sampling）

Skip-Gram的缺点就在于softmax计算起来很慢。本节的负采样，与Skip-Gram模型做相似的事情，但是用了一个更加有效的学习算法。

这个算法中要做的是构造一个新的监督学习问题，给定一对单词，比如orange和juice，预测这是否为一对上下文词-目标词（context-target）。在这个例子中orange和juice就是个正样本，标记为1；那么orange和king就是个负样本，标为0。然后将其转化为一系列二分类问题使你可以非常有效的学习词向量。

![](http://www.ai-start.com/dl2017/images/54beb302688f6a298b63178534281575.png)

生成这些数据的方式是选择一个上下文词，再选一个目标词，就是表的第一行，它给了一个正样本。然后用相同的上下文词，再从字典中选取$K$个随机的词，并标记0，这些就会成为负样本。如果从字典中随机选到的词，正好出现在了词距内，也没关系。

关于$K$的选取，小数据集的话，$K$从5到20比较好。**==如果数据集很大，$K$就选的小一点==**。对于更大的数据集$K$就等于2到5，**==数据集越小$K$就越大==**。

为了定义模型，使用记号$c$表示上下文词，记号$t$表示可能的目标词，用$y$表示0和1，表示是否是一对上下文-目标词。要做的就是定义一个逻辑回归模型，给定输入的$c$，$t$对的条件下，$y=1$的概率，即：
$$
P\left( y = 1 \middle| c,t \right) = \sigma(\theta_{t}^{T}e_{c})
$$
这个模型基于逻辑回归模型，将一个sigmoid函数作用于$\theta_{t}^{T}e_{c}$，参数和之前一样，每一个可能的目标词有一个参数向量$\theta_{t}$和另一个参数向量$e_{c}$ 。如果有$K$个样本，可以把$1:K$的正负样本比例，即每一个正样本都有$K$个对应的负样本来训练一个类似逻辑回归的模型。

![](http://www.ai-start.com/dl2017/images/f36df292b7444e9b7379fa7c14626fa2.png)

算法步骤：

1. 输入one-hot向量$O_{6257}$，再传递给$E$

2. 通过两者相乘获得嵌入向量$e_{6257}$，然后得到10,000个可能的逻辑回归分类问题。**把这些看作10,000个二分类逻辑回归分类器**，但并不是每次迭代都训练全部10,000个，我们只训练其中的5个。

   要训练对应真正目标词那一个分类器，再训练4个随机选取的负样本，这就是$K=4$的情况。所以不使用一个巨大的10,000维度的softmax，因为计算成本很高，每次迭代要做的只是训练它们其中的5个。一般而言就是$K+1$个，其中$K$个负样本和1个正样本。这也是为什么这个算法计算成本更低，因为只需更新$K+1$个逻辑单元，相对而言每次迭代的成本比更新10,000维的softmax分类器成本低。

这个技巧就叫负采样，因为做的是，有一个正样本词，然后会特意生成一系列负样本，所以叫负采样，然后在每次迭代中你选择4个不同的随机的负样本词去训练算法。

**负样本选择：**

一个办法是根据其在语料中的经验频率进行采样，就是通过词出现的频率对其进行采样，但问题是这会导致在like、the、of、and诸如此类的词上有很高的频率。另一个极端就是用1除以词汇表总词数，即每个样本抽取到概率为$\frac{1}{\left|v\right|}$，均匀且随机地抽取负样本，这对于英文单词的分布是非常没有代表性的。所以论文的作者根据经验，既不用经验频率，也不用均匀分布，它位于这两个极端的采样方法之间，他们采用以下方式进行采样：
$$
P\left( w_{i} \right) = \frac{f\left( w_{i} \right)^{\frac{3}{4}}}{\sum_{j = 1}^{10,000}{f\left( w_{j} \right)^{\frac{3}{4}}}}
$$
$f(w_{i})$是观测到的在语料库中的某个英文词的词频，通过$\frac{3}{4}$次方的计算，使其处于完全独立的分布和训练集的观测分布两个极端之间，很多研究者现在使用这个方法，似乎也效果不错。

### 8. GloVe 词向量（GloVe Word Vectors）

另一个在NLP社区有着一定势头的算法是GloVe算法，这个算法并不如Word2Vec或是Skip-Gram模型用的多，但是也有人热衷于它。GloVe代表用词表示的全局变量（global vectors for word representation）。在此之前，我们曾通过挑选语料库中位置相近的两个词，列举出词对，即上下文和目标词，GloVe算法做的就是使其关系开始明确化。

假定$X_{{ij}}$是单词$i$在单词$j$上下文中出现的次数，那么这里$i$和$j$就和$t$和$c$的功能一样，所以可以认为$X_{{ij}}$等同于$X_{{tc}}$。也可以遍历训练集，然后数出单词$i$在不同单词$j$上下文中出现的个数，单词$t$在不同单词$c$的上下文中共出现多少次。

根据上下文和目标词的定义，大概会得出$X_{{ij}}$等于$X_{ji}$这个结论。事实上，如果将上下文和目标词的范围定义为出现于左右各10词以内的话，那么就会有一种对称关系。如果对上下文的选择是，上下文总是目标词前一个单词的话，那么$X_{{ij}}$和$X_{ji}$就不会像这样对称了。不过对于GloVe算法，可以定义上下文和目标词为任意两个位置相近的单词，假设是左右各10词的距离，那么$X_{{ij}}$就是一个能够获取单词$i$和单词$j$出现位置相近时或是彼此接近的频率的计数器。

GloVe模型做的就是进行优化，将他们之间的差距进行最小化处理：
$$
\text{mini}\text{mize}\sum_{i = 1}^{10,000}{\sum_{j = 1}^{10,000}{f\left( X_{{ij}} \right)\left( \theta_{i}^{T}e_{j} + b_{i} + b_{j}^{'} - logX_{{ij}} \right)^{2}}}
$$
其中$\theta_{i}^{T}e_{j}$，想一下$i$和$j$与$t$和$c$的功能一样，$\theta_{t}^{T}e_{c}$想要告诉的是这两个单词之间有多少联系，$t$和$c$之间有多紧密，$i$和$j$之间联系程度如何，换句话说就是他们同时出现的频率是多少。然后，要做的是解决参数$\theta$和$e$的问题，然后准备用梯度下降来最小化上面的公式。

现在一些附加的细节是如果$X_{{ij}}$是等于0的话，那么$log0$就是未定义的，是负无穷大的，所以想要对$X_{{ij}}$为0时进行求和，因此要做的就是添加一个额外的加权项$f\left(X_{{ij}}\right)$。如果$X_{{ij}}$等于0的话，同时会用一个约定，即$0log0= 0$，这个的意思是如果$X_{{ij}} =0$，先不要进行求和，所以这个$log0$项就是不相关项。

上面的求和公式表明，这个和仅是一个上下文和目标词关系里连续出现至少一次的词对的和。$f\left(X_{{ij}}\right)$的另一个作用是，有些词在英语里出现十分频繁，比如说this，is，of，a等等，有些情况，这叫做停止词，但是在频繁词和不常用词之间也会有一个连续统（continuum）。不过也有一些不常用的词，比如durion，想将其考虑在内，但又不像那些常用词这样频繁。因此，这个加权因子$f\left(X_{{ij}}\right)$就可以是一个函数，即使是像durion这样不常用的词，它也能给予大量有意义的运算，同时也能够给像this，is，of，a这样在英语里出现更频繁的词但不至于有过分的权重。因此有一些对加权函数f的选择有着启发性的原则，就是既不给这些词（this，is，of，a）过分的权重，也不给这些不常用词（durion）太小的权值。如果想要知道f是怎么能够启发性地完成这个功能的话，可以看一下GloVe算法论文。

最后，一件有关这个算法有趣的事是$\theta$和$e$现在是完全对称的，所以$\theta_{i}$和$e_{j}$就是对称的。如果只看数学公式的话，$\theta_{i}$和$e_{j}$的功能其实很相近，可以将它们颠倒或者将它们进行排序，实际上他们都输出了最佳结果。因此一种训练算法的方法是一致地初始化$\theta$和$e$，然后使用梯度下降来最小化输出，当每个词都处理完之后取平均值。所以，给定一个词$w$，就会有$e_{w}^{(final)}= \frac{e_{w} +\theta_{w}}{2}$。因为$\theta$和$e$在这个特定的公式里是对称的，而不像之前了解的模型，$\theta$和$e$功能不一样，因此也不能像那样取平均。

![](http://www.ai-start.com/dl2017/images/ec4b604d619dd617f14c2a34945c075d.png)

在上图表格中，第一行的嵌入向量是来表示Gender的，第二行是来表示Royal的，然后是是Age，在之后是Food等等。但是在使用我们了解过的一种算法来学习一个词嵌入时，会发生一件事，就是不能保证嵌入向量的独立组成部分是能够理解的，为什么呢？

假设说有个空间，里面的第一个轴（上图编号1所示）是Gender，第二个轴（上图编号2所示）是Royal，能够保证的是第一个嵌入向量对应的轴（上图编号3所示）是和第一和第二基轴有联系的。具体而言，这个学习算法会选择上图编号3所示作为第一维的轴，所以给定一些上下文词，第一维可能是上图编号3所示，第二维也许是上图编号4所示，或者它可能不是正交的，也可能是第二个非正交轴上图编号5所示，它可以是学习到的词嵌入中的第二部分。当我们看到$\theta_{i}^{T}e_{j}$的时候，如果有某个可逆矩阵$A$，那么可以将其简单地替换成$\left(A\theta_{i} \right)^{T}(A^{- T}e_{j})$，将其展开：
$$
\left( A\theta_{i} \right)^{T}\left( A^{- T}e_{j} \right) = \theta_{i}^{T}A^{T}A^{- T}e_{j} = \theta_{i}^{T}e_{j}
$$
不能保证这些用来表示特征的轴能够等同于人类可能简单理解的轴，具体而言，第一个特征可能是个Gender、Roya、Age、Food Cost和Size的组合，也许是名词或是一个行为动词和其他所有特征的组合，所以很难看出独立组成部分，然后解释出它的意思。尽管有这种类型的线性变换，平行四边形映射也说明了我们解决了这个问题，当在类比其他问题时，这个方法也是行得通的。因此尽管存在特征量潜在的任意线性变换，最终还是能学习出解决类似问题的平行四边形映射。

### 9. 情感分类（Sentiment Classification）

情感分类任务就是看一段文本，然后分辨这个人是否喜欢他们讨论的这个东西，这是NLP中最重要的模块之一，经常用在许多应用中。情感分类一个最大的挑战就是标记的训练集可能没有那么多，但是有了词嵌入，即使只有中等大小的标记的训练集，也能构建一个不错的情感分类器。

比如，输入$x$是一段文本，而输出$y$是要预测的相应情感，比如说是一个餐馆评价的星级。

情感分类一个最大的挑战就是可能标记的训练集没有那么多。对于情感分类任务来说，训练集大小从10,000到100,000个单词都很常见，甚至有时会小于10,000个单词，采用了词嵌入能够带来更好的效果，尤其是只有很小的训练集时。

![](http://www.ai-start.com/dl2017/images/ea844a0290e66d1c76a31e34b632dc0c.png)

假设有一个句子"dessert is excellent"，然后在词典里找这些词，要构建一个分类器能够把它映射成输出四个星。取这些词，找到相应的one-hot向量，乘以嵌入矩阵$E$，$E$可以从一个很大的文本集里学习到，比如它可以从一亿个词或者一百亿个词里学习嵌入，然后用来提取单词的嵌入向量，如$e_{8928}$。

如果在很大的训练集上训练$E$，比如一百亿的单词，就会获得很多知识，甚至从有些不常用的词中获取，然后应用到自己的问题上，即使自己的标记数据集里没有这些词。

然后取这些向量$e$，比如是300维度的向量，然后把它们求和或者求平均。上图编号3所示单元会得到一个300维的特征向量，把这个特征向量送进softmax分类器，然后输出$\hat y$。这个softmax能够输出5个可能结果的概率值，从一星到五星，这个就是5个可能输出的softmax结果用来预测$y$的值。

这里用的平均值运算单元算法适用于任何长短的评论，因为即使评论是100个词长，也可以对这一百个词的特征向量求和或者平均它们，然后得到一个表示一个300维的特征向量表示，然后把它送进softmax分类器，所以这个平均值运算效果不错。它实际上会把所有单词的意思给平均起来，或者把例子中所有单词的意思加起来就可以用了。

这个算法有一个问题就是没考虑词序，比如这样一个负面的评价，"Completely lacking in good taste, good service, and good ambiance."，有3个good。如果仅仅把所有单词的词嵌入加起来或者平均下来，最后的特征向量会有很多good的表示，分类器很可能认为这是一个好的评论，尽管事实上这是一个一星的差价。

![](http://www.ai-start.com/dl2017/images/de4b6513a8d1866bccf1fac3c0d0d6d2.png)

有一个更加复杂的模型，不用简单的把所有的词嵌入都加起来，是用一个RNN来做情感分类。

首先取这条评论，"Completely lacking in good taste, good service, and good ambiance."，找出每一个one-hot向量，用每一个one-hot向量乘以词嵌入矩阵$E$，得到词嵌入表达$e$，然后把它们送进RNN里。

RNN的工作就是在最后一步（上图编号1所示）计算一个特征表示，用来预测$\hat y$，这是一个多对一的网络结构的例子。有了这样的算法，考虑词的顺序效果就更好了，它就能意识到"things are lacking in good taste"，这是个负面的评价，“not good”也是一个负面的评价。而不像原来的算法一样，只是把所有的加在一起得到一个大的向量，根本意识不到“not good”和 “good”不是一个意思。

如果训练一个这样的算法，最后会得到一个很合适的情感分类的算法。由于词嵌入是在一个更大的数据集里训练的，这样效果会更好，更好的泛化一些没有见过的新的单词。比如其他人可能会说，"Completely absent of good taste, good service, and good ambiance."，即使absent这个词不在标记的训练集里，如果是在一亿或者一百亿单词集里训练词嵌入，它仍然可以正确判断，并且泛化的很好，甚至这些词是在训练集中用于训练词嵌入的，但是可以不在专门用来做情感分类问题的标记的训练集中。

### 10. 词嵌入除偏（Debiasing Word Embeddings）

现在机器学习和人工智能算法正渐渐地被信任用以辅助或是制定极其重要的决策，因此尽可能地确保它们不受非预期形式偏见影响，比如说性别歧视、种族歧视等等。

本节中当术语bias，指性别、种族、性取向方面的偏见，同时这也通常用于机器学习的学术讨论中。就是说一个已经完成学习的词嵌入可能会输出Man：Computer Programmer，同时输出Woman：Homemaker，那个结果看起来是错的，并且它执行了一个十分不良的性别歧视。如果算法输出的是Man：Computer Programmer，同时Woman：Computer Programmer这样子会更合理。同时也发现如果Father：Doctor，有些完成学习的词嵌入会输出Mother：Nurse。结果看起来是错的，并且它执行了一个十分不良的性别歧视。因此根据训练模型所使用的文本，词嵌入能够反映出性别、种族、年龄、性取向等其他方面的偏见，尽量修改学习算法来尽可能减少或是理想化消除这些非预期类型的偏见是十分重要的。

![](http://www.ai-start.com/dl2017/images/cf60f429ef532a2b3bbad3db98b054c5.png)

假设说我们已经完成一个词嵌入的学习，每个词所在位置如上图所示，首先要做的事就是辨别出想要减少或想要消除的特定偏见的趋势。为了便于说明，集中讨论性别歧视，不过这些想法对于其他类型的偏见都是通用的。辨别出偏见趋势，主要有以下三个步骤：

1. 对于性别歧视这种情况来说，能做的是$e_{\text{he}}-e_{\text{she}}$，因为它们的性别不同，然后将$e_{\text{male}}-e_{\text{female}}$，然后将这些值取平均，将这些差简单地求平均。上图编号3所示的趋势看起来就是性别趋势或说是偏见趋势，然后上图编号4所示的趋势与我们想要尝试处理的特定偏见并不相关，因此这就是个无偏见趋势。

   在这种情况下，偏见趋势可以将它看做1D子空间，所以无偏见趋势就会是299D的子空间。偏见趋势可以比1维更高，同时相比于取平均值，实际上它会用一个更加复杂的算法叫做SVU，也就是奇异值分解，如果对主成分分析（Principle  Component Analysis）很熟悉的话，奇异值分解算法的一些方法和主成分分析  (PCA)其实很类似。

2. 中和步骤，对于那些定义不确切的词可以将其处理一下，避免偏见。在更通常的情况下，希望像doctor或babysitter这些词成为种族中立的。

   ![](http://www.ai-start.com/dl2017/images/4102795b004ff090ed83dc654f585852.png)

   所以对于像doctor和babysitter这种单词就可以将它们在上图编号1所示的轴上进行处理，来减少或是消除他们的性别歧视趋势的成分，也就是说减少它们在这个水平方向上的距离（上图编号2方框内所示的投影），这就是第二个中和步。

3. 均衡步骤，对于grandmother和grandfather，或者是girl和boy，这些词嵌入，只希望性别是其区别。在这个例子中，babysitter和grandmother之间的距离或者说是相似度，实际上是小于babysitter和grandfather之间的。因此这可能会加重不良状态，或者可能是非预期的偏见，也就是说grandmothers相比于grandfathers最终更有可能输出babysitting。

   ![](http://www.ai-start.com/dl2017/images/9b27d865dff73a2f10abbdc1c7fc966b.png)

   所以在最后的均衡步中，想要确保的是像grandmother和grandfather这样的词都能够有一致的相似度，或者说是相等的距离，和babysitter或是doctor这样性别中立的词一样。这其中会有一些线性代数的步骤，但它主要做的就是将grandmother和grandfather移至与中间轴线等距的一对点上，现在性别歧视的影响也就是这两个词与babysitter的距离就完全相同了。

   所以总体来说，会有许多对像grandmother-grandfather，boy-girl，sorority-fraternity，girlhood-boyhood，sister-brother，niece-nephew，daughter-son这样的词对，通过均衡步来解决它们。

最后一个细节是怎样才能决定哪个词是中立的呢？论文作者做的就是训练一个分类器来尝试解决哪些词是有明确定义的，哪些词是性别确定的，哪些词不是。结果表明英语里大部分词在性别方面上是没有明确定义的，只有一小部分词像是grandmother-grandfather，girl-boy，sorority-fraternity等等，不是性别中立的。因此一个线性分类器能够告诉你哪些词可以通过中和步来预测这个偏见趋势，或将其与这个本质是299D的子空间进行处理。需要平衡的词对的数实际上是很小的，至少对于性别歧视这个例子来说，完整的算法比这里展示的更复杂一些。

均衡背后的关键思想是确保一对特定的单词与49维$g_\perp$距离相等 。均衡步骤还可以确保两个均衡步骤现在与$e_{receptionist}^{debiased}$ 距离相同，或者用其他方法进行均衡。下图演示了均衡算法的工作原理： 

![](http://www.ai-start.com/dl2017/images/equalize10.png)

主要步骤如下: 
$$
\begin{eqnarray*}
 \mu &=& \frac{e_{w1} + e_{w2}}{2}\tag{1} \\
\mu_{B} &=& \frac {\mu \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis} \tag{2} \\
 \mu_{\perp} &=& \mu - \mu_{B} \tag{3} \\
 e_{w1B} &=& \frac {e_{w1} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}  \tag{4} \\
 e_{w2B} &=&\frac {e_{w2} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis} \tag{5} \\
 e_{w1B}^{corrected} &=& \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w1B}} - \mu_B} {|(e_{w1} - \mu_{\perp}) - \mu_B)|} \tag{6} \\
 e_{w2B}^{corrected} &=& \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w2B}} - \mu_B} {|(e_{w2} - \mu_{\perp}) - \mu_B)|} \tag{7} \\
e_1 &=& e_{w1B}^{corrected} + \mu_{\perp} \tag{8} \\
e_2 &=& e_{w2B}^{corrected} + \mu_{\perp} \tag{9}
\end{eqnarray*}
$$
总结一下，减少或者是消除学习算法中的偏见问题是个十分重要的问题，因为这些算法会用来辅助制定越来越多的社会中的重要决策，在本节分享了一套如何尝试处理偏见问题的办法，不过这仍是一个许多学者正在进行主要研究的领域。