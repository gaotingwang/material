[TOC]

## 一、环境搭建、基础知识

### 1. python虚拟环境

国内可以使用python豆瓣源镜像进行加速，镜像地址：[https://pypi.douban.com/simple/](https://pypi.douban.com/simple/)

```shell
$ sudo pip install -i https://pypi.douban.com/simple/ django
```

虚拟环境好处：开发环境相互隔离

```shell
# 安装virtualenv
$ sudo pip install virtualenv

# 在当前目录下创建scrapy-spider的虚拟环境
$ virtualenv scrapy-spider # 默认使用Python
# 使用Python3环境, 通过 -p 指定Python3的安装目录
$ virtualenv scrapy-spider3 -p /Library/Frameworks/Python.framework/Versions/3.6/bin/python3

# 进入虚拟环境目录
$ cd scrapy-spider/bin

# 激活环境(退出使用deactivate)
$ source activate

```

每次进入虚拟环境都需要找到`scrapy-spider`虚拟环境的目录，然后执行`$source activate`，非常麻烦，所以可以使用虚拟环境管理`virtualenvwrapper`

```shell
# 安装虚拟环境管理
$ pip install virtualenvwrapper

# 需要在环境变量中指定WORKON_HOME
# 如在~/.zshrc 中指定export PATH=$PATH:WORKON_HOME = /Users/xxx/python-env
# 然后source virtualenvwrapper的.sh文件路径
$ source ~/.zshrc

# 通过环境管理器在/Users/xxx/python-env目录下安装虚拟环境
$ mkvirtualenv py2 # 创建完成后自动进入环境
# 安装Python3
$ mkvirtualenv --python=/Library/Frameworks/Python.framework/Versions/3.6/bin/python3 py3

# 列出虚拟环境
$ workon

# 下次进入指定的虚拟环境
$ workon py2
```

通过`pip install scrapy`安装scrapy部分包失败时，可以访问[https://www.lfd.uci.edu/~gohlke/#pythonlibs](https://www.lfd.uci.edu/~gohlke/#pythonlibs)搜索失败的包，手动下载下来，通过`pip install lxml.whl`进行本地重新安装。

scrapy 是框架，requests和beautifulsoup是库。scrapy基于twisted，而twisted是异步I/O，所以性能优势大。

### 2. 正则表达式

- 开头结尾限定：`^`以后面指定字符开头 ，`$`以前面指定字符结尾

- 前面的字符次数限定：` *`前面字符出现任意多次，`+`前面字符至少出现一次；  `{2}`前面字符出现2次， `{2,}`出现两次及以上， `{2,5}`出现2到5次；`?`表示0次或1次。

- 贪婪与懒惰：`a.*b`将会匹配**最长的**以a开始，以b结束的字符串，称为贪婪匹配。当需要尽可能少的匹配，可以使用限定符号，如：`*?`重复任意次，但尽可能少重复；`+?`重复1次或更多次，但尽可能少重复；`??`重复0次或1次，但尽可能少重复；`{n,m}?`重复n到m次，但尽可能少重复。

- `.`当前位置匹配任意字符（除换行符）， `|` 表示或的关系，优先匹配左边。

- 当字符放在`[]`中表示当前位置字符满足任意其中一个即可；也可以使用范围如` [a-z] `或`[0-9]`；还有`[^1]`表示非，当前字符不是1就行；当特殊字符如`*.?`等放入`[]`中，不再表示正则中的特殊字符，就指当前字符。

- `\s`匹配当前位置为任意的空白符（包括空格，制表符(Tab)，换行符，中文全角空格等），` \S`为取反的意思。

  ` \w`匹配字母或数字或下划线或汉字（即`[a-zA-Z0-9_]`）等，同理` \W`表示取反。

- `[\u4E00-\u9FA5]`表示汉字，`\d`表示数字

- `()`表示要提取的子字符串，第一个括号对应`group(1)`，第二个`group(2)`

### 3. 深度优先和广度优先

- **深度优先：**

  从某个顶点v出发，依次从它的未被访问的邻接点出发深度优先搜索遍历图，直至图中所有和v有路径相通的顶点都被访问到。 若此时尚有其他顶点未被访问到，则另选一个未被访问的顶点作起始点，重复上述过程，直至图中所有顶点都被访问到为止。

  深度优先搜索是一个**递归的过程**：

  ```python
  def depth_tree(node_root):
      if node_root is not None:
          print(node_root.data) # 输出当前节点
          if node_root.left is not None: # 先递归左节点
              return depth_tree(node_root.left)
          if node_root.right is not None: # 后遍历右节点
              return depth_tree(node_root.right)
  ```

  

- **广度优先：**

  从顶点v出发，**依次访问v的各个未曾访问过的邻接点**，然后分别从这些邻接点出发依次访问它们的邻接点，并使得“先被访问的顶点的邻接点先于后被访问的顶点的邻接点被访问。

  广度优先是通过**队列**形式实现的：

  ```python
  def level_queen(node_root):
      if node_root is None:
          return
  
      queens = [node_root]
      while queens:
          node = queens.pop(0)
          print(node.data)
          # 将左右子节点依次放入队列中
          if node.left is not None:
              queens.append(node.left)
          if node.right is not None:
              queens.append(node.right)
  ```

  

### 4. Python中字符编码

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/python/%E7%BC%96%E7%A0%811.jpg" height="250px">

在计算机内存中，是统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。比如用记事本编辑的时候，从文件读取的UTF-8字符被转换为Unicode字符到内存里，编辑完成后，保存的时候再把Unicode转换为UTF-8保存到文件中。

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/python/%E7%BC%96%E7%A0%812.png" height="220px">

浏览网页的时候，服务器会把动态生成的Unicode内容转换为UTF-8再传输到浏览器，所以可以看到很多网页的源码上会有类似`<meta charset="UTF-8" />`的信息，表示该网页正是用的UTF-8编码。

因为Python的诞生比Unicode标准发布的时间还要早，所以最早的Python只支持ASCII编码，普通的字符串`'ABC'`在**早期Python内部都是ASCII编码的**。

- Python 2.x版本虽然支持Unicode，但在语法上需要`'xxx'`（ASCII形式）和`u'xxx'`（Unicode形式）两种字符串表示方式。
- ==Python 3.x版本中，把`'xxx'`和`u'xxx'`都统一成Unicode编码==，即写不写前缀`u`都是一样的，而以字节形式表示的字符串则必须加上`b`前缀：`b'xxx'`。

`decode()`是将字符串按照指定编码如gbk，解码成Unicode字符串。如果不指定，则使用`sys.getdefaultencoding() `指定的默认编码进行解码。

`encode()`时必须保证字符串是一个Unicode字符串，然后把Unicode形式的字符串按照指定编码如UTF-8，编码成指定编码字符串。Python 2中如果是非Unicode字符串，即ASCII形式，会先按`sys.getdefaultencoding() `指定的编码即ASCII进行`decode()`，然后再进行`encode()`。Python 3中因为就是Unicode形式，所以不存在`decode()`。

```python
# python2 中对str直接进行encode, 会先进行decode("ascii")转换为Unicode
"中文".encode("utf-8")

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe4 in position 0: ordinal not in range(128)
    

# 因为我们编辑不是采用ascii进行编码的，正确形式：手动指定自己的decode编码方式
"中文".decode("gbk").encode("utf-8")

'\xe4\xb8\xad\xe6\x96\x87'


# python3 中，因为str就是Unicode形式，可以直接encode, python3 中str也不提供decode()方法
"中文".encode("utf-8")

b'\xe4\xb8\xad\xe6\x96\x87'
```

由于Python源代码也是一个文本文件，当源代码中包含中文的时候，在保存源代码时，就需要务必指定保存为UTF-8编码。当Python解释器读取源代码时，为了让它按UTF-8编码读取，通常在文件开头写上这两行：

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
```

第一行注释是为了告诉Linux/OS X系统，这是一个Python可执行程序，Windows系统会忽略这个注释；
第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则源代码中写的中文输出可能会有乱码。

**申明了UTF-8编码并不意味着`.py`文件就是UTF-8编码的，只是指定了Python解释器以UTF-8编码读取源代码**。

### 5. url去重常见策略

scrapy使用的是url经过md5加密等方法hash后放入set中(set具有唯一性，有去重功能)，md5后长度还是太长，占用空间大。

优化：使用bitmap方法，将url通过hash映射到8个bit中的某一位，即一个url只占1bit，此方法冲突较高。bloomfilter方法对bitmap进行了改进，多重hash函数降低冲突。

1kb = 1024byte , 1byte = 8 bit

### 6. XPath

XPath（XML Path Language）即为XML路径语言，它是一种用来确定XML文档中某部分位置的语言。XPath基于XML的树状结构，提供在数据结构树中找寻节点的能力。

节点是通过沿着路径或者 step 来选取的，具体路径选取语法：

| 表达式          | 描述                                                        |
| --------------- | ----------------------------------------------------------- |
| nodename        | 选取此节点的所有子节点。                                    |
| /bookstore      | 选取根元素 bookstore， 则此路径始终代表到某元素的绝对路径   |
| bookstore/book  | 选取属于 bookstore 的所有book子元素，不包括孙子节点。       |
| //book          | 选取所有 book 子元素，不管它们在文档中的位置。              |
| bookstore//book | 选择属于 bookstore 元素的所有 book后代 元素，包括孙子节点。 |
| //@class        | 选取属性为 class 的所有元素。                               |

谓语用来查找某个特定的节点或者包含某个指定的值的节点，谓语被嵌在方括号中：

在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果：

| 路径                          | 结果                                                         |
| ----------------------------- | ------------------------------------------------------------ |
| /bookstore/book[1]            | 选取属于 bookstore 子元素的第一个 book 元素。                |
| /bookstore/book[last()]       | 选取属于 bookstore 子元素的最后一个 book 元素。              |
| /bookstore/book[last()-1]     | 选取属于 bookstore 子元素的倒数第二个 book 元素。            |
| /bookstore/book[position()<3] | 选取最前面的两个book 子元素。                                |
| //title[@lang]                | 选取所有拥有 lang 的属性的 title 元素。                      |
| //title[@lang='eng']          | 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。   |
| /bookstore/book[price>35.00]  | 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。 |

XPath 还可以使用通配符来选取未知的 XML 元素。

| 通配符            | 描述                      |
| ----------------- | ------------------------- |
| *                 | 匹配任何元素节点。        |
| @*                | 匹配任何属性节点。        |
| h1/text()         | 匹配h1标签里的文本值。    |
| //div/a 丨//div/p | 选取所有div元素的a和p元素 |

有时候自己写的XPath明明对的，却获取不到数据，是因为**F12产生的源码（是js加载完后的代码），不同于网页源代码**，==response.xpath()是根据网页源代码来提取信息的==。

```python
# xpath可以有多种多样的写法：

# 第一种很容易出错，因为其中div很可能是js生成添加进去的
re_selector = response.xpath("/html/body/div[1]/div[3]/div[1]/div[1]/h1/text()")
# 后面两种是通过id和class进行定位
re2_selector = response.xpath('//*[@id="post-110287"]/div[1]/h1/text()')
re3_selector = response.xpath('//div[@class="entry-header"]/h1/text()')
```

XPath还有内置函数，如`contains()`:

```python
# //span[@class='eng']是class只能为eng
# //span[contains(@class, 'bookmark-btn')]是class中值可以有多个，只要包含指定的就可以
response.xpath("//span[contains(@class, 'bookmark-btn')]/text()").extract()[0]
```



### 7. CSS选择器

| 表达式          | 说明                                                         |
| --------------- | ------------------------------------------------------------ |
| *               | 选择所有元素。                                               |
| \#firstname     | 选择 id="firstname" 的所有元素。                             |
| .intro          | 选择 class="intro" 的所有元素。                              |
| div p           | 选择 \<div\> 元素内部的所有 \<p\> 元素，包括儿子孙子。       |
| div>p           | 选择父元素为 \<div> 元素的所有 \<p> 元素，只有儿子。         |
| div+p           | 选择紧接在 \<div> 元素之后的 \<p> 元素，兄弟关系且紧挨着。   |
| p~ul            | 选择前面有 \<p> 元素的每个 \<ul> 元素，兄弟但可以不挨着。    |
| [target]        | 选择带有 target 属性所有元素<br/> 如a[src*="abc"]，选择src属性中包含 "abc" 子串的 \<a> 元素。 |
| :not(p)         | 选择非 \<p> 元素的每个元素。                                 |
| p:nth-child(3)  | 选择是第三个子元素的\<p> 元素。                              |
| p:nth-child(2n) | 选择第偶数个\<p> 元素                                        |

CSS还有伪类选择器：通过`::`来使用，如获取p元素内的文本值`p::text`，a元素href属性的值`a::attr(href)`

## 二、爬取真实数据

###Scrapy项目构建

- 通过`workon scrapy-spider3 `进入到`scrapy-spider3`的虚拟环境，然后`cd`到工作目录

- 创建工程框架：

  ```shell
  $ scrapy startproject Spider
  
  # 会在指定的工作目下创建Spider项目，使用的默认模板在虚拟环境目录下的scrapy/templates/包中
  New Scrapy project 'Spider', using template directory 'env/scrapy-spider3/lib/python3.6/site-packages/scrapy/templates/project', created in:/Users/xxx/Documents/Pycharm/Spider
  
  You can start your first spider with:
      cd Spider
      scrapy genspider example example.com
  ```

- 创建具体网站的爬虫：

  ```shell
  $ cd Spider
  # 通过scrapy genspider指定要爬的具体网站的名称和网站的地址
  $ scrapy genspider jobbole blog.jobbole.com
  
  # 之后会在项目的Spider.spiders目录下生成jobbole.py文件
  Created spider 'jobbole' using template 'basic' in module:
    Spider.spiders.jobbole
  ```

- 执行具体网站的爬虫:

  ```shell
  $ scrapy crawl jobbole
  ```

- scrapy也提供了对具体的网站地址的debug

  ```shell
  # 执行后会有一些scrapy对象可以直接使用，比较关心的是response对象
  $ scrapy shell http://blog.jobbole.com/110287/
  
  >>> title = response.xpath('//div[@class="entry-header"]/h1/text()')
  
  >>> title.extract()[0]
  ['2016 腾讯软件开发面试题（部分）']
  
  # 带有请求头的请求地址
  $ scrapy shell -s User-Agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)Chrome/63.0.3239.84 Safari/537.36' http://blog.jobbole.com/110287/
  ```

###Scrapy使用说明

Scrapy中的`Request`的`callback`使用了Twisted作为框架，Twisted有些特殊的地方是它是事件驱动的，并且比较适合异步的代码。在任何情况下，都不要写阻塞的代码。阻塞的代码包括:

- 访问文件、数据库或者Web
- 产生新的进程并需要处理新进程的输出，如运行shell命令
- 执行系统层次操作的代码，如等待系统队列

数据爬取的任务就是从非结构的数据中提取出结构性的数据, Item可以让我们自定义需要的数据结构，`itme.py`中可以定义了不同的要放入管道中的`Item`类。

所以一个具体的spider去爬取页面数据，将得到的结构化数据放入到`Item`中，之后scrapy会将这些`Item`放入`pipeline`中（不直接对Item数据进行处理，是为了防止阻塞）。最后将`pipeline`管道中的数据获取到，可以对这些数据做想要的任何操作，如写入数据库。

`pipelines.py`中定义不同的管道对相应`Item`对象的处理方式，如写入数据库。

在`scrapy.pipelines.images`中定义了关于文本、图像`ImagesPipeline`、多媒体`MediaPipeline`处理的管道。如果需要对图片处理，需要依赖`pillow`:

```shell
$ pip install pillow
```

对mysql的操作需要依赖`mysqlclient`

```shell
$ pip install mysqlclient
```

`setting.py`中可以通过`ITEM_PIPELINES`配置对`Item`的不同管道进行设置

### Itemloader机制

ItemLoader提供了一个容器，可以配置某一个字段该使用哪种规则，简化代码便于以后的维护。ItemLoader主要方法：`add_css`，`add_xpath`直接从选择器中获取Item的字段值，有些值不是通过选择器获得的，可以通过` add_value` 方法进行添加。最后通过`item = itemloader.load_item()`加载出需要的item。

通过选择器获取到的值都是数组形式的，如果Item的大部分字段都是取数组第一个值，可以定制自己的ItemLoader：

```python
# 自定义itemloader实现默认提取第一个
class ItemFirstValueLoader(ItemLoader):
    default_output_processor = TakeFirst()
```

如果对Item的个别字段需要做特殊处理：

```python
class JobBoleItem(scrapy.Item):
    title = scrapy.Field()
    create_date = scrapy.Field(
        # input_processor是针对数组中的每个值
        # MapCompose是对值依次调用传入的方法
        input_processor=MapCompose(lambda value:"2018/" + value, date_convert)
    )
    front_image_url = scrapy.Field(
        # 图片处理的管道是按照数组进行处理的，所以对image不使用自定义中的output_processor
        output_processor=MapCompose(lambda value:value)
    )
    comment_nums = scrapy.Field(
        input_processor=MapCompose(get_nums)
    )
    tags = scrapy.Field(
        input_processor=MapCompose(remove_comment_tags),
        # 多标签也不是使用自定义中的output_processor，采用对数组中的值按“,”连接
        output_processor=Join(",")
    )
```

###Crawel模板

```shell
# 可以查看到所有可用模板
$ scrapy genspider --list 
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

# 通过-t参数选用模板，默认是basic
$ scrapy genspider -t crawl lagou www.lagou.com

```

crawl模板提供了一些可以让我们进行简单follow的规则（rule），link，迭代爬取：

```python
class LagouSpider(CrawlSpider):
    name = 'lagou'
    allowed_domains = ['www.lagou.com']
    start_urls = ['http://www.lagou.com/']

    rules = (
        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        i = {}
        #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()
        #i['name'] = response.xpath('//div[@id="name"]').extract()
        #i['description'] = response.xpath('//div[@id="description"]').extract()
        return i

```

在crawl模板初始化时就会调用`_compile_rules`：

```python
def _compile_rules(self):
    def get_method(method):
        if callable(method):
            return method
        elif isinstance(method, six.string_types):
            return getattr(self, method, None)

    # 将rules进行一个copy
    self._rules = [copy.copy(r) for r in self.rules]
    # 调用get_method方法，为rule对象定义具体的callback、process_links、process_request方法
    for rule in self._rules:
        rule.callback = get_method(rule.callback)
        rule.process_links = get_method(rule.process_links)
        rule.process_request = get_method(rule.process_request)
```

crawl模板与basic模板不同的是：basic模板开始函数是`start_requests()`，默认返回处理函数`parse()`；crawl模板`parse()`会调用`_parse_response()`，而`_parse_response()`依赖于`parse_start_url()`和`process_results()`，所以只需要覆写`parse_start_url()`和`process_results()`就可以，不用重写`parse()`。

```python
def _parse_response(self, response, callback, cb_kwargs, follow=True):
    if callback:
        # callback来自与parse_start_url()，所以需要重写parse_start_url()
        cb_res = callback(response, **cb_kwargs) or ()
        # process_results()需要重写
        cb_res = self.process_results(response, cb_res)
        for requests_or_item in iterate_spider_output(cb_res):
            yield requests_or_item

    if follow and self._follow_links:
        for request_or_item in self._requests_to_follow(response):
            yield request_or_item
```

最后`_parse_response()`会调用`_requests_to_follow()`方法执行`rules`中的`LinkExtractor`去完成请求。

```python
def _requests_to_follow(self, response):
        if not isinstance(response, HtmlResponse):
            return
        seen = set()
        # 通过enumerate把self._rules变成一个可迭代对象
        for n, rule in enumerate(self._rules):
            # 通过link_extractor.extract_links抽取出具体的link
            links = [lnk for lnk in rule.link_extractor.extract_links(response)
                     if lnk not in seen]
            if links and rule.process_links:
                # 执行process_links
                links = rule.process_links(links)
            for link in links:
                seen.add(link)
                # 根据link发起Request,回调self._response_downloaded函数,又会执行self._parse_respose
                r = self._build_request(n, link)
                yield rule.process_request(r)
```

关于`Rule`参数说明：

- LinkExtractor：定义从页面爬取链接对象
- follow：指定从response提取的链接是否需要跟进。 如果 callback 为None， follow 默认设置为 True ，否则默认为 False 。
- process_links：是一个callable或string(该spider中同名的函数将会被调用)。 从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤链接。
- process_request：是一个callable或string(该spider中同名的函数将会被调用)。 该规则提取到每个request时都会调用该函数。该函数必须返回一个request或者None， 用来过滤request。

关于`LinkExtractor`参数说明：

- allow ：符合这个url就爬取
- deny : 符合这个url规则就放弃
- allow_domin : 这个域名下的才处理
- deny_domains : 这个域名下的不处理
- restrict_xpaths：页面中有多块区域可以获取到内容，进一步限定xpath获取位置

应用举例：

```python
# Spider一般可用于比较单一的请求地址内容爬取，如仅对/a/xxx 下的内容获取
# CrawlSpider使用于多个不同请求地址的爬取，如：/a，/b/xx，/c
class LagouSpider(CrawlSpider):
    name = 'lagou'
    allowed_domains = ['www.lagou.com']
    start_urls = ['http://www.lagou.com/']

    # 对满足rules的url进行爬取
    rules = (   
        Rule(LinkExtractor(allow=("zhaopin/.*",)), follow=True), # follow=True对当前页面进一步爬取
        # 因为没有指定callback函数，走默认的parse()调用_parse_response()方法
        # 又因为没有实现parse_start_url()和process_results()，所以对页面结果也不会进行解析
        # 最后_parse_response()会调用_requests_to_follow()，对页面中满足rules的url继续爬取
        Rule(LinkExtractor(allow=("gongsi/j\d+.html",)), follow=True),
        # 爬取到/jobs/页面结果后，执行parse_job()回调函数
        Rule(LinkExtractor(allow=r'jobs/\d+.html'), callback='parse_job', follow=True),
    )

    def parse_job(self, response):
    	pass
```



## 三、scrapy进阶

### scrapy突破反扒技术

![](https://gtw.oss-cn-shanghai.aliyuncs.com/python/%E7%88%AC%E8%99%AB%E4%B8%8E%E5%8F%8D%E7%88%AC%E8%99%AB.png)

反爬虫的目的在于：一些初级爬虫简单粗暴，容易打垮服务器；或者忘记或无法关闭的失控爬虫，长时间爬取；或者是基于数据保护或商业竞争。



### scrapy中间件开发

![](https://gtw.oss-cn-shanghai.aliyuncs.com/python/Scrapy%E6%9E%B6%E6%9E%84%E5%9B%BE.png)

1. spider一般会yield一个request发送给engine 
2. engine的`schedule(request, spider)`方法，通过调用`scheduler.enqueue_request(request)`方法将request放入scheduler中
3. 之后engine会从scheduler队列中通过`_next_request_from_scheduler(spider)` 获取一个request
4. engine拿到request之后，通过downloadermiddleware 给downloader
5. downloader通过request下载页面完成后，再经由downloadermiddleware发送response回来给engine
6. engine拿到response之后返回给spider
7. spider进行处理，解析出item & request
8. item--->itempipeline；request--->跳转步骤二



#### 1. user-agent随机切换

在`scrapy\downloadermiddlewares\useragent.py`目录下，scrapy默认提供了一个useragent的downloadermiddleware，可以模仿useragent写自己的随机切换user-agent：

```python
class RandomUserAgentMiddleware(object):
    """
        随机更换user-agent
    """
    def __init__(self, crawler):
        super(RandomUserAgentMiddleware, self).__init__()
        self.ua = UserAgent()
        self.ua_type = crawler.settings.get("RANDOM_UA_TYPE", "random")

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)

    # 进入downloader之前，先对request进行包装预处理，处理成目标网站想要到request
    def process_request(self, request, spider):
        """
        对request进行设置随机user-agent
        """
        def get_ua():
            # 通过getattr()获取ua对象的random属性的值，即相当于调用ua.random方法
            return getattr(self.ua, self.ua_type)

        request.headers.setdefault('User-Agent', get_ua())
    

# 之后在setting.py文件中设定DOWNLOADER_MIDDLEWARES
DOWNLOADER_MIDDLEWARES = {
   'GtwSpiders.middlewares.RandomUserAgentMiddleware':555,
}    
```

#### 2. 动态IP代理

爬取西刺代理，创建代理池保存到数据库，之后从数据库获取代理IP，在downloadermiddleware中为request设置代理地址：

```python
def process_request(self, request, spider):
    get_ip = GetIP()
    # 设置随机代理ip
    request.meta["proxy"] = get_ip.get_random_ip()
```

也可以参考使用[scrapy_proxies](https://github.com/aivarsk/scrapy-proxies)创建ip代理池。
不介意收费，可以直接使用[scrapy-crawlera](https://github.com/scrapy-plugins/scrapy-crawlera)；如果可以使用vpn翻墙，也可以考虑使用[Tor洋葱网络](https://www.torproject.org/)。

#### 3. Cookie的禁用&设置下载速度

禁用cookie后，服务器就无法进行追踪，但有些网站需要cookie打开：

```python
# setting.py
COOKIES_ENABLED = False # 禁用cookie

AUTOTHROTTLE_START_DELAY = 5 # 下载器在下载同一网站的下个页面的等待时间
```

给不同的spider设置自己的setting值：

```python
class LagouSpider(CrawlSpider):
    custom_settings = {
        "COOKIES_ENABLED": True
    }
```

#### 4. 图片验证码

可以找一些打码平台，去调用相关接口识别

####5. 动态网站抓取处理

`pip install selenium`安装selenium插件，集成selenium和phantomjs到scrapy中，需要重新定义一个模拟浏览器request的DownloaderMiddleware：

```python
class JSPageMiddleware(object):
    """
    通过chrome请求动态网页
    """

    # 通过chrome请求动态网页
    def process_request(self, request, spider):
        # 也可以对request.url做正则匹配，对一类url进行浏览器访问
        if spider.name == "jobbole":
            # 从spider对象中获取browser对象
            spider.browser.get(request.url)
            import time
            time.sleep(3)
            print ("访问:{0}".format(request.url))

            # 已经通过浏览器获取到页面结果，所以不需要再经过下载器downloader
            # 遇到Response, scrapy不会再向downloader发送请求
            return HtmlResponse(url=spider.browser.current_url,
                                body=spider.browser.page_source,
                                encoding="utf-8",
                                request=request)
```

如果想使用无界面浏览器，需要`pip install pyvirtualdisplay`，之后：

```python
from pyvirtualdisplay import Display
display = Display(visible=0, size=(800, 600))
display.start()

browser = webdriver.Chrome()
browser.get("url")
```

也有一些其他的模拟js请求加载页面的方式：

- `scrapy-splash`: 支持分布式，稳定性不如chorme
- `selenium grid`:支持分布式
- `splinter` 

#### 6. scrapy暂停重启

```shell
# 要想做到暂停，之后能从暂停处重启，需要将scrapy的状态保存下来
# 不同的spider不能共用一个目录
# 同一spider在不同run的情况下也不能共用一个目录
$ scrapy crawl jobble -s JOBDIR=job_info/001 # JOBDIR指定状态保存目录
```

#### 7. 数据收集

Scrapy提供了方便的收集数据的机制。数据以key/value方式存储，**值大多是数值** ，该机制叫做数据收集器([Stats Collector](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/stats.html))。

```python
# httperror.py中通过handle_httpstatus_list为每个spider指定可以访问的错误页面状态码
# 否则默认只可以访问200~300的页面
handle_httpstatus_list = [404]

def parse(self, response):

    # 数据收集器stats是放在crawler中
    if response.status == 404:
    	self.crawler.stats.inc_value("not_found_page") # 次数+1
```

#### 8. Extensions

扩展框架提供一个机制，使得能将自定义功能绑定到Scrapy。扩展只是正常的类，它们在Scrapy启动时被实例化、初始化。Scrapy扩展(包括middlewares和pipelines)的主要入口是 `from_crawler` 类方法， 它接收一个 `Crawler`类的实例，该实例是控制Scrapy crawler的主要对象。 如果扩展需要，可以通过`crawler`这个对象访问settings，signals，stats，控制爬虫的行为。扩展需要关联到 [signals](https://scrapy-chs.readthedocs.io/zh_CN/latest/topics/signals.html#topics-signals) 并执行它们触发的任务。

```python
class SpiderOpenCloseLogging(object):

    def __init__(self, item_count):
        ...

    @classmethod
    def from_crawler(cls, crawler):
        ...
        # instantiate the extension object
        ext = cls(item_count)

        # connect the extension object to signals
        # 扩展程序相关执行，主要是与信号量signal进行绑定对应的执行方法
        crawler.signals.connect(ext.my_spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(ext.my_spider_closed, signal=signals.spider_closed)

        # return the extension object
        return ext
```

scrapy log配置

email发送

## 四、scrapy redis分布式爬虫

在scrapy中scheduler是运行在队列的，而队列是在单机内存中的，服务器上爬虫是无法利用内存的队列做任何处理，所以scrapy不支持分布式。

分布式需要解决的问题：

- requests队列集中管理
- 去重集中管理

参考[scrapy-redis](https://github.com/rmax/scrapy-redis) ，首先通过`pip install redis`初始化redis，之后必须在setting.py文件中设置：

```python
# 将存储request队列的scheduler放在redis中
SCHEDULER = "scrapy_redis.scheduler.Scheduler"

# 通过redis共享去重filter
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"

# 将Item序列化放入redis中
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 300
}
```

之后自己的spider需要继承RedisSpider

```python
from scrapy_redis.spiders import RedisSpider

class MySpider(RedisSpider):
    name = 'myspider'
    # 也可以自己指定start_url的redis key
    redis_key = 'myspider:start_urls'

    def parse(self, response):
        # do stuff
        pass
```

初始化start_urls也全部放在redis中，所以需要向redis中放入start_url:

```shell
$ redis-cli lpush myspider:start_urls http://google.com
```

启动spider时，不再是之前的本地scheduler完成的，而是由scrapy-redis提供的scheduler，所以启动命令：

```shell
$ scrapy runspider myspider.py
```

所有yield出去的request都会被scrapy_redis\scheduler.py
处理以及重写的enqueue_request接收

## 五、elasticsearch django实现搜索引擎

安装es依赖：

```shell
$ pip install elasticsearch-dsl
```

通过pipeline将数据写入es中：

```python
class LagouType(Document):
    suggest = Completion(analyzer=ik_analyzer)
    title = Text(analyzer="ik_max_word")
    url = Keyword()
    work_years_min = Integer()
    publish_time = Date()
    job_advantage = Text(analyzer="ik_max_word")

    # 定义了es中对应的index
    class Index:
        name = 'lagou'
        doc_type = "job"

    class Meta:
        doc_type = "job"

class ElasticSearchPipeline(object):
    # 将线数据写入到es中
    def process_item(self, item, spider):
        job = LagouType()
        job.meta.id = self["url_object_id"]
        job.title = self["title"]
        job.url = self["url"] 
        ...
        
        job.save()
        return item
```

直接使用elasticsearch底层接口查询：

```python
# 获取elasticsearch的查询接口
client = Elasticsearch(hosts=["localhost"])
response = client.search(index="jobbole",
                         request_timeout=60,
                         body={
                             "query": {
                                 "multi_match": {
                                     "query": key_words,
                                     "fields": ["tags", "title", "content"]
                                 }
                             },
                             "from": (page - 1) * 10,
                             "size": 10,
                             "highlight": {
                                 "pre_tags": ['<span class="keyWord">'],
                                 "post_tags": ['</span>'],
                                 "fields": {
                                     "title": {},
                                     "content": {},
                                 }
                             }
                         })
```

## 六、项目部署

[scrapyd](https://github.com/scrapy/scrapyd) 是运行scrapy-spider的服务，可以部署scrapy项目，并通过httpjson的API来控制爬虫

```shell
# 在scrapy的环境下安装scrapyd
$ pip install scrapyd

# 在虚拟环境/scripts目录下会有一个scrapyd.exe
```

虚拟环境下进入一个指定目录空间如：/users/xxx/spider/scrapyd/ , 运行scrapyd命令,相当于启动了scrapy-spider的服务端，http://127.0.0.1:6800

之后需要scrapyd-client，将本地spider打包发送到服务端：

```shell
# 安装客户端，打包scrapy项目
$ pip install scrapyd-client # 项目打包，可以不依赖scrapy环境

# 会在环境的/env/scrapy-spider3/lib/python3.6/site-packages/scrapyd-client目录下，
# 生成一个scrapyd-deploy文件
```

scrapy项目的`scrapy.cfg`中的`[deploy]`，就是为scrapyd服务的，来配置相关的部署配置。可以配置`[deploy:gtw]` 来指定部署项目的名称。

```python
[deploy:gtw]
url = http://localhost:6800/ # 指定部署服务的地址URL
project = GtwSpiders # 指定项目名称
```

通过`scrapyd-deploy`命令，将项目上传到服务上：

```shell
$ scrapy list # 查看爬虫列表

# 运行scrapyd-deploy命令，将项目上传到服务上
# -p 要和配置文件中project保持一致
$ scrapyd-deploy gtw -p GtwSpiders #打包上传

# 会在/users/xxx/spider/scrapyd/eggs目录下看到上传的项目
```

运行指定爬虫：

```shell
$ curl http://localhost:6800/schedule.json -d project=GtwSpiders -d spider=jobbole
```

取消爬虫运行：

```shell
$ curl http://localhost:6800/cancel.json -d project=myproject -d job=6487ec79947edab326d6db28a2d86511e8247444
```

删除爬虫项目：

```shell
$ curl http://localhost:6800/delproject.json -d project=GtwSpiders
```



