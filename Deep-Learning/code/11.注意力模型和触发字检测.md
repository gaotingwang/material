## 注意力模型

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/attn_model.png" style="width:500;height:500px;">

注意力模型的一些属性:

- 此模型中有两个单独的 LSTMs，在图片底部的一个是双向 LSTM。 因为在attention机制之前, 所以将它称为 *pre-attention* Bi-LSTM。在图顶端的LSTM 是在attention机制之后, 所以将它称为*post-attention* LSTM。*pre-attention* Bi-LSTM通过$T_x$个时间步，*post-attention* LSTM 通过$T_y$ 个时间步。
- 使用$a^{<t>} = [{\overrightarrow{a}}^{<t>}, {\overleftarrow{a}}^{<t>}]$来表示*pre-attention* Bi-LSTM的向前方向和向后方向的激活的串联。
- *post-attention* LSTM有输出激活 单元$s^{<t>}$和隐藏的单元$c^{<t>}$，通过$s^{<t>}，c^{<t>}$从一个时间步到下一个。在这个模型中,  *post-activation* LSTM在$t$时刻将不会有$y^{<t-1>}$ 作为输入，只有$s^{<t>}$和$c^{<t>}$作为输入。

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/attn_mechanism.png" style="width:500;height:500px;">

attention机制告诉一个神经网络的机器翻译模型, 在每一步它应该注意什么。告诉我们，应该花多少注意力在输入的内容$content$上。将开始图中的Attention模块放大展开得到上图：

- 当输入上下文$c^{<t>}$的时候就会生成$y^{<t>}$，$c^{<t>}$值取决于注意力参数，即$\alpha^{<t,t'>}$。参数$\alpha$告诉我们上下文有多少取决于得到的特征。所以定义上下文的方式实际上来源于被注意力权重加权的不同时间步中的特征值$c^{<t>} = \sum_{t'}^{T_x}\alpha^{<t, t'>}a^{<t'>}$。即$\alpha^{<t,t'>}$，是$y^{<t>}$花费在$a^{<t'>}$上的注意力数量。

- $\alpha^{<t,t'>}=\frac{exp(e^{<t,t'>})}{\sum_{t'=1}^{T_x}exp(e^{<t,t'>})}$，关于如何计算这些$e$项，一种方式是用小的神经网络。$s^{<t-1>}$就是编码网络在上个时间步的状态，如果想要生成$y^{<t>}$，那么$s^{<t-1>}$就是上一时间步的隐藏状态。$s^{<t-1>}$是给小神经网络的其中一个输入，然后是一堆$a^{<t'>}$。

  即如果想要决定花多少注意力在$t'$的激活值$a^{<t'>}$上。似乎它会很大程度上取决于上一个时间步的的隐藏状态的激活值$s^{<t-1>}$。因为上下文输入到这里，还没有计算出来当前状态的激活值，但是可以看看上一个生成翻译的RNN的隐藏状态。然后对于每一个位置$t'$的词都看向它们自己的特征值$a^{<t'>}$，所以$\alpha^{<t,t'>}$和$e^{<t,t'>}$应该取决于$s^{<t-1>}$和$a^{<t'>}$这两个量。

实现上图的一个注意力模块：

```python
# 首先，明确 Ty 拷贝都具有相同的权重是很重要的。也就是说, Ty的权重不应该每次重新初始化，应该具有共享权重

# Defined shared layers as global variables
# 1. 如上图先使用 RepeatVector 将s⟨t−1⟩节点的值复制Tx次
repeator = RepeatVector(Tx)
# 2. 然后将 s⟨t−1⟩ 和 a⟨t⟩ 串联起来以计算 e⟨t,t′⟩
concatenator = Concatenate(axis=-1)
# 3. 此处Dense有两层，相当于其中的2层神经网络，激活函数先用tanh, 然后用relu
densor1 = Dense(10, activation = "tanh")
densor2 = Dense(1, activation = "relu")
# 4. 然后通过 softmax 传递来计算α⟨t,t′⟩
activator = Activation(softmax, name='attention_weights')
# 5. 最后通过点乘相加计算content
dotor = Dot(axes = 1)
```

将定义组装成具体方法：

```python
def one_step_attention(a, s_prev):
    """
    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights
    "alphas" and the hidden states "a" of the Bi-LSTM.
    
    Arguments:
    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)
    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)
    
    Returns:
    context -- context vector, input of the next (post-attetion) LSTM cell
    """
    
    # 将s_prev重复Tx次后shape = (m, Tx, n_s) ，为了后面可以和a进行concatenate
    s_prev = repeator(s_prev) # (None, 30, 64) 
    # 将s_prev与a连接起来在最后一个维度方向上连接起来
    concat = concatenator([s_prev, a]) # (None, 30, 128) 
    # 用densor1全连接计算一个中间量
    e = densor1(concat) # (None, 30, 10)  
    # 再用densor2的全连接计算一个量
    energies = densor2(e) # (None, 30, 1) 
    # 对这个量使用激活函数，得出α
    alphas = activator(energies) # (None, 30, 1)
    # 最后用点乘alphas和a相加，得到一个post-attention LSTM-cell
    context = dotor([alphas, a]) # (None, 1, 64) 
    
    return context
```



最后按照最开始的图，实现整个注意力模型：

1. 首先为了LSTM-cell共享权重，定义全局变量：

```python
post_activation_LSTM_cell = LSTM(n_s, return_state = True)
output_layer = Dense(len(machine_vocab), activation=softmax)
```

2. 最后实现完成注意力模型：

```python
def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):
    """
    Arguments:
    Tx -- length of the input sequence
    Ty -- length of the output sequence
    n_a -- hidden state size of the Bi-LSTM
    n_s -- hidden state size of the post-attention LSTM
    human_vocab_size -- size of the python dictionary "human_vocab"
    machine_vocab_size -- size of the python dictionary "machine_vocab"

    Returns:
    model -- Keras model instance
    """
    
    # Define the inputs of your model with a shape (Tx,)
    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)
    X = Input(shape=(Tx, human_vocab_size)) # (None, 30, 37) 
    s0 = Input(shape=(n_s,), name='s0') # (None, 64) 
    c0 = Input(shape=(n_s,), name='c0') # (None, 64)  
    s = s0
    c = c0
    
    # Initialize empty list of outputs
    outputs = []
    
    # 1. 定义pre-attention Bi-LSTM 双向的RNN，对序列进行前向和后向计算
    # 使用Bidirectional，Remember to use return_sequences=True.
    a = Bidirectional(LSTM(n_a, return_sequences = True), merge_mode='concat')(X) # (None, 30, 64) 
    
    # 2. 遍历每一个Ty时间步，每个时间步调用注意力模型单元
    for t in range(Ty):
    
        # 2.1 调用注意力模型单元
        context = one_step_attention(a, s) # (None, 1, 64) 
        
        # 2.2 注意力模型单元之后使用post-attention LSTM cell
        # Don't forget to pass: initial_state = [hidden state, cell state] 
        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c]) 
        
        # 2.3 将Dense图层应用于post-attention LSTM的隐藏状态s的输出
        out = output_layer(s) # (None, 11) 
        
        # 2.4 Append "out" to the "outputs" list
        outputs.append(out)
    
    # 3. 根据输入输出初始化模型
    model = Model([X, s0, c0], outputs = outputs)
    
    return model
```

3. 最后，根据损失函数编译模型、训练、预测：

```python
opt = Adam(lr = 0.005, beta_1 = 0.9, beta_2 = 0.999,decay = 0.01)  
model.compile(loss = 'categorical_crossentropy', optimizer = opt,metrics = ['accuracy']) 
model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)
model.predict([source, s0, c0])
```



## 触发字检测

训练样本使用10秒的长度, 10 秒的时间可以被离散到不同的值数。原音频$441000Hz \times 10s = 441000 $ 和 频谱图的$T_x = 5511$。在前一种情况下, 每个步骤表示10秒/441000≈0.000023 秒钟；在第二种情况下, 每个步骤代表 10/5511≈0.0018 秒。每个表示形式对应的时间正好为10秒，只是它们离散不同的程度。考虑 Ty=1375 。这意味着, 对于模型的输出, 我们将10s 离散为1375个时间间隔 (每个长度 10/1375≈0.0072s), 并尝试预测每个间隔是否有人最近完成了 "activate"。

在随机时间将新的音频剪辑插入到10sec 背景上, 确保任何新插入的段不会与前面的段重叠。合成训练数据的另一个原因: 按照上面的描述, 生成标签 $y^{<t>}$相对简单。相比之下, 如果在麦克风上录制了10sec 音频, 那么一个人听它, 并在 "activate " 完成时手动标记是相当耗时的。

最后还有pydub 模块合成音频的10000个数字，这对应于将10s 离散为10000个时间间隔 (每个长度10/10000 = 0.001s), 0.001 秒即1ms。所以, 根据1ms 的时间间隔离散, 这意味着我们在合成音频中的10s使用了10000步骤。

<img src="https://gtw.oss-cn-shanghai.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/deep-learning/%E8%A7%A6%E5%8F%91%E5%AD%97%E6%A3%80%E6%B5%8B.png" style="width:600px;height:600px;">

最后根据样本，结合上图编写模型：

```python
def model(input_shape):
    """
    Function creating the model's graph in Keras.
    
    Argument:
    input_shape -- shape of the model's input data (using Keras conventions)

    Returns:
    model -- Keras model instance
    """
    
    X_input = Input(shape = input_shape)
    
    # Step 1: CONV layer
    # 此模型的一个关键步骤首先是1D的卷积。它输入5511步频谱图, 输出为1375
    # 此层扮演的角色类似二维的卷积，即提取低级特征, 然后可能生成较小维度的输出。
    # 计算上, 1-D的conv层也有助于加快模型的速度
    # with 196 filters, a filter size of 15, and stride of 4 ==> 5511 -> 1375
    X = Conv1D(filters = 196, kernel_size = 15, strides = 4)(X_input) # (1375, 196)
    X = BatchNormalization()(X)                      # Batch normalization
    X = Activation('relu')(X)                        # ReLu activation
    X = Dropout(0.8)(X)                              # dropout (use 0.8)

    # Step 2: First GRU Layer
    X = GRU(units = 128, return_sequences=True)(X)   # (1375, 128)
    X = Dropout(0.8)(X)                              # dropout (use 0.8)
    X = BatchNormalization()(X)                      # Batch normalization
    
    # Step 3: Second GRU Layer 
    X = GRU(units = 128, return_sequences=True)(X)   # (1375, 128)
    X = Dropout(0.8)(X)                              # dropout (use 0.8)
    X = BatchNormalization()(X)                      # Batch normalization
    X = Dropout(0.8)(X)                              # dropout (use 0.8)
    
    # Step 4: Time-distributed dense layer
    # TimeDistributed封装器将一个层应用于输入的1375个每个时间片
    # 即对每个时间片使用dense层, 后跟一个sigmoid, 因此用于dense层的参数在每一时间步中都是相同的
    X = TimeDistributed(Dense(1, activation = "sigmoid"))(X)  # (1375, 1)

    model = Model(inputs = X_input, outputs = X)
    
    return model 
```

