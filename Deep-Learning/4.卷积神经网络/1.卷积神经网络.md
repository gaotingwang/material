[TOC]

## 卷积神经网络（Foundations of Convolutional Neural Networks）

多亏了深度学习，计算机视觉成为一个飞速发展的一个领域。第一，计算机视觉的高速发展标志着新型应用产生的可能。其次，人们对于计算机视觉的研究是如此富有想象力和创造力，由此衍生出新的神经网络结构与算法，这实际上启发人们去创造出计算机视觉与其他领域的交叉成果。

但在应用计算机视觉时要面临一个挑战，就是数据的输入可能会非常大。举个例子，在过去的课程中，一般操作的都是64×64的小图片，数据量为12288，这其实还好，因为64×64真的是很小的一张图片。如果要操作更大的图片，比如一张1000×1000的图片，它足有1兆那么大，特征向量的维度达到了1000×1000×3，将会是300万。

如果要输入300万的数据量，这就意味着，特征向量$x$的维度高达300万。所以在第一隐藏层中，假设有1000个隐藏单元。如果使用了标准的全连接网络，这个矩阵的大小将会是1000×300万，意味着矩阵$W^{[1]}$会有30亿个参数，这是个非常巨大的数字。在参数如此大量的情况下，难以获得足够的数据来防止神经网络发生过拟合和竞争需求，要处理包含30亿参数的神经网络，巨大的内存需求让人不太能接受。

为此，需要进行卷积计算，它是卷积神经网络中非常重要的一块。

### 1. 边缘检测示例（Edge detection example）

卷积运算是卷积神经网络最基本的组成部分，使用边缘检测作为入门样例，看看卷积是如何进行运算的。

![](http://www.ai-start.com/dl2017/images/47c14f666d56e509a6863e826502bda2.png)

举个例子，给了这样一张图片，让电脑去搞清楚这张照片里有什么物体，做的第一件事是检测图片中的垂直边缘，比如说，在这张图片中的栏杆就对应垂直线。同样，可能也想检测水平边缘，比如说这些栏杆就是很明显的水平线，它们也能被检测到。所以如何在图像中检测这些边缘？

![](http://www.ai-start.com/dl2017/images/7099a5373f2281626aa8ddd47a180571.png)

看一个例子，上图是一个6×6的灰度图像。为了检测图像中的垂直边缘，可以构造一个3×3矩阵。在卷积神经网络的术语中，这个3x3的矩阵被称为过滤器，像这样$\begin{bmatrix}1 & 0 & -1\\ 1 & 0 & -1\\ 1 & 0 & -1\end{bmatrix}$。在论文中它有时候会被称为核，而不是过滤器。对这个6×6的图像进行卷积运算，在数学中“$*$”就是卷积的标准标志，用3×3的过滤器对其进行卷积。

![](http://www.ai-start.com/dl2017/images/d6ecaeb7228172a00bc3948e8b214a27.png)

这个卷积运算的输出将会是一个4×4的矩阵，可以将它看成一个4×4的图像。为了计算第一个元素，在4×4左上角的那个元素，使用3×3的过滤器，将其覆盖在输入图像。然后进行元素乘法运算：
$$
\begin{bmatrix} 3 \times 1 & 0 \times 0 & 1 \times \left(1 \right) \\ 1 \times 1 & 5 \times 0 & 8 \times \left( - 1 \right) \\ 2 \times1 & 7 \times 0 & 2 \times \left( - 1 \right) \\ \end{bmatrix} = \begin{bmatrix}3 & 0 & - 1 \\ 1 & 0 & - 8 \\ 2 & 0 & - 2 \\\end{bmatrix}
$$
然后将该矩阵每个元素相加得到最左上角的元素，即$3+1+2+0+0 +0+(-1)+(-8) +(-2)=-5$。

为了弄明白第二个元素是什么，要把蓝色的方块，向右移动一步，继续做同样的元素乘法，然后加起来，所以是 $0×1+5×1+7×1+1×0+8×0+2×0+2×(-1)+ 9×(-1)+5×(-1)=-4 $。

![](http://www.ai-start.com/dl2017/images/ad626a7a5a1cda8eb679e15f953f84a7.png)



同理可以得到后面两个元素分别为：0和8。

接下来为了得到下一行的元素，现在把蓝色块下移，现在蓝色块在这个位置：

![](http://www.ai-start.com/dl2017/images/348ff3ef87dd57f40b0ed0e0571f7751.png)

重复进行元素乘法，然后加起来。通过这样做得到-10。再将其右移得到-2，接着是2，3。以此类推，这样计算完矩阵中的其他元素。

因此6×6矩阵和3×3矩阵进行卷积运算得到4×4矩阵。这些图片和过滤器是不同维度的矩阵，但左边矩阵容易被理解为一张图片，中间的这个被理解为过滤器，右边的图片我们可以理解为另一张图片。这个就是垂直边缘检测器。

为什么这个可以做垂直边缘检测呢？为了讲清楚，用一个简单的例子。这是一个简单的6×6图像，左边的一半是10，右边一般是0。如果把它当成一个图片，左边那部分看起来是白色的，右边是灰色。图片里，有一个特别明显的垂直边缘在图像中间，这条垂直线是从白色到深色的过渡线。

![](http://www.ai-start.com/dl2017/images/50836692632e32453f0eefcbbf58551b.png)

当用一个3×3过滤器进行卷积运算的时候，卷积运算后，得到的是右边的矩阵。如果把最右边的矩阵当成图像，它中间有段亮一点的区域，对应检查到这个6×6图像中间的垂直边缘。这里的维数似乎有点不正确，检测到的边缘太粗了。因为在这个例子中，图片太小了。

如果用一个1000×1000的图像，而不是6×6的图片，会发现可以很好地检测出图像中的垂直边缘。在这个例子中，在输出图像中间的亮处，表示在图像中间有一个特别明显的垂直边缘。

从垂直边缘检测中可以得到的启发是，因为我们使用3×3的矩阵（过滤器），所以垂直边缘是一个3×3的区域，左边是明亮的像素，中间的并不需要考虑，右边是深色像素。在这个6×6图像的中间部分，明亮的像素在左边，深色的像素在右边，就被视为一个垂直边缘，卷积运算提供了一个方便的方法来发现图像中的垂直边缘。

### 2. 更多边缘检测内容（More edge detection）

如何区分正边和负边，这实际就是由亮到暗与由暗到亮的区别，也就是边缘的过渡。

![](http://www.ai-start.com/dl2017/images/6a248e5698d1f61ac4ba0238363c4a37.png)

这幅图与上一章的区别是：它的颜色被翻转了，变成了左边比较暗，而右边比较亮。最后中间的过渡部分被翻转了，之前的30翻转成了-30，表明是由暗向亮过渡，而不是由亮向暗过渡。

如果不在乎这两者的区别，可以取出矩阵的绝对值。但这个特定的过滤器确实可以为我们区分这两种明暗变化的区别。

再来看看更多的边缘检测的例子，我们已经见过这个3×3的过滤器，它可以检测出垂直的边缘。所以，下图右边这个过滤器，能检测出水平的边缘。提醒一下：**一个垂直边缘过滤器是一个3×3的区域，它的左边相对较亮，而右边相对较暗。相似的，右边这个水平边缘过滤器也是一个3×3的区域，它的上边相对较亮，而下方相对较暗**。

![](http://www.ai-start.com/dl2017/images/199323db1d4858ef2463f34323e1d85f.png)

这里还有个更复杂的例子，左上方和右下方都是亮度为10的点。如果你将它绘成图片，右上角是比较暗的地方，这边都是亮度为0的点，我把这些比较暗的区域都加上阴影。而左上方和右下方都会相对较亮。如果你用这幅图与水平边缘过滤器卷积，就会得到右边这个矩阵。

![](http://www.ai-start.com/dl2017/images/eb8668010205b08fbcbcde7c2bb1fee2.png)

再次强调，我们现在所使用的都是相对很小的图片，仅有6×6。但这些中间的数值，比如说这个10（右边矩阵中黄色方框标记元素）代表的是左边这块区域（左边6×6矩阵中黄色方框标记的部分）。这块区域左边两列是正边，右边一列是负边，正边和负边的值加在一起得到了一个中间值。但假如这个一个非常大的1000×1000的类似这样棋盘风格的大图，就不会出现这些亮度为10的过渡带了，因为图片尺寸很大，这些中间值就会变得非常小。

总而言之，通过使用不同的过滤器，可以找出垂直的或是水平的边缘。但事实上，对于这个3×3的过滤器来说，我们使用了其中的一种数字组合。

在历史上，在计算机视觉的文献中，曾公平地争论过怎样的数字组合才是最好的，所以还可以使用这种：$\begin{bmatrix}1 & 0 & - 1 \\ 2 & 0 & - 2 \\ 1 & 0 & - 1 \\\end{bmatrix}$，叫做Sobel的过滤器，它的优点在于增加了中间一行元素的权重，这使得结果的鲁棒性会更高一些。计算机视觉的研究者们也会经常使用其他的数字组合，比如这种：$\begin{bmatrix} 3& 0 & - 3 \\ 10 & 0 & - 10 \\ 3 & 0 & - 3 \\\end{bmatrix}$，这叫做Scharr过滤器，它有着和之前完全不同的特性，实际上也是一种垂直边缘检测，如果将其翻转90度，你就能得到对应水平边缘检测。

![](http://www.ai-start.com/dl2017/images/f889ad7011738a23d78070e8ed2df04e.png)

随着深度学习的发展，我们学习的其中一件事就是当你真正想去检测出复杂图像的边缘，不一定要去使用那些研究者们所选择的这九个数字，但你也可以从中获益匪浅。把这矩阵中的9个数字当成9个参数，并且在之后可以学习使用反向传播算法，其目标就是去理解这9个参数。当得到左边这个6×6的图片，将其与这个3×3的过滤器进行卷积，将会得到一个出色的边缘检测。通过反向传播，可以学习许多不同的过滤器。还有另一种过滤器，这种过滤器对于数据的捕捉能力甚至可以胜过任何之前这些手写的过滤器。相比这种单纯的垂直边缘和水平边缘，它可以检测出45°或70°或73°，甚至是任何角度的边缘。

所以将矩阵的所有数字都设置为参数，通过数据反馈，让神经网络自动去学习它们，我们会发现神经网络可以学习一些低级的特征，例如这些边缘的特征。反向传播算法能够让神经网络学习任何它所需要的3×3的过滤器，并在整幅图片上去应用它。这种方式可以让神经网络检测到任何特征，不管是垂直的边缘，水平的边缘，还有其他奇怪角度的边缘，甚至是其它的连名字都没有的过滤器。

### 3. Padding

为了构建深度神经网络，需要学会使用的一个基本的卷积操作就是padding。

之前讲的如果用一个3×3的过滤器卷积一个6×6的图像，最后会得到一个4×4矩阵。这背后的数学解释是，如果有一个$n×n$的图像，用$f×f$的过滤器做卷积，那么输出的维度就是$(n-f+1)×(n-f+1)$。这样的话会有两个缺点：

1. 每次做卷积操作，图像就会缩小，可能做了几次之后，图像就会变得很小了，可能会缩小到只有1×1的大小。

2. 如果注意角落边缘的像素，比如绿色阴影标记的只被一个输出所使用，但如果是在中间的像素点，比如红色阴影标记的就会有许多3×3的区域与之重叠。所以在角落或者边缘区域的像素点在输出中采用较少，意味着丢掉了图像边缘位置的许多信息。

   ![](http://www.ai-start.com/dl2017/images/170e076ceaeb70339baa7b25ad5f5e6c.png)

在建立深度神经网络时，如果图像每经过一层都缩小的话，经过100层网络后，就会得到一个很小的图像，所以这是个问题。另一个问题是图像边缘的大部分信息都丢失了。

![](http://www.ai-start.com/dl2017/images/208104bae9256fba5d8e37e22a9f5408.png)

为了解决这些问题，可以沿着图像边缘填充一层像素$p=1$，习惯上可以用0去填充，这样输出维度就变成了：
$$
(n+2p-f+1)×(n+2p-f+1)
$$
涂绿的像素点（左边矩阵）影响了输出中的这些格子（右边矩阵）。这样一来，丢失信息和图像边缘信息发挥作用较小的这一缺点就被削弱了。

如果想的话，也可以填充两个像素点$p=2$，实际上还可以填充更多像素。至于选择填充多少像素，通常有两个选择，分别叫做Valid卷积和Same卷积：

- Valid卷积意味着不填充，这样的话，如果有一个$n×n$的图像，用一个$f×f$的过滤器卷积，它将会得出一个$(n-f+1)×(n-f+1)$维的输出。

- 另一个经常被用到的叫做Same卷积，那意味填充后，输出大小和输入大小是一样的。

  因此如果有一个$n×n$的图像，用$p$个像素填充边缘，如果想让$n+2p-f+1=n$的话，使得输出和输入大小相等。对等式求解得$p=\frac{f-1}{2}$。所以当$f$是一个奇数的时候，只要选择相应的填充尺寸，就能确保得到和输入相同尺寸的输出。

计算机视觉中，$f$通常是奇数，甚至是很少看到一个偶数的过滤器在计算机视觉里使用，主要有两个原因：

- 其中一个可能是，如果$f$是一个偶数，那么只能使用一些不对称填充。只有$f$是奇数的情况下，Same卷积才会有自然的填充，我们可以以同样的数量填充四周，而不是左边填充多一点，右边填充少一点，这样不对称的填充。

- 第二个原因是当有一个奇数维过滤器，它就有一个中心点。有时在计算机视觉里，如果有一个中心像素点会更方便，便于指出过滤器的位置。

  ![](http://www.ai-start.com/dl2017/images/ca5382358f30c1349fff98d1e52366b4.png)

为了指定卷积操作中的padding，可以指定$p$的值。也可以使用Valid卷积，也就是$p=0$。也可使用Same卷积填充像素，使输出和输入大小相同。

### 4. 卷积步长（Strided convolutions）

卷积中的步幅是构建卷积神经网络的另一个基本操作。

![](http://www.ai-start.com/dl2017/images/2739e8477bc1e66d482ee8aff917acab.png)

之前移动蓝框的步长是1，现在移动的步长是2，跳过了一个位置，然后还是将每个元素相乘并求和。

![](http://www.ai-start.com/dl2017/images/9cacda308d53adb7d154a3b259569f45.png)

当移动到下一行的时候，也是使用步长2而不是步长1。所以在这个例子中，我们用3×3的矩阵卷积一个7×7的矩阵，得到一个3×3的输出。

输入和输出的维度是由下面的公式决定的，如果用一个$f×f$的过滤器卷积一个$n×n$的图像，padding为$p$，步幅为$s$，输出维度于是变为:
$$
\frac{n+2p - f}{s} + 1 \times \frac{n+2p - f}{s} + 1
$$
最后的一个细节，如果商不是一个整数怎么办？在这种情况下，我们**向下取整**。这个原则实现的方式是，只在蓝框完全填充满的图像内部时，才对它进行运算。如果有任意一个蓝框移动到了外面，那就不要进行相乘操作，这是一个惯例。

![](http://www.ai-start.com/dl2017/images/16196714c202bb1c8022219394543bf5.png)

结合向下取整的情况总结一下维度，如果有一个$n×n$的矩阵与一个$f×f$的矩阵卷积，Padding是$p$，步幅为$s$，输出尺寸就是：
$$
\lfloor\frac{n+2p - f}{s} + 1\rfloor \times \lfloor\frac{n+2p - f}{s} + 1\rfloor
$$
最后补充一下，如果看一本典型的数学教科书，对卷积的定义实际上有一个步骤是首先要做，就是在把这个过滤器进行翻转，将$\begin{bmatrix}3 & 4 & 5 \\ 1 & 0 & 2 \\  - 1 & 9 & 7 \\ \end{bmatrix}$变为$\begin{bmatrix} 7& 2 & 5 \\ 9 & 0 & 4 \\  - 1 & 1 & 3 \\\end{bmatrix}$，相当于对过滤器做了个镜像（整理者注：此处应该是先顺时针旋转90得到$\begin{bmatrix}-1 & 1 & 3 \\ 9 & 0 & 4 \\ 7 & 2 & 5 \\\end{bmatrix}$，再水平翻转得到$\begin{bmatrix}  7& 2 & 5 \\ 9 & 0 & 4 \\  - 1& 1 & 3 \\\end{bmatrix}$）。

我们在卷积运算时，跳过了这个镜像操作。从技术上讲，我们实际上做的，被称为互相关（cross-correlation）而不是卷积（convolution）。按照机器学习的惯例，通常不进行翻转操作，在大部分的深度学习文献中都把它叫做卷积运算，因此也在这里使用这个约定。在信号处理中或某些数学分支中，对卷积的定义包含翻转，使得卷积运算符拥有结合律性质，即$(A*B)*C=A*(B*C)$。这对于一些信号处理应用来说很好，但对于深度神经网络来说它不重要，因此省略了这个双重镜像操作，简化了代码，而且神经网络也能正常工作。

### 5. 三维卷积（Convolutions over volumes）

卷积不仅仅可以在二维图像上，也可以在三维立体上。

如下图假设想检测RGB彩色图像的特征，左边是6×6×3的图像，中间是3×3×3的过滤器，**最后一个数字通道数必须和过滤器中的通道数相匹配**。

![](http://www.ai-start.com/dl2017/images/9b0b0e9062f8814a6a462ea64449f89e.png)

为了计算这个卷积操作的输出，要做的就是把这个3×3×3的过滤器先放到最左上角的位置，依次取这27个数，乘以对应位置的红绿蓝通道中的数字，最后把这些数都加起来，就得到了输出的第一个数字。以此类推，滑动黄色立方体，计算下一个输出。

那么这个能干什么呢？举个例子，这个过滤器是3×3×3的，如果想检测图像红色通道的边缘，那么可以将第一个过滤器设为$\begin{bmatrix}1 & 0 & - 1 \\ 1 & 0 & - 1 \\ 1 & 0 & - 1 \\\end{bmatrix}$，而绿色通道全为$\begin{bmatrix} 0& 0 & 0 \\ 0 &0 & 0 \\ 0 & 0 & 0 \\\end{bmatrix}$，蓝色也全为0。如果把这三个堆叠在一起形成一个3×3×3的过滤器，那么这就是一个检测垂直边界的过滤器，但只对红色通道有用。如果不关心垂直边界在哪个颜色通道里，那么可以用一个这样的过滤器，$\begin{bmatrix}1 & 0 & - 1 \\ 1 & 0 & - 1 \\ 1 & 0 & - 1 \\ \end{bmatrix}$，$\begin{bmatrix}1 & 0 & - 1 \\ 1 & 0 & - 1 \\ 1 & 0 & - 1 \\ \end{bmatrix}$，$\begin{bmatrix}1 & 0 & - 1 \\ 1 & 0 & - 1 \\ 1 & 0 & - 1 \\\end{bmatrix}$，所有三个通道都是一样。按照计算机视觉的惯例，当输入有特定的高宽和通道数时，**==过滤器可以有不同的高，不同的宽，但是必须有一样的通道数==**。理论上，我们的过滤器只关注红色通道，或者只关注绿色或者蓝色通道也是可行的。

还有最后一个概念，对建立卷积神经网络至关重要。如果想要同时检测垂直边缘和水平边缘，还有45°倾斜的边缘，还有70°倾斜的边缘怎么做？即如果想同时用多个过滤器怎么办？

![](http://www.ai-start.com/dl2017/images/794b25829ae809f93ac69f81eee79cd1.png)

可以使用多个不同的过滤器。先和第一个过滤器卷积（黄色），可以得到第一个4×4的输出，然后卷积第二个过滤器（橘色），得到一个不同的4×4的输出。做完卷积后，最后把这两个输出堆叠在一起，这样就得到了一个4×4×2的输出立方体。

总结一下维度，如果有一个$n \times n \times n_{c}$的输入图像，然后卷积一个$f×f×n_{c}$的过滤器，这里的$n_{c}$是通道数目，必须数值相同。然后就得到了$（n-f+1）×（n-f+1）×n_{c^{'}}$，这里$n_{c^{'}}$是下一层的通道数，它就是使用的过滤器个数。

这个对立方体卷积的概念真的很有用，现在可以用它的一小部分直接在三个通道的RGB图像上进行操作。更重要的是，可以检测两个特征，比如垂直和水平边缘，或者更多的不同特征，并且输出的通道数会等于要检测的特征数。

### 6. 单层卷积网络（One layer of a convolutional network）

已经讲了如何通过两个过滤器卷积处理一个三维图像，并输出两个不同的4×4矩阵，最终各自形成一个卷积神经网络层。然后对第一个矩阵增加偏差，它是一个实数，之后应用非线性函数，如激活函数ReLU，输出结果是一个4×4矩阵。对于第二个4×4矩阵，加上不同的偏差，也是一个实数，然后应用非线性函数，最终得到另一个4×4矩阵。最后重复之前的步骤，把这两个矩阵堆叠起来，得到一个4×4×2的矩阵。

![](http://www.ai-start.com/dl2017/images/f75c1c3fb38083e046d3497656ab4591.png)

注意前向传播中一个操作就是$z^{[1]} = W^{[1]}a^{[0]} + b^{[1]}$，其中$a^{[0]} =x$，执行非线性函数得到$a^{[1]}$，即$a^{[1]} = g(z^{[1]})$。这里的输入是$a^{\left\lbrack 0\right\rbrack}$，也就是$x$，这些过滤器用变量$W^{[1]}$表示。在卷积过程中，我们对这27x2个数进行操作，因为用了两个过滤器。实际执行了一个线性函数，得到一个4×4的矩阵。卷积操作的输出结果是一个4×4的矩阵，它的作用类似于$W^{[1]}a^{[0]}$，也就是这两个4×4矩阵的输出结果，然后加上偏差。

图中蓝色边框标记的部分就是应用激活函数ReLU之前的值，它的作用类似于$z^{[1]}$，最后应用非线性函数，得到的这个4×4×2矩阵，成为神经网络的下一层，也就是激活层。

这就是$a^{[0]}$到$a^{[1]}$的演变过程：首先执行线性函数，把所有元素相乘做卷积，具体做法是运用线性函数再加上偏差，然后应用激活函数ReLU。这样就通过神经网络的一层把一个6×6×3的维度$a^{[0]}$演化为一个4×4×2维度的$a^{[1]}$，这就是卷积神经网络的一层。

假设有10个过滤器，且过滤器的维度为3×3×3，参数共有(27 + 1) x 10，即共有280个参数。注意一点，不论输入图片有多大，1000×1000也好，5000×5000也好，参数始终都是280个。用这10个过滤器来提取特征，如垂直边缘，水平边缘和其它特征。即使这些图片很大，但参数却很少，这就是卷积神经网络的一个特征，叫作“**避免过拟合**”。

**总结卷积层的各种标记：**

以$l$层为例，用$f^{[l]}$表示过滤器大小，则滤器大小为$f^{[l]} \times f^{[l]}$。
用$p^{[l]}$来标记padding的数量。
用$s^{[l]}$标记步幅。
输入的高度和宽度也有可能不同，所以分别用上下标$H$和$W$来标记，即输入层为$n_{H}^{\left\lbrack l - 1 \right\rbrack} \times n_{W}^{\left\lbrack l - 1 \right\rbrack} \times n_{c}^{\left\lbrack l - 1\right\rbrack}$。神经网络这一层中会有输出，其大小为$n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]}$，这就是输出图像的大小。

根据之前提到计算输出维度的公式$\lfloor\frac{n+2p - f}{s} + 1\rfloor$。在这个新表达式中，$l$层输出图像的高度，即$n_{H}^{[l]} = \lfloor\frac{n_{H}^{\left\lbrack l - 1 \right\rbrack} +2p^{[l]} - f^{[l]}}{s^{[l]}} +1\rfloor$，同样可以计算出宽度，即$n_{W}^{[l]} = \lfloor\frac{n_{W}^{\left\lbrack l - 1 \right\rbrack} +2p^{[l]} - f^{[l]}}{s^{[l]}} +1\rfloor$。这就是由$n_{H}^{\left\lbrack l - 1 \right\rbrack}$推导$n_{H}^{[l]}$以及$n_{W}^{\left\lbrack l - 1\right\rbrack}$推导$n_{W}^{[l]}$的过程。

==输出图像中的通道数量就是神经网络中这一层所使用的过滤器的数量==。==过滤器中通道的数量必须与输入中通道的数量一致==，所以过滤器维度等于$f^{[l]} \times f^{[l]} \times n_{c}^{\left\lbrack l - 1 \right\rbrack}$。

当执行批量梯度下降或小批量梯度下降时，如果有$m$个例子，就是有$m$个激活值的集合，那么输出$A^{[l]} = m \times n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]}$。

权重参数的维度是与过滤器的维度一样，为$f^{[l]} \times  f^{[l]} \times  n_{c}^{[l - 1]}$，$n_{c}^{[l-1]}$表示的是上一层的通道数。如果有多少个过滤器，则$n_{c}^{[l]}$是过滤器的数量。权重也就是过滤器的集合再乘以过滤器的总数量，即$f^{[l]} \times f^{[l]} \times  n_{c}^{[l - 1]} \times n_{c}^{[l]}$。

最后看偏差参数，每个过滤器都有一个偏差参数，它是一个实数。偏差包含了这些变量，它是该维度上的一个向量。后续课程中会看到，为了方便，偏差在代码中表示为一个$1×1×1×n_{c}^{[l]}$的四维向量。

**卷积网路维度顺序说明：**

卷积有很多种标记方法，这是最常用的卷积符号。关于高度，宽度和通道的顺序并没有完全统一的标准卷积，有些作者会采用把通道放在首位的编码标准。实际上在一些架构中，当检索这些图片时，会有一个变量或参数来标识计算通道数量和通道损失数量的先后顺序。只要保持一致，这两种卷积标准都可用。

**具体示例说明：**

假设有一张图片，要对图片分类，把这张图片输入定义为$x$，然后辨别图片中有没有猫，用0或1表示。针对这个示例，用一张比较小的图片，大小是39×39×3。

![](http://www.ai-start.com/dl2017/images/7181dca1d3663d558944cbfd428c4727.png)

1. 假设第一层用一个3×3的过滤器来提取特征，那么$f^{[1]} = 3$。$s^{[1]} = 1$，$p^{[1]} =0$，所以高度和宽度使用valid卷积，如果有10个过滤器。

   因为它是一个vaild卷积，根据$\frac{39 + 0 - 3}{1} + 1 = 37$，所以输出是37×37，所以最终输出结果的大小为37x37x10。第一层标记为$n_{H}^{[1]} = n_{W}^{[1]} = 37$，$n_{c}^{[1]} = 10$，$n_{c}^{[1]}$等于第一层中过滤器的个数，这就是第一层激活值的维度。

2. 假设还有另外一个卷积层，这次采用的过滤器是5×5的矩阵，步幅为2，即$s^{\left\lbrack 2 \right\rbrack} = 2$，padding为0，即$p^{\left\lbrack 2 \right\rbrack} = 0$，且有20个过滤器。

   因为步幅是2，维度缩小得很快，大小从37×37减小到17×17，过滤器是20个，所以输出结果的通道数也是20。因此$n_{H}^{\left\lbrack 2 \right\rbrack} = n_{W}^{\left\lbrack 2 \right\rbrack} = 17$，$n_{c}^{\left\lbrack 2 \right\rbrack} = 20$，这次$a^{\left\lbrack 2 \right\rbrack}$的输出结果为17×17×20。

3. 构建最后一个卷积层，假设过滤器还是5×5，步幅为2，即$f^{\left\lbrack 2 \right\rbrack} = 5$，$s^{\left\lbrack 3 \right\rbrack} = 2$，假设使用了40个过滤器，padding为0，所以最后结果为7×7×40。

到此，这张39×39×3的输入图像就处理完毕了，为图片提取了7×7×40个特征，计算出来就是1960个特征。然后对该卷积进行处理，可以将其展开成1960个单元。

![](http://www.ai-start.com/dl2017/images/f97b7dfd7775fea9f9d18d8b404ad4ef.png)

最后激活函数填充内容是logistic回归单元还是softmax回归单元，完全取决于我们是想识别图片上有没有猫，还是想识别$K$种不同对象中的一种。

明确一点，最后这一步是处理所有数字，即全部的1960个数字，把它们展开成一个很长的向量。为了预测最终的输出结果，我们把这个长向量填充到softmax回归函数中，用$\hat y$表示最终神经网络的预测输出。

这是卷积神经网络的一个典型范例，设计卷积神经网络时，确定这些超参数（过滤器的大小、步幅、padding以及使用多少个过滤器）比较费工夫，后面会针对选择参数的问题提供一些建议和指导。

### 7. 池化层（Pooling layers）

除了卷积层，卷积网络也经常使用池化层来缩减模型的大小，提高计算速度，同时提高所提取特征的鲁棒性。

**最大池化（max pooling）：**

假如输入是一个4×4矩阵，用到的池化类型是**最大池化（max pooling）**。执行最大池化的树池过程非常简单，假设池化为一个2x2的矩阵，即把4×4的输入拆分成不同的区域，这个区域用不同颜色来标记。对于2×2的输出，输出的每个元素都是其对应颜色区域中的最大元素值。

![](http://www.ai-start.com/dl2017/images/9687f796debffce065daa8c654b9b7b7.png)

这就像是应用了一个规模为2x2的过滤器，计算出该区域的最大值，然后向右移动2个步幅。即$f=2$，$s=2$。

这是对最大池化功能的直观理解，你可以把这个4×4输入看作是某些特征的集合。数字大意味着可能探测到了某些特定的特征，最大化操作的功能就是只要在任何一个象限内提取到某个特征，它都会保留在最大化的池化输出里。必须承认，人们使用最大池化的主要原因是此方法在很多实验中效果都很好。

其中一个有意思的特点就是，它有一组超参数（$f$和$s$），但并没有参数需要学习。实际上，梯度下降没有什么可学的，一旦确定了$f$和$s$，它就是一个固定运算，梯度下降无需改变任何值。

**如果输入是三维的，那么输出也是三维的**。==计算最大池化的方法就是分别对每个通道执行刚刚的计算过程，即对每个通道都单独执行最大池化计算==，以上就是最大池化算法。

**平均池化：**

另外还有一种类型的池化，平均池化，它不太常用。这种运算顾名思义，选取的不是每个过滤器的最大值，而是平均值。

![](http://www.ai-start.com/dl2017/images/1b5bbeadd76a7d65d3201c1e3de467ca.png)

目前来说，最大池化比平均池化更常用。但也有例外，就是深度很深的神经网络，可以用平均池化来分解规模为7×7×1000的网络的表示层，使用7x7的过滤器，然后在整个空间内求平均值，得到1×1×1000。

**总结：**

池化的超级参数包括过滤器大小$f$和步幅$s$，常用的参数值为$f=2$，$s=2$，应用频率非常高，其效果相当于高度和宽度缩减一半。也有使用$f=3$，$s=2$的情况。至于其它超级参数就要看用的是最大池化还是平均池化了。也可以根据自己意愿增加表示padding的其他超级参数，但很少很少这么用，在最大池化时，往往很少用到超参数padding，当然也有例外的情况。

大部分情况下，最大池化的输入就是$n_{H} \times n_{W} \times n_{c}$，假设没有padding，则输出$\lfloor\frac{n_{H} - f}{s} +1\rfloor \times \lfloor\frac{n_{w} - f}{s} + 1\rfloor \times n_{c}$，输入通道与输出通道个数相同。

需要注意的一点是，池化过程中没有需要学习的参数。执行反向传播时，反向传播没有参数适用于最大池化。只有这些设置过的超参数，可能是手动设置的，也可能是通过交叉验证设置的。最后最大池化只是计算神经网络某一层的静态属性。

### 8. 卷积神经网络示例（Convolutional neural network example）

构建全卷积神经网络的构造模块我们已经掌握得差不多了，一个典型的卷积神经网络通常有三层：

1. 一个是卷积层，通常用**CONV**来标注；
2. 还有一层是池化层，称之为**POOL；**
3. 最后一个是全连接层，用**FC**表示。

虽然仅用卷积层也有可能构建出很好的神经网络，但大部分神经望楼架构师依然会添加池化层和全连接层。结合例子最后看一下全连接层（fully connected）。

假设，有一张大小为32×32×3的输入图片，一张RGB模式的图片，想做手写体数字识别，想识别它是从0-9数字中的哪一个，我们构建一个神经网络来实现这个功能。

![](http://www.ai-start.com/dl2017/images/8b70ab4bfd6764598058a201c3e402b4.png)

输入是32×32×3的矩阵，假设第一层使用过滤器大小为5×5，步幅是1，padding是0，过滤器个数为6。然后增加偏差，应用非线性函数，如ReLU非线性函数。最后输出为28×28×6，将这层标记为CONV1。

然后构建一个池化层，这里选择用最大池化，参数$f=2$，$s=2$（padding为0）。最大池化使用的过滤器为2×2，步幅为2，表示层的高度和宽度会减少一半。因此，28×28变成了14×14，通道数量保持不变，所以最终输出为14×14×6，将该输出标记为POOL1。

在卷积神经网络文献中，卷积有两种分类，这与所谓层的划分存在一致性。一类卷积是一个卷积层和一个池化层一起作为一层，作为神经网络的Layer1。另一类卷积是把卷积层作为一层，而池化层单独作为一层。这里把CONV1和POOL1共同作为一个卷积，并标记为Layer1。一般在统计网络层数时，只计算具有权重的层，也就是把CONV1和POOL1作为Layer1。

![](http://www.ai-start.com/dl2017/images/f00469a1fbea2ace64f1280849f9cd7d.png)

再为它构建一个卷积层，过滤器大小为5×5，步幅为1，这次用16个过滤器，最后输出一个10×10×16的矩阵，标记为CONV2。

然后做最大池化，超参数$f=2$，$s=2$。高度和宽度会减半，最后输出为5×5×16，标记为POOL2，这就是神经网络的第二个卷积层，即Layer2。

**全连接层（fully connected）:**

5×5×16矩阵包含400个元素，现在将POOL2平整化为一个大小为400的一维向量。我们可以把平整化结果想象成这样的一个神经元集合，然后利用这400个单元构建下一层。下一层含有120个单元，这就是我们第一个全连接层，标记为FC3。

![](http://www.ai-start.com/dl2017/images/c6894969e33cfc84b16267b752baf644.png)

这400个单元与120个单元紧密相连，这就是全连接层。它很像之前课中讲过的单神经网络层，这是一个标准的神经网络。它的权重矩阵为$W^{\left\lbrack 3 \right\rbrack}$，维度为120×400。这就是所谓的“全连接”，因为这400个单元与这120个单元的每一项连接，还有一个偏差参数。最后输出120个维度，因为有120个输出。

然后对这个120个单元再添加一个全连接层，这层更小，假设它含有84个单元，标记为FC4。

最后，用这84个单元填充一个softmax单元。如果想通过手写数字识别来识别手写0-9这10个数字，那么softmax就会有10个输出。

此例中的卷积神经网络很典型，看上去它有很多超参数，关于如何选定这些参数，后面会提供更多建议。**常规做法是，尽量不要自己设置超参数，而是查看文献中别人采用了哪些超参数，选一个在别人任务中效果很好的架构**，那么它也有可能适用于你自己的应用程序。

在神经网络中，另一种常见模式就是一个或多个卷积后面跟随一个池化层，然后一个或多个卷积层后面再跟一个池化层，然后是几个全连接层，最后是一个softmax。这是神经网络的另一种常见模式。

**总结：**

==随着神经网络深度的加深，高度$n_{H}$和宽度$n_{W}$通常都会减少，而通道数量会增加，然后得到一个全连接层。==

关于卷积网路有3点要注意：

1. 第一，池化层和最大池化层没有参数；
2. 第二卷积层的参数相对较少，其实许多参数都存在于神经网络的全连接层。
3. 随着神经网络的加深，激活值尺寸会逐渐变小，如果激活值尺寸下降太快，也会影响神经网络性能。

一个卷积神经网络包括**卷积层**、**池化层**和**全连接层**。许多计算机视觉研究正在探索如何把这些基本模块整合起来，构建高效的神经网络，整合这些基本模块确实需要深入的理解。

### 9. 为什么使用卷积？（Why convolutions?）

最后来分析一下卷积在神经网络中如此受用的原因，然后对如何整合这些卷积，如何通过一个标注过的训练集训练卷积神经网络做个简单概括。

和只用全连接层相比，卷积层的两个主要优势在于**参数共享**和**稀疏连接**。

- 参数共享

  特征检测如垂直边缘检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。也就是说，如果用一个3×3的过滤器检测垂直边缘，那么图片的左上角区域，以及旁边的各个区域都可以使用这个3×3的过滤器。每个特征检测器以及输出都可以在输入图片的不同区域中使用同样的参数，以便提取垂直边缘或其它特征。

  直观感觉是，一个特征检测器，如垂直边缘检测器用于检测图片左上角区域的特征，这个特征很可能也适用于图片的右下角区域。因此在计算图片左上角和右下角区域时，不需要添加其它特征检测器。假如有一个这样的数据集，其左上角和右下角可能有不同分布，也有可能稍有不同，但很相似，整张图片共享特征检测器，提取效果也很好。

- 稀疏连接

  ![](http://www.ai-start.com/dl2017/images/7503372ab986cd3aedda7674bedfd5f0.png)

  图中绿色圈中的0是通过3×3的卷积计算得到的，它只依赖于对应绿色阴影框中3×3的输入的单元格，这个元素0仅与36个输入特征中9个相连接，而且其它像素值都不会对输出产生任影响，这就是稀疏连接的概念。同理，红色标记的元素 30仅仅依赖于红色阴影框中的9个特征，只有这9个输入特征与输出相连接，其它像素对输出没有任何影响。

神经网络可以通过这两种机制减少参数，以便我们用更小的训练集来训练它，从而预防过度拟合。实际上，我们用同一个过滤器生成各层中，图片的所有像素值，希望网络通过自动学习变得更加健壮，以便更好地取得所期望的平移不变属性。

**训练卷积网络：**

比如要构建一个猫咪检测器，$x$表示一张图片，$\hat{y}$是二进制标记。

选定了一个卷积神经网络，输入图片，增加卷积层和池化层，然后添加全连接层，最后输出一个softmax，即$\hat{y}$。

卷积层和全连接层有不同的参数$w$和偏差$b$，我们可以用任何参数集合来定义代价函数。一个类似于之前讲过的代价函数，并随机初始化其参数$w$和$b$，最后求得$\text{Cost}\ J = \frac{1}{m}\sum_{i = 1}^{m}{L(\hat{y}^{(i)},y^{(i)})}$。

所以训练神经网络，要做的就是使用梯度下降法，或其它算法，例如Momentum梯度下降法，含RMSProp或其它因子的梯度下降来优化神经网络中所有参数，以减少代价函数$J$的值。通过上述操作就可以构建一个高效的猫咪检测器或其它检测器。