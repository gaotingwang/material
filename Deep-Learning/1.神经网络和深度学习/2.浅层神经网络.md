[TOC]

## 浅层神经网络

神经网络看起来是下面这个样子。可以把许多**sigmoid**单元堆叠起来形成一个神经网络。它包含了之前讲的计算的两个步骤：首先计算出值$z$，然后通过$\sigma(z)$计算值$a$。

![浅层神经网络](http://www.ai-start.com/dl2017/images/L1_week3_4.png)

这里使用符号$^{[m]}$表示第$m$层网络（层数是从0开始，即$m=0$）。这样可以保证$^{[m]}$不会和之前用来表示第$i$个训练样本的$^{(i)}$混淆。

### 1. 神经网络的表示（Neural Network Representation）

![神经网络](http://www.ai-start.com/dl2017/images/L1_week3_3.png)

输入特征$x_1$、$x_2$、$x_3$，它们被竖直地堆叠起来，这叫做神经网络的**输入层**。它包含了神经网络的输入；然后这里有另外一层称之为**隐藏层**。在本例中最后一层只由一个结点构成，而这个只有一个结点的层被称为**输出层**，它负责产生预测值。

解释隐藏层的含义：在一个神经网络中，当你使用监督学习训练它的时候，训练集包含了输入$x$也包含了目标输出$y$，所以术语隐藏层的含义是在训练集中，这些中间结点的准确值我们是不知道到的，也就是说你看不见它们在训练集中应具有的值。你能看见输入的值，你也能看见输出的值，但是隐藏层中的东西，在训练集中你是无法看到的。所以这也解释了词语隐藏层，只是表示你无法在训练集中看到他们。

现在再引入几个符号，就像之前用向量$x$表示输入特征。这里有个可代替的记号==$a^{[0]}$可以用来表示输入特征==。$a$表示激活的意思，它意味着网络中不同层的值会传递到它们后面的层中，输入层将$x$传递给隐藏层，所以将输入层的激活值称为$a^{[0]}$；下一层即隐藏层也同样会产生一些激活值，将其记作$a^{[1]}$，所以具体地，这里的第一个单元或结点将其表示为$a^{[1]}_{1}$，第二个结点的值我们记为$a^{[1]}_{2}$以此类推。所以这里的是一个四维的向量如果写成Python代码，那么它是一个规模为$4\times1$的矩阵或一个大小为4的列向量，也称为四个隐藏层单元。

在约定俗成的符号传统中，在这里所看到的这个例子，只能叫做一个两层的神经网络。原因是当我们**计算网络的层数时，输入层是不算入总层数内**，所以隐藏层是第一层，输出层是第二层。第二个惯例是我们将输入层称为第零层，所以在技术上，这仍然是一个三层的神经网络，因为这里有输入层、隐藏层，还有输出层。但是在传统的符号使用中，如果你阅读研究论文或者在这门课中，你会看到人们将这个神经网络称为一个两层的神经网络，因为我们不将输入层看作一个标准的层。

最后，我们要看到的隐藏层以及最后的输出层是带有参数的，这里的隐藏层将拥有两个参数$W$和$b$，给它们加上上标$^{[1]}$($W^{[1]}$,$b^{[1]}$)，表示这些参数是和第一层这个隐藏层有关系的。在这个例子中我们会看到$W$是一个$4\times3$的矩阵，而$b$是一个$4\times1$的向量，第一个数字4源自于我们需要计算四个结点或隐藏层单元，然后数字3源自于这里有三个输入特征。相似的输出层也有一些与之关联的参数$W^{[2]}$以及$b^{[2]}$。从维数上来看，它们的规模分别是$1\times4$以及$1\times1$。

### 2. 计算一个神经网络的输出（Computing a Neural Network's output）

![神经网络神经单元计算示意](http://www.ai-start.com/dl2017/images/L1_week3_6.png)

关于神经网络是怎么计算的，从之前提及的逻辑回归开始，如上图所示。用圆圈表示神经网络的计算单元，逻辑回归的计算有两个步骤，首先按步骤计算出$z$，然后在第二步中以**sigmoid**函数为激活函数计算$z$（得出$a$），一个神经网络只是这样子做了好多次重复计算。
$$
\left[
		\begin{array}{c}
		z^{[1]}_{1}\\
		z^{[1]}_{2}\\
		z^{[1]}_{3}\\
		z^{[1]}_{4}\\
		\end{array}
		\right]
		 =
	\overbrace{
	\left[
		\begin{array}{c}
		...W^{[1]T}_{1}...\\
		...W^{[1]T}_{2}...\\
		...W^{[1]T}_{3}...\\
		...W^{[1]T}_{4}...
		\end{array}
		\right]
		}^{W^{[1]}}
		*
	\overbrace{
	\left[
		\begin{array}{c}
		x_1\\
		x_2\\
		x_3\\
		\end{array}
		\right]
		}^{input}
		+
	\overbrace{
	\left[
		\begin{array}{c}
		b^{[1]}_1\\
		b^{[1]}_2\\
		b^{[1]}_3\\
		b^{[1]}_4\\
		\end{array}
		\right]
		}^{b^{[1]}}
$$
向量化的过程是将神经网络中的一层神经元参数纵向堆积起来，例如隐藏层中的$w$纵向堆积起来变成一个$(4,3)$的矩阵，用符号$W^{[1]}$表示。
$$
a^{[1]} =
	\left[
		\begin{array}{c}
		a^{[1]}_{1}\\
		a^{[1]}_{2}\\
		a^{[1]}_{3}\\
		a^{[1]}_{4}
		\end{array}
		\right]
		= \sigma(z^{[1]})
$$
对于神经网络的第一层，给予一个输入$x$，得到$a^{[1]}$，$x$可以表示为$a^{[0]}$。通过相似的衍生后会发现，后一层的表示同样可以写成类似的形式，得到$a^{[2]}$，$\hat{y} = a^{[2]}$ 。

![神经网络向量化实现](http://www.ai-start.com/dl2017/images/L1_week3_7.png)

当有一个包含一层隐藏层的神经网络，需要去实现以计算得到输出的是右边的四个等式，并且可以看成是一个向量化的计算过程，计算出隐藏层的四个逻辑回归单元和整个隐藏层的输出结果，如果编程实现需要的也只是这四行代码。

### 3. 多样本向量化（Vectorizing across multiple examples）

上一节给出了如何计算出$z^{[1]}$，$a^{[1]}$，$z^{[2]}$，$a^{[2]}$。

对于一个给定的输入特征向量$X$，这四个等式可以计算出$\alpha^{[2]}$等于$\hat{y}$。这是针对于单一的训练样本。如果有$m$个训练样本,那么就需要重复这个过程。

用第一个训练样本$x^{(1)}$来计算出预测值$\hat{y}^{(1)}$，就是第一个训练样本上得出的结果。

然后，用$x^{(2)}$来计算出预测值$\hat{y}^{(2)}$，循环往复，直至用$x^{(m)}$计算出$\hat{y}^{(m)}$。

如果有一个非向量化形式的实现，而且要计算出它的预测值，对于所有训练样本，需要让$i$从1到$m$实现这四个等式：

$z^{[1](i)}=W^{[1](i)}x^{(i)}+b^{[1](i)}$

$a^{[1](i)}=\sigma(z^{[1](i)})$

$z^{[2](i)}=W^{[2](i)}a^{[1](i)}+b^{[2](i)}$

$a^{[2](i)}=\sigma(z^{[2](i)})$

对于上面的这个方程中的$^{(i)}$，是所有依赖于训练样本数的变量，即将$(i)$添加到$x$，$z$和$a$。如果想计算$m$个训练样本上的所有输出，就应该向量化整个计算，以简化这列:
$$
x =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
		\right]
$$

$$
W^{[1]} = 
\left[
\begin{array}{c}
		...W^{[1]T}_{1}...\\
		...W^{[1]T}_{2}...\\
		...W^{[1]T}_{3}...\\
		...W^{[1]T}_{4}...
\end{array}
\right]
$$

$$
A^{[1]} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		a^{[1](1)} & a^{[1](2)} & \cdots & a^{[1](m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
		\right]
$$

现在 $W^{[1]}$ 是一个矩阵，$x^{(1)},x^{(2)},x^{(3)}$都是列向量，矩阵乘以列向量得到列向量，下面将它们用图形直观的表示出来:
$$
W^{[1]}  x =
		\left[
                  \begin{array}{c}
                          ...W^{[1]T}_{1}...\\
                          ...W^{[1]T}_{2}...\\
                          ...W^{[1]T}_{3}...\\
                          ...W^{[1]T}_{4}...
                  \end{array}
                  \right]
		
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		x^{(1)} & x^{(2)} & x^{(3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		w^{(1)}x^{(1)} & w^{(1)}x^{(2)} & w^{(1)}x^{(3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=\\
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		z^{[1](1)} & z^{[1](2)} & z^{[1](3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=
		Z^{[1]}
$$
之前对单个样本的计算写成$z^{[1](i)} = W^{[1]}x^{(i)} + b^{[1]}$ ，使用这种形式，因为当有不同的训练样本时，将它们堆到矩阵$X$的各列中，那么它们的输出也就会相应的堆叠到矩阵 $Z^{[1]}$ 的各列中。现在就可以直接计算矩阵 $Z^{[1]}$ 加上$b^{[1]}$，因为列向量 $b^{[1]}$ 和矩阵 $Z^{[1]}$的列向量有着相同的尺寸，而**Python**的广播机制对于这种矩阵与向量直接相加的处理方式是，将向量与矩阵的每一列相加。
所以这一节只是说明了为什么公式 $Z^{[1]} =W^{[1]}X + \ b^{[1]}$是前向传播的第一步计算的正确向量化实现，但事实证明，类似的分析可以发现，前向传播的其它步也可以使用非常相似的逻辑，即如果将输入按列向量横向堆叠进矩阵，那么通过公式计算之后，也能得到成列堆叠的输出。

$$
\left.
		\begin{array}{r}
		\text{$z^{[1](i)} = W^{[1](i)}x^{(i)} + b^{[1]}$}\\
		\text{$\alpha^{[1](i)} = \sigma(z^{[1](i)})$}\\
		\text{$z^{[2](i)} = W^{[2](i)}\alpha^{[1](i)} + b^{[2]}$}\\
		\text{$\alpha^{[2](i)} = \sigma(z^{[2](i)})$}\\
		\end{array}
		\right\}
		\implies
		\begin{cases}
		\text{$A^{[1]} = \sigma(z^{[1]})$}\\
		\text{$z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$}\\ 
		\text{$A^{[2]} = \sigma(z^{[2]})$}\\ 
		\end{cases}
$$
从水平上看，矩阵$A$代表了各个训练样本。从竖直上看，矩阵$A$的不同的索引对应于不同的隐藏单元。对于矩阵$Z，X$情况也类似，水平方向上，对应于不同的训练样本；竖直方向上，对应不同的输入特征，而这就是神经网络输入层中各个节点。

使用向量化的方法，可以不需要显示循环，而直接通过矩阵运算从$X$就可以计算出 $A^{[1]}$，实际上$X$可以记为 $A^{[0]}$，使用同样的方法就可以由神经网络中的每一层的输入 $A^{[i-1]}$ 计算输出 $A^{[i]}$。其实这些方程有一定对称性，其中第一个方程也可以写成$Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]}$ 。这里我们有一个双层神经网络，后续会看到随着网络的深度变大，基本上也还是重复这两步运算，只不过是比这里的重复次数更多。

以上就是对神经网络向量化实现的正确性的解释，到目前为止，我们仅使用**sigmoid**函数作为激活函数，事实上这并非最好的选择，后续会深入的讲解如何使用更多不同种类的激活函数。

### 4. 激活函数（Activation functions）

使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。到目前为止，只用过**sigmoid**激活函数，但是，有时其他的激活函数效果会更好。

更通常的情况下，使用不同的函数$g( z)$，$g$可以是除了**sigmoid**函数以外的非线性函数。**tanh**函数或者双曲正切函数是总体上都优于**sigmoid**函数的激活函数。

![激励函数](http://www.ai-start.com/dl2017/images/L1_week3_9.jpg)

**tanh函数**:

如图，$a = tan(z)=\frac{e^{z} - e^{- z}}{e^{z} + e^{- z}}$的值域是位于-1和+1之间。事实上，**tanh**函数是**sigmoid**的向下平移和伸缩后的结果。对它进行了变形后，穿过了$(0,0)$点，并且值域介于+1和-1之间。

结果表明，如果在隐藏层上使用**tanh**函数效果总是优于**sigmoid**函数。因为函数值域在-1和+1的激活函数，其均值是更接近零均值的。在训练一个算法模型时，==如果使用**tanh**函数代替**sigmoid**函数中心化数据，使得数据的平均值更接近0而不是0.5==，这会使下一层学习简单一点。

在讨论优化算法时，很多场合基本已经不用**sigmoid**激活函数了，**tanh**函数在所有场合都优于**sigmoid**函数。但有一个例外：==在二分类的问题中，对于输出层，因为$y$的值是0或1，所以想让$\hat{y}$的数值介于0和1之间，而不是在-1和+1之间。所以需要使用**sigmoid**激活函数==。

所以，在不同的神经网络层中，激活函数可以不同。为了表示不同的激活函数，在不同的层中，使用方括号上标来指出。$g^{[1]}$的激活函数，可能会跟$g^{[2]}$不同。方括号上标$[1]$代表隐藏层，方括号上标$[2]$表示输出层。

==**sigmoid**函数和**tanh**函数两者共同的缺点是，在$z$特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度==。

**ReLu函数**:

在机器学习另一个很流行的函数是：修正线性单元的函数（**ReLu**），函数表示为$ a =max( 0,z) $。

只要$z$是正值的情况下，导数恒等于1，当$z$是负值的时候，导数恒等于0。从实际上来说，当使用$z$的导数时，$z$=0的导数是没有定义的。但是当编程实现的时候，$z$的取值刚好等于0，这个值概率相当小，所以，在实践中，不需要担心这个值，$z$是等于0的时候，假设一个导数是1或者0效果都可以。

**Leaky Relu函数**：

这里也有另一个版本的**Relu**被称为**Leaky Relu**。当$z$是负值时，这个函数的值不是等于0，而是轻微的倾斜，函数表示为$a = max( 0.01z,z)$。这个函数通常比**Relu**激活函数效果要好，尽管在实际中**Leaky ReLu**使用的并不多。

$z$在**ReLu**的梯度一半都是0，但是，有足够的隐藏层使得z值大于0，所以对大多数的训练数据来说学习过程仍然可以很快。

**Relu函数优点**：

- 在$z$的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个**if-else**语句，而**sigmoid**函数需要进行浮点四则运算，在实践中，使用**ReLu**激活函数神经网络通常会比使用**sigmoid**或者**tanh**激活函数学习的更快。
- **sigmoid**和**tanh**函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而**Relu**和**Leaky ReLu**函数大于0部分都为常熟，不会产生梯度弥散现象。(同时应该注意到的是，**Relu**进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而**Leaky ReLu**不会有这问题)

**总结**：

这有一些选择激活函数的经验法则：

如果输出是0、1值（二分类问题），则在输出层选择**sigmoid**函数，然后其它的所有单元都选择**Relu**函数。这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用**Relu**激活函数。有时，也会使用**tanh**激活函数，但**Relu**的一个缺点是：当$z$是负值的时候，导数等于0，但这在实践中问题不大。

**sigmoid**激活函数：除了输出层是一个二分类问题基本不会用它。

**tanh**激活函数：**tanh**是非常优秀的，几乎适合所有场合。

**ReLu**激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用**ReLu**或者**Leaky ReLu**。

### 5. 为什么需要非线性激活函数？（why need a nonlinear activation function?）

理解为什么使用非线性激活函数对于神经网络十分关键！事实证明：要让神经网络能够计算出有趣的函数，必须使用非线性激活函数。

假设在神经网络正向传播的过程中，我们去掉函数$g$，然后令$a^{[1]} = z^{[1]}$，或者可以令$g(z)=z$，这个有时被叫做线性激活函数（更学术点的名字是恒等激励函数，因为它们就是把输入值输出）。

现在改变前面的式子，令：
$$
\begin{eqnarray*}
a^{[1]} = z^{[1]} = W^{[1]}x + b^{[1]} \tag{1} \\
a^{[2]} = z^{[2]} = W^{[2]}a^{[1]}+ b^{[2]} \tag{2}
\end{eqnarray*}
$$
将式子$(1)$代入式子$(2)$中，则：
$$
a^{[2]} = z^{[2]} = W^{[2]}(W^{[1]}x + b^{[1]}) + b^{[2]} = W^{[2]}W^{[1]}x + W^{[2]}b^{[1]} + b^{[2]}
$$
简化多项式得:
$$
a^{[2]} = z^{[2]} = W^{'}x + b^{'} 
$$
事实证明，如果使用线性激活函数或者没有使用一个激活函数，那么无论神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。在这里线性隐层一点用也没有，因为这两个线性函数的组合本身就是线性函数，所以除非你引入非线性，否则你无法计算更有趣的函数，即使你的网络层数再多也不行。

只有一个地方可以使用线性激活函数$g(z)=z$，就是在做机器学习中的回归问题。$y$ 是一个实数，举个例子：比如想预测房地产价格，$y$ 就不是二分类任务0或1，而是一个实数，从0到正无穷。如果$y$ 是个实数，那么在输出层用线性激活函数也许可行，输出也是一个实数，从负无穷到正无穷。

总而言之，不能在隐藏层用线性激活函数，可以用**ReLU**或者**tanh**或者**leaky ReLU**或者其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层。

### 6. 激活函数的导数（Derivatives of activation functions）

在神经网络中使用反向传播的时候，需要计算激活函数的斜率或者导数。针对以下四种激活函数，求其导数如下：

1. **sigmoid activation function**

   ![sigmoid函数](http://www.ai-start.com/dl2017/images/L1_week3_10.png)

   其具体的求导如下：
   $$
   \frac{d}{dz}g(z) = {\frac{1}{1 + e^{-z}} (1-\frac{1}{1 + e^{-z}})}=g(z)(1-g(z))
   $$

2. **Tanh activation function**

   ![tanh函数](http://www.ai-start.com/dl2017/images/L1_week3_11.png)

   其具体的求导如下：
   $$
   \frac{d}{{\rm d}z}g(z) = 1 - (tanh(z))^{2}
   $$

3. **Rectified Linear Unit (ReLU)**

   ![ReLu函数](http://www.ai-start.com/dl2017/images/L1_week3_12.png)

   其求导如下：
   $$
   g(z)^{'}=
     \begin{cases}
     0&	\text{if z < 0}\\
     1&	\text{if z > 0}\\
   undefined&	\text{if z = 0}
   \end{cases}
   $$
   注：通常在$z$= 0的时候给定其导数1,0；当然$z$=0的情况很少

4. **Leaky linear unit (Leaky ReLU)**

   求导与**ReLU**类似：
   $$
   g(z)=\max(0.01z,z) \\
   	\\
   	\\
   g(z)^{'}=
   \begin{cases}
   0.01& 	\text{if z < 0}\\
   1&	\text{if z > 0}\\
   undefined&	\text{if z = 0}
   \end{cases}
   $$



### 7. 神经网络的梯度下降（Gradient descent for neural networks）

逻辑回归的公式：
$$
\left.
	\begin{array}{l}
	{x }\\
	{w }\\
	{b }
	\end{array}
	\right\}
	\implies{z={w}^Tx+b}
	\implies{a = \sigma(z)} 
	\implies{{L}\left(a,y \right)}
$$
回想之前逻辑回归，我们有这个正向传播步骤，其中我们计算$z$，然后$a$，然后损失函数$L$。
$$
\underbrace{
	\left.
	\begin{array}{l}
	{x }\\
	{w }\\
	{b }
	\end{array}
	\right\}
	}_{dw={dz}\cdot x, db =dz}
	\impliedby\underbrace{{z={w}^Tx+b}}_{
	{\frac{{dL}}{dz}}={\frac{{dL}}{da}}\cdot{\frac{da}{dz}=a-y}
	}
	\impliedby\underbrace{{a = \sigma(z)} 
	\impliedby{L(a,y)}}_{da={\frac{{d}}{da}}{L}\left(a,y \right)=(-y\log{\alpha} - (1 - y)\log(1 - a))^{'}={-\frac{y}{a}} + {\frac{1 - y}{1 - a}{}} }
$$
神经网络的计算中，与逻辑回归十分类似，但中间会有多层的隐藏层计算：

- 正向传播（forward propagation）

  1. $z^{[1]} = W^{[1]}x + b^{[1]}$
  2. $a^{[1]} = \sigma(z^{[1]})$
  3. $z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$
  4. $a^{[2]} = g^{[2]}(z^{[z]}) = \sigma(z^{[2]})$

- 反向传播（back propagation）

  向后推算出$da^{[2]}$，然后推算出$dz^{[2]}$，接着推算出$da^{[1]}$，然后推算出$dz^{[1]}$。不需要对$x$求导，因为$x$是固定的，我们也不是想优化$x$。向后推算出$da^{[2]}$，然后推算出$dz^{[2]}$的步骤可以合为一步：

  1. $dZ^{[2]}=A^{[2]}-Y$

  2. $dW^{[2]}={\frac{1}{m}}dZ^{[2]}{A^{[1]}}^{T}$(注意：逻辑回归中；为什么${A^{[1]}}^{T}$多了个转置：$dw$中的$w$是一个列向量，而$W^{[2]}$是个行向量，故需要加个转置)

  3. $db^{[2]} = {\frac{1}{m}}np.sum(dZ^{[2]},axis=1,keepdims=True)$

  4. $\underbrace{dZ^{[1]}}_{(n^{[1]}, m)} = \underbrace{W^{[2]T}dZ^{[2]}}_{(n^{[1]}, m)}*\underbrace{g[1]^{'}(Z^{[1]})}_{(n^{[1]}, m)}$

  5. $dW^{[1]} = {\frac{1}{m}}dZ^{[1]}x^{T}$

  6. $db^{[1]} = {\frac{1}{m}}np.sum(dZ^{[1]},axis=1,keepdims=True) $

吴恩达老师认为反向传播的推导是机器学习领域最难的数学推导之一，矩阵的导数要用链式法则来求，如果这章内容掌握不了也没大的关系，只要有这种直觉就可以了。     

### 8. 随机初始化（Random+Initialization）

当你训练神经网络时，权重随机初始化是很重要的。对于逻辑回归，把权重初始化为0当然也是可以的。但是对于一个神经网络，如果你把权重或者参数都初始化为0，那么梯度下降将不会起作用。

![初始化神经网络参数](http://www.ai-start.com/dl2017/images/L1_week3_13.png)

假设有一个如上图的神经网络，$W^{[1]}$是$2\times2$的矩阵，将其初始化全为0的矩阵。不管给网络输入任何样本，最后得到的$a_{1}^{[1]}$ 和 $a_{2}^{[1]}$都会相等。因为两个隐含单元计算同样的函数，在做反向传播计算时，这会导致$\text{dz}_{1}^{[1]}$ 和 $\text{dz}_{2}^{[1]}$也会一样，对称这些隐含单元会初始化得一样，这样输出的权值也会一模一样$[0\;0]$。

因为他们完全对称，也就意味着计算同样的函数，并且肯定的是最终经过每次训练的迭代，这两个隐含单元仍然是同一个函数。$dW$会是一个这样的矩阵，每一行有同样的值因此我们做权重更新把权重$W^{[1]}:={W^{[1]}-\alpha dW}$每次迭代后的$W^{[1]}$，第一行等于第二行。

由此可以推导，如果你把权重都初始化为0，那么由于隐含单元开始计算同一个函数，当前层的所有的隐含单元就会输出一样的值，即隐含单元仍是对称的。通过推导，两次、三次、无论多少次迭代，不管训练网络多长时间，隐含单元仍然计算的是同样的函数。因此这种情况下超过1个隐含单元也没什么意义，因为他们计算同样的东西。

==如果初始化成0，由于所有的隐含单元都是对称的，无论你运行梯度下降多久，他们一直计算同样的函数==。这没有任何帮助，因为你想要两个不同的隐含单元计算不同的函数，这个问题的解决方法就是随机初始化参数。你应该这么做：把$W^{[1]}$设为`np.random.randn(2,2)`(生成高斯分布)，通常再乘上一个小的数，比如0.01，这样把它初始化为很小的随机数。然后$b$没有这个对称的问题（叫做**symmetry breaking problem**），所以可以把 $b$ 初始化为0，因为只要随机初始化$W$你就有不同的隐含单元计算不同的东西，因此不会有**symmetry breaking**问题了。相似的，对于$W^{[2]}$你可以随机初始化，$b^{[2]}$可以初始化为0：

$W^{[1]} = np.random.randn(2,2)\;*\;0.01\;,\;b^{[1]} = np.zeros((2,1))$
$W^{[2]} = np.random.randn(2,2)\;*\;0.01\;,\;b^{[2]} = 0$

你也许会疑惑，这个常数从哪里来，为什么是0.01，而不是100或者1000。我们通常倾向于初始化为很小的随机数。==因为如果你用**tanh**或者**sigmoid**激活函数，或者说只在输出层有一个**Sigmoid**，如果（数值）波动太大，当你计算激活值时$z^{[1]} = W^{[1]}x + b^{[1]}\;,\;a^{[1]} = \sigma(z^{[1]})=g^{[1]}(z^{[1]})$，如果$W$很大，$z$就会很大。这种情况下你很可能停在**tanh**/**sigmoid**函数的平坦的地方，这些地方梯度很小也就意味着梯度下降会很慢，造成学习也就很慢==。

回顾一下：如果$w$很大，那么你很可能最终停在（甚至在训练刚刚开始的时候）$z$很大的值，这会造成**tanh**/**Sigmoid**激活函数饱和在龟速的学习上，如果没有**sigmoid**/**tanh**激活函数在你整个的神经网络里，就不成问题。但如果你做二分类并且你的输出单元是**Sigmoid**函数，那么你不会想让初始参数太大，因此这就是为什么乘上0.01或者其他一些小数是合理的尝试，对于$w^{[2]}$一样。

事实上有时有比0.01更好的常数，当你训练一个只有一层隐藏层的网络时（这是相对浅的神经网络，没有太多的隐藏层），设为0.01可能也可以。但当你训练一个非常非常深的神经网络，你可能会选择一个不同于的常数而不是0.01。后面会讨论怎么并且何时去选择一个不同于0.01的常数，但是无论如何它通常都会是个相对小的数。